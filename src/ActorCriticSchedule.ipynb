{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "import pygame\n",
    "import random\n",
    "from collections import deque\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get device\n",
    "CPU or GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create environment\n",
    "The Schedule needs an environment, lets create it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a class for the schedule problem using the gym interface\n",
    "class ScheduleGym():\n",
    "    def __init__(self, num_days, num_hours, num_classes, num_subjects, verbose=False):\n",
    "        self.num_days = num_days\n",
    "        self.num_hours = num_hours\n",
    "        self.num_classes = num_classes\n",
    "        self.num_subjects = num_subjects\n",
    "        self.num_slots = num_days * num_hours\n",
    "        self.target_hours = np.zeros((num_classes, num_subjects), dtype=int) # Target hours for each class and subject\n",
    "        self.schedule = -1*np.ones((num_classes, num_days, num_hours), dtype=int) # Schedule for each class, -1 means no subject assigned\n",
    "        self.num_actions_left = 1000 #Number of actions left to take\n",
    "        self.verbose = verbose #Debug on or off\n",
    "        #This is to go from a one dimensional action space to a 4 dimensional action space\n",
    "        #class_id, day, hour, subject_id\n",
    "        self.max_values = np.array([num_classes, num_days, num_hours, num_subjects])\n",
    "        self.cumprod_max_values = np.cumprod(self.max_values[::-1])[::-1]\n",
    "        # self.decoder_base = np.cumprod(self.max_values)\n",
    "        # self.encoder_base = np.flip(np.cumprod(np.flip(self.max_values)))\n",
    "        self.initial_hours_to_assign = 0 #How many hours we have initially to assign, used to calculate score later\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        for class_id in range(self.num_classes):\n",
    "            for subject_id in range(self.num_subjects):\n",
    "                self.target_hours[class_id, subject_id] = np.random.randint(1, 5)\n",
    "        \n",
    "        self.initial_hours_to_assign = self.target_hours.sum().sum()\n",
    "\n",
    "        self.schedule = -1*np.ones((self.num_classes, self.num_days, self.num_hours), dtype=int) # Schedule for each class, -1 means no subject assigned\n",
    "        self.num_actions_left = self.initial_hours_to_assign * 10 #Optimally we would need to take number of hours to assign steps to complete the schedule, lets give it some wiggle room\n",
    "        \n",
    "        #Return the current state,info\n",
    "        info = {}\n",
    "        #return (self.target_hours, self.schedule), info\n",
    "        return self.state2vector(), info\n",
    "    \n",
    "    def state2vector(self):\n",
    "        #Convert the state to a vector\n",
    "        return np.concatenate([self.target_hours.flatten(), self.schedule.flatten()])/(self.initial_hours_to_assign * 10.0 + 1.0)\n",
    "    \n",
    "    def render(self):\n",
    "        #Print the schedule\n",
    "        for class_id in range(self.num_classes):\n",
    "            print(f\"Class {class_id + 1}:\")\n",
    "            for day in range(self.num_days):\n",
    "                print(f\"Day {day + 1}: {self.schedule[class_id, day]}\")\n",
    "            print()\n",
    "        print(f'Fitness: {self.fitness()}, Actions left: {self.num_actions_left}')\n",
    "\n",
    "        #print the target hours\n",
    "        print(\"Target Hours:\")\n",
    "        for class_id in range(self.num_classes):\n",
    "            print(f\"Class {class_id + 1}: {self.target_hours[class_id]}\")\n",
    "\n",
    "\n",
    "\n",
    "    def decode_action(self, actions):\n",
    "        actions = np.array(actions).reshape(-1) # Ensure numbers is a 1D column array\n",
    "        aAll = np.zeros((actions.shape[0], len(self.max_values)), dtype=int)\n",
    "        for i in range(len(self.max_values) - 1):\n",
    "            aAll[:, i] = actions // self.cumprod_max_values[i+1]\n",
    "            actions -= aAll[:, i]*self.cumprod_max_values[i+1]\n",
    "        aAll[:,-1] = actions\n",
    "        \n",
    "        return aAll\n",
    "    \n",
    "    # Go from a 4D action to a 1D action\n",
    "    def encode_action(self, actions):\n",
    "        number = np.zeros(actions.shape[0], dtype=int)\n",
    "        for i in range(len(self.max_values) - 1):\n",
    "            number += actions[:,i]*self.cumprod_max_values[i+1]\n",
    "        number += actions[:,-1]\n",
    "        \n",
    "        return number\n",
    "\n",
    "            \n",
    "    #next_state, reward, done, truncated, info = env.step(action)\n",
    "    # def step(self, action):\n",
    "    #     # Update the schedule based on the action\n",
    "\n",
    "    #     #Check if the action is a tuple or a single value\n",
    "    #     if isinstance(action, tuple):\n",
    "    #         #We are already in the decoded format\n",
    "    #         class_id, day, hour, subject_id = action\n",
    "    #     elif isinstance(action, np.ndarray):\n",
    "    #         class_id = action[0]\n",
    "    #         day = action[1]\n",
    "    #         hour= action[2]\n",
    "    #         subject_id = action[3]\n",
    "    #     else:\n",
    "    #         #Need to go from 1D to 4D\n",
    "    #         decoded = self.decode_action(action).squeeze()\n",
    "    #         class_id = decoded[0]\n",
    "    #         day = decoded[1]\n",
    "    #         hour= decoded[2]\n",
    "    #         subject_id = decoded[3]\n",
    "\n",
    "         \n",
    "    #     #If subject_id is >= num_subjects then this is a remove action for the class_id, day, hour slot\n",
    "    #     #If the slot is already occupied, then the old subject will be placed back into the target hours\n",
    "        \n",
    "    #     current_subject_id = self.schedule[class_id, day, hour]\n",
    "\n",
    "    #     if self.verbose:\n",
    "    #         print('Before action:')\n",
    "    #         for class_id in range(self.num_classes):\n",
    "    #             print(f\"Class {class_id + 1}: {self.target_hours[class_id]}\")\n",
    " \n",
    "    #     #The slot was already booked, lets reomove it (and all of its dependencies)\n",
    "    #     result = \"N/A\"\n",
    "    #     reward = 0\n",
    "    #     if current_subject_id != -1:\n",
    "    #         self.schedule[class_id, day, hour] = -1\n",
    "    #         self.target_hours[class_id, current_subject_id] += 1\n",
    "    #         result = \"Removed\"\n",
    "    #         reward = -0.16 #Penalty for removing a subject\n",
    "\n",
    "        \n",
    "    #     #See if it is an add action\n",
    "    #     elif subject_id < self.num_subjects:\n",
    "    #         #Yes its an add action, lets see if we have enough hours to actually add it\n",
    "    #         if self.target_hours[class_id, subject_id] > 0:\n",
    "    #             self.schedule[class_id, day, hour] = subject_id\n",
    "    #             self.target_hours[class_id, subject_id] -= 1\n",
    "    #             result = \"Added\"\n",
    "    #             reward = 0.15 #Reward for adding a subject\n",
    "    #         else:\n",
    "    #             #We are trying to add a subject that is already empty\n",
    "    #             reward = -0.25 #Penalty for invalid action    \n",
    "    #     else:\n",
    "    #         #We are trying to remove a subject that is not in the schedule, this is an invalid action\n",
    "    #         reward = -0.5 #Penalty for invalid action\n",
    "        \n",
    "    #     self.num_actions_left -= 1\n",
    "    #     done = self.is_done()  \n",
    "    #     if done:\n",
    "    #         reward += 1.0\n",
    "    #         reward += self.fitness()  #If we are done we get the full score of the schedule\n",
    "        \n",
    "    #     if self.verbose:\n",
    "    #         print(f\"Action: {action}, Class: {class_id}, Subject: {subject_id}, Day: {day}, Hour: {hour}, , current_subject_id: {current_subject_id}, Result: {result}, reward: {reward}, done: {done}\")     \n",
    "    #         print('After action:')\n",
    "    #         for class_id in range(self.num_classes):\n",
    "    #             print(f\"Class {class_id + 1}: {self.target_hours[class_id]}\")\n",
    "\n",
    "    #         if done:\n",
    "    #             print('Final schedule:')\n",
    "    #             self.render()\n",
    "                \n",
    "    #     truncated, info = False, {} #To be implemented later if needed\n",
    "    #     #Return the next state, reward, done, truncated and info\n",
    "    #     #return (self.target_hours, self.schedule), reward, done, truncated, info\n",
    "    #     return self.state2vector(), reward, done, truncated, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Update the schedule based on the action\n",
    "\n",
    "        #Check if the action is a tuple or a single value\n",
    "        if isinstance(action, tuple):\n",
    "            #We are already in the decoded format\n",
    "            class_id, day, hour, subject_id = action\n",
    "        elif isinstance(action, np.ndarray):\n",
    "            class_id = action[0]\n",
    "            day = action[1]\n",
    "            hour= action[2]\n",
    "            subject_id = action[3]\n",
    "        else:\n",
    "            #Need to go from 1D to 4D\n",
    "            decoded = self.decode_action(action).squeeze()\n",
    "            class_id = decoded[0]\n",
    "            day = decoded[1]\n",
    "            hour= decoded[2]\n",
    "            subject_id = decoded[3]\n",
    "\n",
    "         \n",
    "        #If subject_id is >= num_subjects then this is a remove action for the class_id, day, hour slot\n",
    "        #If the slot is already occupied, then the old subject will be placed back into the target hours\n",
    "        \n",
    "        current_subject_id = self.schedule[class_id, day, hour]\n",
    "\n",
    "        if self.verbose:\n",
    "            print('Before action:')\n",
    "            for class_id in range(self.num_classes):\n",
    "                print(f\"Class {class_id + 1}: {self.target_hours[class_id]}\")\n",
    " \n",
    "        #The slot was already booked, lets reomove it (and all of its dependencies)\n",
    "        result = \"N/A\"\n",
    "        reward = 0\n",
    "        if current_subject_id != -1:\n",
    "            self.schedule[class_id, day, hour] = -1\n",
    "            self.target_hours[class_id, current_subject_id] += 1\n",
    "            result = \"Removed\"\n",
    "            reward = -0.12 #Penalty for removing a subject\n",
    "\n",
    "        \n",
    "        #See if it is an add action\n",
    "        elif subject_id < self.num_subjects:\n",
    "            #Yes its an add action, lets see if we have enough hours to actually add it\n",
    "            if self.target_hours[class_id, subject_id] > 0:\n",
    "                self.schedule[class_id, day, hour] = subject_id\n",
    "                self.target_hours[class_id, subject_id] -= 1\n",
    "                result = \"Added\"\n",
    "                reward = 0.1 #Reward for adding a subject\n",
    "            else:\n",
    "                #We are trying to add a subject that is already empty\n",
    "                reward = -0.15 #Penalty for invalid action    \n",
    "        else:\n",
    "            #We are trying to remove a subject that is not in the schedule, this is an invalid action\n",
    "            reward = -0.15 #Penalty for invalid action\n",
    "        \n",
    "            # Reward for completing a subject's required hours\n",
    "        if self.target_hours[class_id, subject_id] == 0:\n",
    "            reward += 0.2\n",
    "\n",
    "        completion_percentage = 1.0 - (self.target_hours.sum() / self.initial_hours_to_assign)\n",
    "        reward += completion_percentage * 0.1 \n",
    "\n",
    "        self.num_actions_left -= 1\n",
    "        done = self.is_done()  \n",
    "        if done:\n",
    "            reward += 1.0\n",
    "            reward += self.fitness()  #If we are done we get the full score of the schedule\n",
    "        \n",
    "  \n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Action: {action}, Class: {class_id}, Subject: {subject_id}, Day: {day}, Hour: {hour}, , current_subject_id: {current_subject_id}, Result: {result}, reward: {reward}, done: {done}\")     \n",
    "            print('After action:')\n",
    "            for class_id in range(self.num_classes):\n",
    "                print(f\"Class {class_id + 1}: {self.target_hours[class_id]}\")\n",
    "\n",
    "            if done:\n",
    "                print('Final schedule:')\n",
    "                self.render()\n",
    "                \n",
    "        truncated, info = False, {} #To be implemented later if needed\n",
    "        #Return the next state, reward, done, truncated and info\n",
    "        #return (self.target_hours, self.schedule), reward, done, truncated, info\n",
    "        return self.state2vector(), reward, done, truncated, info       \n",
    "\n",
    "\n",
    "    def is_done(self):\n",
    "        #Check if the schedule is complete\n",
    "        return np.all(self.target_hours == 0) or self.num_actions_left <= 0\n",
    "    \n",
    "    def get_action_sizes(self):\n",
    "        #Return the sizes of the action space\n",
    "        return [self.num_classes, self.num_days, self.num_hours, self.num_subjects]\n",
    "        #return np.prod(self.max_values)\n",
    "    \n",
    "    def get_state_sizes(self):\n",
    "        #Return the shapes of the state space\n",
    "        #return self.target_hours.shape, self.schedule.shape\n",
    "        return self.state2vector().shape\n",
    "\n",
    "    \n",
    "    def fitness(self):\n",
    "        #Calculate the fitness of the schedule\n",
    "        #fitness = self.num_actions_left * 0.001 #We want to maximize the number of actions left\n",
    "        fitness = 0.0\n",
    "        # target hours remaining\n",
    "        target_hours_remaining = self.target_hours.sum().sum()\n",
    "\n",
    "        fitness -= target_hours_remaining * 1\n",
    "\n",
    "        # Count the number of holes in the schedule, that is where no subject is assigned, but is surrounded by subjects\n",
    "        # If there are no subjects assigned to the edges, that is not considered a hole\n",
    "        num_holes = 0\n",
    "\n",
    "        # Create shifted versions of the schedule to compare adjacent hours\n",
    "        left_shifted = np.roll(self.schedule, shift=-1, axis=2)\n",
    "        right_shifted = np.roll(self.schedule, shift=1, axis=2)\n",
    "\n",
    "        # Identify holes: -1 in the current schedule, and not -1 in both the left and right shifted schedules\n",
    "        # Avoid considering the edges by setting the comparison for the first and last hour to False\n",
    "        holes = (self.schedule == -1) & (left_shifted != -1) & (right_shifted != -1)\n",
    "        holes[:, :, 0] = False  # Ignore first hour edge cases\n",
    "        holes[:, :, -1] = False  # Ignore last hour edge cases\n",
    "\n",
    "        # Count the number of holes\n",
    "        num_holes = np.sum(holes)\n",
    "\n",
    "        fitness -= num_holes * 0.15\n",
    "\n",
    "        #return -fitness #Swapped fitness now....\n",
    "\n",
    "        fitness *= 4.2 #\n",
    "        return fitness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an agent that can do scheduling\n",
    "The agent is of type Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class ScheduleAgent(nn.Module):\n",
    "#     def __init__(self, state_size, action_sizes, hidden_dim=256, gamma=0.99):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.action_sizes = action_sizes\n",
    "#         self.gamma = gamma\n",
    "        \n",
    "#         self.shared = nn.Sequential(\n",
    "#             nn.Linear(state_size, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, hidden_dim),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "        \n",
    "#         self.actor_heads = nn.ModuleList([nn.Linear(hidden_dim, dim) for dim in action_sizes])\n",
    "        \n",
    "#         self.critic = nn.Sequential(\n",
    "#             nn.Linear(hidden_dim, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, 1)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, state):\n",
    "#         #Go through the shared layers\n",
    "#         shared_output = self.shared(state)\n",
    "        \n",
    "#         #Each head predicts its own action, like Class, Day, Hour, Subject etc\n",
    "#         action_probs = [torch.softmax(head(shared_output), dim=-1) for head in self.actor_heads]\n",
    "\n",
    "#         #The critic predicts the value of the state\n",
    "#         state_value = self.critic(shared_output)\n",
    "        \n",
    "#         return action_probs, state_value\n",
    "    \n",
    "#     def choose_action(self, state):\n",
    "#         #device = next(self.parameters()).device\n",
    "#         #state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "       \n",
    "#         #Get the probabilities of the actions for each head\n",
    "#         with torch.no_grad():\n",
    "#             action_probs, _ = self.forward(state)\n",
    "        \n",
    "#         #Choose an action for each head\n",
    "#         #actions = [torch.multinomial(probs, 1).item() for probs in action_probs]\n",
    "#         #actions = torch.stack([torch.multinomial(probs, 1) for probs in action_probs]).squeeze(-1)\n",
    "#         actions = torch.tensor([torch.multinomial(probs[0], 1).item() for probs in action_probs], device=state.device)\n",
    "\n",
    "#         return actions\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ScheduleAgent(nn.Module):\n",
    "    def __init__(self, state_size, action_sizes, hidden_dim=256, gamma=0.99):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.action_sizes = action_sizes\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.actor_heads = nn.ModuleList([nn.Linear(hidden_dim, dim) for dim in action_sizes])\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        shared_output = self.shared(state)\n",
    "        \n",
    "        action_probs = [F.softmax(head(shared_output), dim=-1) for head in self.actor_heads]\n",
    "        state_value = self.critic(shared_output)\n",
    "        \n",
    "        return action_probs, state_value\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            action_probs, _ = self.forward(state)\n",
    "        \n",
    "        actions = torch.tensor([torch.multinomial(probs[0], 1).item() for probs in action_probs], device=state.device)\n",
    "        return actions\n",
    "    \n",
    "    def get_value(self, state):\n",
    "        _, state_value = self.forward(state)\n",
    "        return state_value\n",
    "\n",
    "    def get_action_log_probs(self, states, actions):\n",
    "        action_probs, _ = self.forward(states)\n",
    "        # action_log_probs = sum([torch.log(probs[torch.arange(probs.size(0)), action]) \n",
    "        #                         for probs, action in zip(action_probs, actions.T)])\n",
    "        \n",
    "        # action_log_probs = torch.sum(torch.log(torch.stack([torch.gather(probs, 1, action.unsqueeze(1)).squeeze(1) \n",
    "        #                                             for probs, action in zip(action_probs, actions.T)])))\n",
    "        \n",
    "        action_log_probs = torch.stack([torch.log(probs.gather(1, action.unsqueeze(1)).squeeze(1)) \n",
    "                                        for probs, action in zip(action_probs, actions.T)]\n",
    "                                     ).sum(dim=0)\n",
    "\n",
    "        #Also return the entropy of the actions\n",
    "        entropy = torch.stack([-(probs * torch.log(probs)).sum(dim=1) for probs in action_probs]).sum(dim=0)\n",
    "\n",
    "        return action_log_probs, entropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ScheduleGym(num_days=2, num_hours=4, num_classes=1, num_subjects=2)\n",
    "state_dim = env.get_state_sizes()[0]\n",
    "action_dims = env.get_action_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Debug, test on simple environment\n",
    "#env_name = 'CartPole-v1' #Max episode length is 500\n",
    "#env_name = 'Acrobot-v1'\n",
    "env_name = 'MountainCar-v0'\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dims = [env.action_space.n]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ScheduleAgent(state_dim, action_dims, hidden_dim=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,_ = env.reset()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ost = agent(torch.FloatTensor(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = agent.choose_action(torch.FloatTensor(x).unsqueeze(0))\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.1213], grad_fn=<SumBackward1>),\n",
       " tensor([1.0968], grad_fn=<SumBackward1>))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ost3 = agent.get_action_log_probs(torch.FloatTensor(x).unsqueeze(0), actions.unsqueeze(0))\n",
    "ost3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ScheduleAgent(\n",
       "  (shared): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=2048, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (actor_heads): ModuleList(\n",
       "    (0): Linear(in_features=2048, out_features=3, bias=True)\n",
       "  )\n",
       "  (critic): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=2048, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Send the agent to the correct device\n",
    "agent.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(agent.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward: -501.000, End reward: -501.000, Loss: 3528.122, Grad mean: 0.000095, Grad max: 0.233, Actions: 501, Entropy: 1.097\n",
      "Episode 5, Reward: -501.000, End reward: -501.000, Loss: 328.255, Grad mean: 0.000024, Grad max: 0.050, Actions: 501, Entropy: 0.683\n",
      "Episode 10, Reward: -501.000, End reward: -501.000, Loss: 5971.835, Grad mean: 0.000019, Grad max: 0.047, Actions: 501, Entropy: 0.001\n",
      "Episode 15, Reward: -501.000, End reward: -501.000, Loss: 2121.166, Grad mean: 0.000018, Grad max: 0.050, Actions: 501, Entropy: 0.000\n",
      "Episode 20, Reward: -501.000, End reward: -501.000, Loss: 423.792, Grad mean: 0.000018, Grad max: 0.055, Actions: 501, Entropy: 0.000\n",
      "Episode 25, Reward: -501.000, End reward: -501.000, Loss: 455.890, Grad mean: 0.000017, Grad max: 0.057, Actions: 501, Entropy: 0.000\n",
      "Episode 30, Reward: -501.000, End reward: -501.000, Loss: 477.389, Grad mean: 0.000016, Grad max: 0.056, Actions: 501, Entropy: 0.000\n",
      "Episode 35, Reward: -501.000, End reward: -501.000, Loss: 301.860, Grad mean: 0.000016, Grad max: 0.057, Actions: 501, Entropy: 0.000\n",
      "Episode 40, Reward: -501.000, End reward: -501.000, Loss: 303.434, Grad mean: 0.000016, Grad max: 0.057, Actions: 501, Entropy: 0.000\n",
      "Episode 45, Reward: -501.000, End reward: -501.000, Loss: 311.687, Grad mean: 0.000016, Grad max: 0.058, Actions: 501, Entropy: 0.000\n",
      "Episode 50, Reward: -501.000, End reward: -501.000, Loss: 320.471, Grad mean: 0.000016, Grad max: 0.058, Actions: 501, Entropy: 0.000\n",
      "Episode 55, Reward: -501.000, End reward: -501.000, Loss: 299.708, Grad mean: 0.000018, Grad max: 0.058, Actions: 501, Entropy: 0.000\n",
      "Episode 60, Reward: -501.000, End reward: -501.000, Loss: 298.285, Grad mean: 0.000015, Grad max: 0.058, Actions: 501, Entropy: 0.000\n",
      "Episode 65, Reward: -501.000, End reward: -501.000, Loss: 298.039, Grad mean: 0.000018, Grad max: 0.058, Actions: 501, Entropy: 0.000\n",
      "Episode 70, Reward: -501.000, End reward: -501.000, Loss: 301.561, Grad mean: 0.000016, Grad max: 0.058, Actions: 501, Entropy: 0.000\n",
      "Episode 75, Reward: -501.000, End reward: -501.000, Loss: 295.712, Grad mean: 0.000023, Grad max: 0.057, Actions: 501, Entropy: 0.000\n",
      "Episode 80, Reward: -501.000, End reward: -501.000, Loss: 301.489, Grad mean: 0.000016, Grad max: 0.059, Actions: 501, Entropy: 0.000\n",
      "Episode 85, Reward: -501.000, End reward: -501.000, Loss: 297.830, Grad mean: 0.000015, Grad max: 0.059, Actions: 501, Entropy: 0.000\n",
      "Episode 90, Reward: -501.000, End reward: -501.000, Loss: 302.523, Grad mean: 0.000015, Grad max: 0.059, Actions: 501, Entropy: 0.000\n",
      "Episode 95, Reward: -501.000, End reward: -501.000, Loss: 303.493, Grad mean: 0.000015, Grad max: 0.060, Actions: 501, Entropy: 0.000\n",
      "Episode 100, Reward: -501.000, End reward: -501.000, Loss: 302.501, Grad mean: 0.000015, Grad max: 0.059, Actions: 501, Entropy: 0.000\n",
      "Episode 105, Reward: -501.000, End reward: -501.000, Loss: 300.496, Grad mean: 0.000015, Grad max: 0.060, Actions: 501, Entropy: 0.000\n",
      "Episode 110, Reward: -501.000, End reward: -501.000, Loss: 301.662, Grad mean: 0.000015, Grad max: 0.060, Actions: 501, Entropy: 0.000\n",
      "Episode 115, Reward: -501.000, End reward: -501.000, Loss: 299.403, Grad mean: 0.000015, Grad max: 0.060, Actions: 501, Entropy: 0.000\n",
      "Episode 120, Reward: -501.000, End reward: -501.000, Loss: 305.036, Grad mean: 0.000015, Grad max: 0.060, Actions: 501, Entropy: 0.000\n",
      "Episode 125, Reward: -501.000, End reward: -501.000, Loss: 296.209, Grad mean: 0.000015, Grad max: 0.060, Actions: 501, Entropy: 0.000\n",
      "Episode 130, Reward: -501.000, End reward: -501.000, Loss: 300.148, Grad mean: 0.000015, Grad max: 0.060, Actions: 501, Entropy: 0.000\n",
      "Episode 135, Reward: -501.000, End reward: -501.000, Loss: 296.845, Grad mean: 0.000015, Grad max: 0.060, Actions: 501, Entropy: 0.000\n",
      "Episode 140, Reward: -501.000, End reward: -501.000, Loss: 295.999, Grad mean: 0.000015, Grad max: 0.061, Actions: 501, Entropy: 0.000\n",
      "Episode 145, Reward: -501.000, End reward: -501.000, Loss: 299.949, Grad mean: 0.000014, Grad max: 0.061, Actions: 501, Entropy: 0.000\n",
      "Episode 150, Reward: -501.000, End reward: -501.000, Loss: 296.294, Grad mean: 0.000015, Grad max: 0.061, Actions: 501, Entropy: 0.000\n",
      "Episode 155, Reward: -501.000, End reward: -501.000, Loss: 300.540, Grad mean: 0.000015, Grad max: 0.061, Actions: 501, Entropy: 0.000\n",
      "Episode 160, Reward: -501.000, End reward: -501.000, Loss: 299.125, Grad mean: 0.000015, Grad max: 0.061, Actions: 501, Entropy: 0.000\n",
      "Episode 165, Reward: -501.000, End reward: -501.000, Loss: 304.091, Grad mean: 0.000015, Grad max: 0.060, Actions: 501, Entropy: 0.000\n",
      "Episode 170, Reward: -501.000, End reward: -501.000, Loss: 299.282, Grad mean: 0.000015, Grad max: 0.060, Actions: 501, Entropy: 0.000\n",
      "Episode 175, Reward: -501.000, End reward: -501.000, Loss: 297.810, Grad mean: 0.000015, Grad max: 0.060, Actions: 501, Entropy: 0.000\n",
      "Episode 180, Reward: -501.000, End reward: -501.000, Loss: 302.409, Grad mean: 0.000015, Grad max: 0.060, Actions: 501, Entropy: 0.000\n",
      "Episode 185, Reward: -501.000, End reward: -501.000, Loss: 304.686, Grad mean: 0.000015, Grad max: 0.060, Actions: 501, Entropy: 0.000\n",
      "Episode 190, Reward: -501.000, End reward: -501.000, Loss: 297.851, Grad mean: 0.000015, Grad max: 0.061, Actions: 501, Entropy: 0.000\n",
      "Episode 195, Reward: -501.000, End reward: -501.000, Loss: 298.030, Grad mean: 0.000014, Grad max: 0.061, Actions: 501, Entropy: 0.000\n",
      "Episode 200, Reward: -501.000, End reward: -501.000, Loss: 305.920, Grad mean: 0.000014, Grad max: 0.061, Actions: 501, Entropy: 0.000\n",
      "Episode 205, Reward: -501.000, End reward: -501.000, Loss: 306.561, Grad mean: 0.000014, Grad max: 0.061, Actions: 501, Entropy: 0.000\n",
      "Episode 210, Reward: -501.000, End reward: -501.000, Loss: 300.701, Grad mean: 0.000014, Grad max: 0.061, Actions: 501, Entropy: 0.000\n",
      "Episode 215, Reward: -501.000, End reward: -501.000, Loss: 298.533, Grad mean: 0.000014, Grad max: 0.062, Actions: 501, Entropy: 0.000\n",
      "Episode 220, Reward: -501.000, End reward: -501.000, Loss: 308.757, Grad mean: 0.000014, Grad max: 0.062, Actions: 501, Entropy: 0.000\n",
      "Episode 225, Reward: -501.000, End reward: -501.000, Loss: 306.297, Grad mean: 0.000014, Grad max: 0.062, Actions: 501, Entropy: 0.000\n",
      "Episode 230, Reward: -501.000, End reward: -501.000, Loss: 295.619, Grad mean: 0.000027, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 235, Reward: -501.000, End reward: -501.000, Loss: 305.471, Grad mean: 0.000014, Grad max: 0.062, Actions: 501, Entropy: 0.000\n",
      "Episode 240, Reward: -501.000, End reward: -501.000, Loss: 335.522, Grad mean: 0.000014, Grad max: 0.061, Actions: 501, Entropy: 0.000\n",
      "Episode 245, Reward: -501.000, End reward: -501.000, Loss: 326.584, Grad mean: 0.000014, Grad max: 0.063, Actions: 501, Entropy: 0.000\n",
      "Episode 250, Reward: -501.000, End reward: -501.000, Loss: 323.443, Grad mean: 0.000014, Grad max: 0.062, Actions: 501, Entropy: 0.000\n",
      "Episode 255, Reward: -501.000, End reward: -501.000, Loss: 324.250, Grad mean: 0.000014, Grad max: 0.063, Actions: 501, Entropy: 0.000\n",
      "Episode 260, Reward: -501.000, End reward: -501.000, Loss: 337.537, Grad mean: 0.000013, Grad max: 0.062, Actions: 501, Entropy: 0.000\n",
      "Episode 265, Reward: -501.000, End reward: -501.000, Loss: 300.102, Grad mean: 0.000013, Grad max: 0.063, Actions: 501, Entropy: 0.000\n",
      "Episode 270, Reward: -501.000, End reward: -501.000, Loss: 309.954, Grad mean: 0.000013, Grad max: 0.063, Actions: 501, Entropy: 0.000\n",
      "Episode 275, Reward: -501.000, End reward: -501.000, Loss: 307.575, Grad mean: 0.000013, Grad max: 0.064, Actions: 501, Entropy: 0.000\n",
      "Episode 280, Reward: -501.000, End reward: -501.000, Loss: 299.617, Grad mean: 0.000013, Grad max: 0.064, Actions: 501, Entropy: 0.000\n",
      "Episode 285, Reward: -501.000, End reward: -501.000, Loss: 296.689, Grad mean: 0.000013, Grad max: 0.064, Actions: 501, Entropy: 0.000\n",
      "Episode 290, Reward: -501.000, End reward: -501.000, Loss: 306.905, Grad mean: 0.000013, Grad max: 0.063, Actions: 501, Entropy: 0.000\n",
      "Episode 295, Reward: -501.000, End reward: -501.000, Loss: 307.527, Grad mean: 0.000013, Grad max: 0.064, Actions: 501, Entropy: 0.000\n",
      "Episode 300, Reward: -501.000, End reward: -501.000, Loss: 299.279, Grad mean: 0.000014, Grad max: 0.063, Actions: 501, Entropy: 0.000\n",
      "Episode 305, Reward: -501.000, End reward: -501.000, Loss: 297.173, Grad mean: 0.000013, Grad max: 0.064, Actions: 501, Entropy: 0.000\n",
      "Episode 310, Reward: -501.000, End reward: -501.000, Loss: 299.242, Grad mean: 0.000013, Grad max: 0.064, Actions: 501, Entropy: 0.000\n",
      "Episode 315, Reward: -501.000, End reward: -501.000, Loss: 298.113, Grad mean: 0.000029, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 320, Reward: -501.000, End reward: -501.000, Loss: 301.303, Grad mean: 0.000012, Grad max: 0.064, Actions: 501, Entropy: 0.000\n",
      "Episode 325, Reward: -501.000, End reward: -501.000, Loss: 297.521, Grad mean: 0.000013, Grad max: 0.063, Actions: 501, Entropy: 0.000\n",
      "Episode 330, Reward: -501.000, End reward: -501.000, Loss: 303.400, Grad mean: 0.000013, Grad max: 0.064, Actions: 501, Entropy: 0.000\n",
      "Episode 335, Reward: -501.000, End reward: -501.000, Loss: 297.352, Grad mean: 0.000012, Grad max: 0.064, Actions: 501, Entropy: 0.000\n",
      "Episode 340, Reward: -501.000, End reward: -501.000, Loss: 301.773, Grad mean: 0.000012, Grad max: 0.064, Actions: 501, Entropy: 0.000\n",
      "Episode 345, Reward: -501.000, End reward: -501.000, Loss: 306.390, Grad mean: 0.000013, Grad max: 0.064, Actions: 501, Entropy: 0.000\n",
      "Episode 350, Reward: -501.000, End reward: -501.000, Loss: 298.237, Grad mean: 0.000013, Grad max: 0.064, Actions: 501, Entropy: 0.000\n",
      "Episode 355, Reward: -501.000, End reward: -501.000, Loss: 306.536, Grad mean: 0.000013, Grad max: 0.064, Actions: 501, Entropy: 0.000\n",
      "Episode 360, Reward: -501.000, End reward: -501.000, Loss: 308.269, Grad mean: 0.000013, Grad max: 0.065, Actions: 501, Entropy: 0.000\n",
      "Episode 365, Reward: -501.000, End reward: -501.000, Loss: 338.589, Grad mean: 0.000012, Grad max: 0.064, Actions: 501, Entropy: 0.000\n",
      "Episode 370, Reward: -501.000, End reward: -501.000, Loss: 301.503, Grad mean: 0.000012, Grad max: 0.065, Actions: 501, Entropy: 0.000\n",
      "Episode 375, Reward: -501.000, End reward: -501.000, Loss: 295.572, Grad mean: 0.000012, Grad max: 0.065, Actions: 501, Entropy: 0.000\n",
      "Episode 380, Reward: -501.000, End reward: -501.000, Loss: 297.564, Grad mean: 0.000012, Grad max: 0.065, Actions: 501, Entropy: 0.000\n",
      "Episode 385, Reward: -501.000, End reward: -501.000, Loss: 300.446, Grad mean: 0.000012, Grad max: 0.065, Actions: 501, Entropy: 0.000\n",
      "Episode 390, Reward: -501.000, End reward: -501.000, Loss: 308.444, Grad mean: 0.000012, Grad max: 0.065, Actions: 501, Entropy: 0.000\n",
      "Episode 395, Reward: -501.000, End reward: -501.000, Loss: 303.138, Grad mean: 0.000012, Grad max: 0.065, Actions: 501, Entropy: 0.000\n",
      "Episode 400, Reward: -501.000, End reward: -501.000, Loss: 299.086, Grad mean: 0.000012, Grad max: 0.065, Actions: 501, Entropy: 0.000\n",
      "Episode 405, Reward: -501.000, End reward: -501.000, Loss: 300.827, Grad mean: 0.000013, Grad max: 0.065, Actions: 501, Entropy: 0.000\n",
      "Episode 410, Reward: -501.000, End reward: -501.000, Loss: 296.575, Grad mean: 0.000013, Grad max: 0.065, Actions: 501, Entropy: 0.000\n",
      "Episode 415, Reward: -501.000, End reward: -501.000, Loss: 303.174, Grad mean: 0.000012, Grad max: 0.065, Actions: 501, Entropy: 0.000\n",
      "Episode 420, Reward: -501.000, End reward: -501.000, Loss: 309.952, Grad mean: 0.000012, Grad max: 0.066, Actions: 501, Entropy: 0.000\n",
      "Episode 425, Reward: -501.000, End reward: -501.000, Loss: 304.339, Grad mean: 0.000012, Grad max: 0.066, Actions: 501, Entropy: 0.000\n",
      "Episode 430, Reward: -501.000, End reward: -501.000, Loss: 300.934, Grad mean: 0.000012, Grad max: 0.066, Actions: 501, Entropy: 0.000\n",
      "Episode 435, Reward: -501.000, End reward: -501.000, Loss: 302.290, Grad mean: 0.000012, Grad max: 0.065, Actions: 501, Entropy: 0.000\n",
      "Episode 440, Reward: -501.000, End reward: -501.000, Loss: 305.194, Grad mean: 0.000012, Grad max: 0.065, Actions: 501, Entropy: 0.000\n",
      "Episode 445, Reward: -501.000, End reward: -501.000, Loss: 297.777, Grad mean: 0.000012, Grad max: 0.066, Actions: 501, Entropy: 0.000\n",
      "Episode 450, Reward: -501.000, End reward: -501.000, Loss: 299.097, Grad mean: 0.000012, Grad max: 0.066, Actions: 501, Entropy: 0.000\n",
      "Episode 455, Reward: -501.000, End reward: -501.000, Loss: 298.993, Grad mean: 0.000012, Grad max: 0.066, Actions: 501, Entropy: 0.000\n",
      "Episode 460, Reward: -501.000, End reward: -501.000, Loss: 300.517, Grad mean: 0.000012, Grad max: 0.066, Actions: 501, Entropy: 0.000\n",
      "Episode 465, Reward: -501.000, End reward: -501.000, Loss: 300.194, Grad mean: 0.000012, Grad max: 0.066, Actions: 501, Entropy: 0.000\n",
      "Episode 470, Reward: -501.000, End reward: -501.000, Loss: 295.328, Grad mean: 0.000013, Grad max: 0.066, Actions: 501, Entropy: 0.000\n",
      "Episode 475, Reward: -501.000, End reward: -501.000, Loss: 300.417, Grad mean: 0.000012, Grad max: 0.066, Actions: 501, Entropy: 0.000\n",
      "Episode 480, Reward: -501.000, End reward: -501.000, Loss: 306.796, Grad mean: 0.000012, Grad max: 0.066, Actions: 501, Entropy: 0.000\n",
      "Episode 485, Reward: -501.000, End reward: -501.000, Loss: 301.512, Grad mean: 0.000012, Grad max: 0.066, Actions: 501, Entropy: 0.000\n",
      "Episode 490, Reward: -501.000, End reward: -501.000, Loss: 295.465, Grad mean: 0.000012, Grad max: 0.066, Actions: 501, Entropy: 0.000\n",
      "Episode 495, Reward: -501.000, End reward: -501.000, Loss: 299.829, Grad mean: 0.000012, Grad max: 0.066, Actions: 501, Entropy: 0.000\n",
      "Episode 500, Reward: -501.000, End reward: -501.000, Loss: 302.765, Grad mean: 0.000012, Grad max: 0.067, Actions: 501, Entropy: 0.000\n",
      "Episode 505, Reward: -501.000, End reward: -501.000, Loss: 312.842, Grad mean: 0.000012, Grad max: 0.066, Actions: 501, Entropy: 0.000\n",
      "Episode 510, Reward: -501.000, End reward: -501.000, Loss: 295.587, Grad mean: 0.000012, Grad max: 0.067, Actions: 501, Entropy: 0.000\n",
      "Episode 515, Reward: -501.000, End reward: -501.000, Loss: 304.891, Grad mean: 0.000012, Grad max: 0.067, Actions: 501, Entropy: 0.000\n",
      "Episode 520, Reward: -501.000, End reward: -501.000, Loss: 306.768, Grad mean: 0.000012, Grad max: 0.067, Actions: 501, Entropy: 0.000\n",
      "Episode 525, Reward: -501.000, End reward: -501.000, Loss: 313.133, Grad mean: 0.000012, Grad max: 0.068, Actions: 501, Entropy: 0.000\n",
      "Episode 530, Reward: -501.000, End reward: -501.000, Loss: 297.519, Grad mean: 0.000012, Grad max: 0.067, Actions: 501, Entropy: 0.000\n",
      "Episode 535, Reward: -501.000, End reward: -501.000, Loss: 298.750, Grad mean: 0.000012, Grad max: 0.067, Actions: 501, Entropy: 0.000\n",
      "Episode 540, Reward: -501.000, End reward: -501.000, Loss: 318.824, Grad mean: 0.000012, Grad max: 0.068, Actions: 501, Entropy: 0.000\n",
      "Episode 545, Reward: -501.000, End reward: -501.000, Loss: 301.225, Grad mean: 0.000012, Grad max: 0.067, Actions: 501, Entropy: 0.000\n",
      "Episode 550, Reward: -501.000, End reward: -501.000, Loss: 302.811, Grad mean: 0.000012, Grad max: 0.068, Actions: 501, Entropy: 0.000\n",
      "Episode 555, Reward: -501.000, End reward: -501.000, Loss: 303.700, Grad mean: 0.000012, Grad max: 0.068, Actions: 501, Entropy: 0.000\n",
      "Episode 560, Reward: -501.000, End reward: -501.000, Loss: 305.208, Grad mean: 0.000012, Grad max: 0.068, Actions: 501, Entropy: 0.000\n",
      "Episode 565, Reward: -501.000, End reward: -501.000, Loss: 310.737, Grad mean: 0.000012, Grad max: 0.068, Actions: 501, Entropy: 0.000\n",
      "Episode 570, Reward: -501.000, End reward: -501.000, Loss: 301.472, Grad mean: 0.000011, Grad max: 0.068, Actions: 501, Entropy: 0.000\n",
      "Episode 575, Reward: -501.000, End reward: -501.000, Loss: 301.730, Grad mean: 0.000011, Grad max: 0.068, Actions: 501, Entropy: 0.000\n",
      "Episode 580, Reward: -501.000, End reward: -501.000, Loss: 302.002, Grad mean: 0.000012, Grad max: 0.069, Actions: 501, Entropy: 0.000\n",
      "Episode 585, Reward: -501.000, End reward: -501.000, Loss: 309.722, Grad mean: 0.000011, Grad max: 0.068, Actions: 501, Entropy: 0.000\n",
      "Episode 590, Reward: -501.000, End reward: -501.000, Loss: 298.316, Grad mean: 0.000011, Grad max: 0.069, Actions: 501, Entropy: 0.000\n",
      "Episode 595, Reward: -501.000, End reward: -501.000, Loss: 299.730, Grad mean: 0.000011, Grad max: 0.069, Actions: 501, Entropy: 0.000\n",
      "Episode 600, Reward: -501.000, End reward: -501.000, Loss: 298.981, Grad mean: 0.000011, Grad max: 0.069, Actions: 501, Entropy: 0.000\n",
      "Episode 605, Reward: -501.000, End reward: -501.000, Loss: 298.988, Grad mean: 0.000011, Grad max: 0.069, Actions: 501, Entropy: 0.000\n",
      "Episode 610, Reward: -501.000, End reward: -501.000, Loss: 299.546, Grad mean: 0.000011, Grad max: 0.069, Actions: 501, Entropy: 0.000\n",
      "Episode 615, Reward: -501.000, End reward: -501.000, Loss: 296.654, Grad mean: 0.000011, Grad max: 0.069, Actions: 501, Entropy: 0.000\n",
      "Episode 620, Reward: -501.000, End reward: -501.000, Loss: 309.639, Grad mean: 0.000012, Grad max: 0.068, Actions: 501, Entropy: 0.000\n",
      "Episode 625, Reward: -501.000, End reward: -501.000, Loss: 315.456, Grad mean: 0.000012, Grad max: 0.069, Actions: 501, Entropy: 0.000\n",
      "Episode 630, Reward: -501.000, End reward: -501.000, Loss: 297.714, Grad mean: 0.000012, Grad max: 0.069, Actions: 501, Entropy: 0.000\n",
      "Episode 635, Reward: -501.000, End reward: -501.000, Loss: 299.866, Grad mean: 0.000012, Grad max: 0.069, Actions: 501, Entropy: 0.000\n",
      "Episode 640, Reward: -501.000, End reward: -501.000, Loss: 296.124, Grad mean: 0.000011, Grad max: 0.069, Actions: 501, Entropy: 0.000\n",
      "Episode 645, Reward: -501.000, End reward: -501.000, Loss: 312.375, Grad mean: 0.000012, Grad max: 0.069, Actions: 501, Entropy: 0.000\n",
      "Episode 650, Reward: -501.000, End reward: -501.000, Loss: 308.299, Grad mean: 0.000011, Grad max: 0.069, Actions: 501, Entropy: 0.000\n",
      "Episode 655, Reward: -501.000, End reward: -501.000, Loss: 304.273, Grad mean: 0.000011, Grad max: 0.069, Actions: 501, Entropy: 0.000\n",
      "Episode 660, Reward: -501.000, End reward: -501.000, Loss: 302.957, Grad mean: 0.000011, Grad max: 0.070, Actions: 501, Entropy: 0.000\n",
      "Episode 665, Reward: -501.000, End reward: -501.000, Loss: 295.394, Grad mean: 0.000011, Grad max: 0.070, Actions: 501, Entropy: 0.000\n",
      "Episode 670, Reward: -501.000, End reward: -501.000, Loss: 297.160, Grad mean: 0.000011, Grad max: 0.070, Actions: 501, Entropy: 0.000\n",
      "Episode 675, Reward: -501.000, End reward: -501.000, Loss: 307.270, Grad mean: 0.000011, Grad max: 0.070, Actions: 501, Entropy: 0.000\n",
      "Episode 680, Reward: -501.000, End reward: -501.000, Loss: 309.842, Grad mean: 0.000011, Grad max: 0.069, Actions: 501, Entropy: 0.000\n",
      "Episode 685, Reward: -501.000, End reward: -501.000, Loss: 301.403, Grad mean: 0.000011, Grad max: 0.070, Actions: 501, Entropy: 0.000\n",
      "Episode 690, Reward: -501.000, End reward: -501.000, Loss: 303.630, Grad mean: 0.000011, Grad max: 0.070, Actions: 501, Entropy: 0.000\n",
      "Episode 695, Reward: -501.000, End reward: -501.000, Loss: 303.528, Grad mean: 0.000011, Grad max: 0.071, Actions: 501, Entropy: 0.000\n",
      "Episode 700, Reward: -501.000, End reward: -501.000, Loss: 300.835, Grad mean: 0.000014, Grad max: 0.068, Actions: 501, Entropy: 0.000\n",
      "Episode 705, Reward: -501.000, End reward: -501.000, Loss: 301.119, Grad mean: 0.000011, Grad max: 0.071, Actions: 501, Entropy: 0.000\n",
      "Episode 710, Reward: -501.000, End reward: -501.000, Loss: 299.230, Grad mean: 0.000011, Grad max: 0.071, Actions: 501, Entropy: 0.000\n",
      "Episode 715, Reward: -501.000, End reward: -501.000, Loss: 303.823, Grad mean: 0.000010, Grad max: 0.072, Actions: 501, Entropy: 0.000\n",
      "Episode 720, Reward: -501.000, End reward: -501.000, Loss: 298.225, Grad mean: 0.000011, Grad max: 0.071, Actions: 501, Entropy: 0.000\n",
      "Episode 725, Reward: -501.000, End reward: -501.000, Loss: 298.259, Grad mean: 0.000011, Grad max: 0.071, Actions: 501, Entropy: 0.000\n",
      "Episode 730, Reward: -501.000, End reward: -501.000, Loss: 308.668, Grad mean: 0.000010, Grad max: 0.071, Actions: 501, Entropy: 0.000\n",
      "Episode 735, Reward: -501.000, End reward: -501.000, Loss: 310.371, Grad mean: 0.000010, Grad max: 0.087, Actions: 501, Entropy: 0.000\n",
      "Episode 740, Reward: -501.000, End reward: -501.000, Loss: 301.500, Grad mean: 0.000010, Grad max: 0.072, Actions: 501, Entropy: 0.000\n",
      "Episode 745, Reward: -501.000, End reward: -501.000, Loss: 301.529, Grad mean: 0.000010, Grad max: 0.073, Actions: 501, Entropy: 0.000\n",
      "Episode 750, Reward: -501.000, End reward: -501.000, Loss: 297.584, Grad mean: 0.000011, Grad max: 0.072, Actions: 501, Entropy: 0.000\n",
      "Episode 755, Reward: -501.000, End reward: -501.000, Loss: 312.994, Grad mean: 0.000013, Grad max: 0.071, Actions: 501, Entropy: 0.000\n",
      "Episode 760, Reward: -501.000, End reward: -501.000, Loss: 298.209, Grad mean: 0.000011, Grad max: 0.071, Actions: 501, Entropy: 0.000\n",
      "Episode 765, Reward: -501.000, End reward: -501.000, Loss: 297.582, Grad mean: 0.000010, Grad max: 0.072, Actions: 501, Entropy: 0.000\n",
      "Episode 770, Reward: -501.000, End reward: -501.000, Loss: 303.808, Grad mean: 0.000013, Grad max: 0.071, Actions: 501, Entropy: 0.000\n",
      "Episode 775, Reward: -501.000, End reward: -501.000, Loss: 302.068, Grad mean: 0.000010, Grad max: 0.073, Actions: 501, Entropy: 0.000\n",
      "Episode 780, Reward: -501.000, End reward: -501.000, Loss: 301.454, Grad mean: 0.000010, Grad max: 0.074, Actions: 501, Entropy: 0.000\n",
      "Episode 785, Reward: -501.000, End reward: -501.000, Loss: 303.121, Grad mean: 0.000010, Grad max: 0.074, Actions: 501, Entropy: 0.000\n",
      "Episode 790, Reward: -501.000, End reward: -501.000, Loss: 298.811, Grad mean: 0.000010, Grad max: 0.074, Actions: 501, Entropy: 0.000\n",
      "Episode 795, Reward: -501.000, End reward: -501.000, Loss: 303.961, Grad mean: 0.000010, Grad max: 0.075, Actions: 501, Entropy: 0.000\n",
      "Episode 800, Reward: -501.000, End reward: -501.000, Loss: 297.170, Grad mean: 0.000010, Grad max: 0.074, Actions: 501, Entropy: 0.000\n",
      "Episode 805, Reward: -501.000, End reward: -501.000, Loss: 299.676, Grad mean: 0.000010, Grad max: 0.074, Actions: 501, Entropy: 0.000\n",
      "Episode 810, Reward: -501.000, End reward: -501.000, Loss: 297.845, Grad mean: 0.000015, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 815, Reward: -501.000, End reward: -501.000, Loss: 297.658, Grad mean: 0.000010, Grad max: 0.074, Actions: 501, Entropy: 0.000\n",
      "Episode 820, Reward: -501.000, End reward: -501.000, Loss: 297.566, Grad mean: 0.000009, Grad max: 0.073, Actions: 501, Entropy: 0.000\n",
      "Episode 825, Reward: -501.000, End reward: -501.000, Loss: 301.663, Grad mean: 0.000010, Grad max: 0.073, Actions: 501, Entropy: 0.000\n",
      "Episode 830, Reward: -501.000, End reward: -501.000, Loss: 303.179, Grad mean: 0.000010, Grad max: 0.074, Actions: 501, Entropy: 0.000\n",
      "Episode 835, Reward: -501.000, End reward: -501.000, Loss: 309.402, Grad mean: 0.000010, Grad max: 0.075, Actions: 501, Entropy: 0.000\n",
      "Episode 840, Reward: -501.000, End reward: -501.000, Loss: 301.902, Grad mean: 0.000009, Grad max: 0.074, Actions: 501, Entropy: 0.000\n",
      "Episode 845, Reward: -501.000, End reward: -501.000, Loss: 298.767, Grad mean: 0.000010, Grad max: 0.075, Actions: 501, Entropy: 0.000\n",
      "Episode 850, Reward: -501.000, End reward: -501.000, Loss: 298.782, Grad mean: 0.000010, Grad max: 0.075, Actions: 501, Entropy: 0.000\n",
      "Episode 855, Reward: -501.000, End reward: -501.000, Loss: 297.244, Grad mean: 0.000010, Grad max: 0.075, Actions: 501, Entropy: 0.000\n",
      "Episode 860, Reward: -501.000, End reward: -501.000, Loss: 297.978, Grad mean: 0.000010, Grad max: 0.075, Actions: 501, Entropy: 0.000\n",
      "Episode 865, Reward: -501.000, End reward: -501.000, Loss: 303.974, Grad mean: 0.000009, Grad max: 0.075, Actions: 501, Entropy: 0.000\n",
      "Episode 870, Reward: -501.000, End reward: -501.000, Loss: 297.824, Grad mean: 0.000010, Grad max: 0.070, Actions: 501, Entropy: 0.000\n",
      "Episode 875, Reward: -501.000, End reward: -501.000, Loss: 303.596, Grad mean: 0.000010, Grad max: 0.075, Actions: 501, Entropy: 0.000\n",
      "Episode 880, Reward: -501.000, End reward: -501.000, Loss: 306.367, Grad mean: 0.000009, Grad max: 0.075, Actions: 501, Entropy: 0.000\n",
      "Episode 885, Reward: -501.000, End reward: -501.000, Loss: 297.278, Grad mean: 0.000010, Grad max: 0.074, Actions: 501, Entropy: 0.000\n",
      "Episode 890, Reward: -501.000, End reward: -501.000, Loss: 300.656, Grad mean: 0.000009, Grad max: 0.075, Actions: 501, Entropy: 0.000\n",
      "Episode 895, Reward: -501.000, End reward: -501.000, Loss: 299.435, Grad mean: 0.000009, Grad max: 0.075, Actions: 501, Entropy: 0.000\n",
      "Episode 900, Reward: -501.000, End reward: -501.000, Loss: 297.863, Grad mean: 0.000009, Grad max: 0.075, Actions: 501, Entropy: 0.000\n",
      "Episode 905, Reward: -501.000, End reward: -501.000, Loss: 305.452, Grad mean: 0.000009, Grad max: 0.076, Actions: 501, Entropy: 0.000\n",
      "Episode 910, Reward: -501.000, End reward: -501.000, Loss: 316.577, Grad mean: 0.000009, Grad max: 0.075, Actions: 501, Entropy: 0.000\n",
      "Episode 915, Reward: -501.000, End reward: -501.000, Loss: 297.743, Grad mean: 0.000009, Grad max: 0.075, Actions: 501, Entropy: 0.000\n",
      "Episode 920, Reward: -501.000, End reward: -501.000, Loss: 306.565, Grad mean: 0.000009, Grad max: 0.076, Actions: 501, Entropy: 0.000\n",
      "Episode 925, Reward: -501.000, End reward: -501.000, Loss: 305.337, Grad mean: 0.000009, Grad max: 0.075, Actions: 501, Entropy: 0.000\n",
      "Episode 930, Reward: -501.000, End reward: -501.000, Loss: 298.806, Grad mean: 0.000009, Grad max: 0.076, Actions: 501, Entropy: 0.000\n",
      "Episode 935, Reward: -501.000, End reward: -501.000, Loss: 297.284, Grad mean: 0.000009, Grad max: 0.076, Actions: 501, Entropy: 0.000\n",
      "Episode 940, Reward: -501.000, End reward: -501.000, Loss: 295.751, Grad mean: 0.000010, Grad max: 0.076, Actions: 501, Entropy: 0.000\n",
      "Episode 945, Reward: -501.000, End reward: -501.000, Loss: 296.977, Grad mean: 0.000009, Grad max: 0.076, Actions: 501, Entropy: 0.000\n",
      "Episode 950, Reward: -501.000, End reward: -501.000, Loss: 305.128, Grad mean: 0.000009, Grad max: 0.076, Actions: 501, Entropy: 0.000\n",
      "Episode 955, Reward: -501.000, End reward: -501.000, Loss: 321.931, Grad mean: 0.000009, Grad max: 0.075, Actions: 501, Entropy: 0.000\n",
      "Episode 960, Reward: -501.000, End reward: -501.000, Loss: 296.773, Grad mean: 0.000010, Grad max: 0.076, Actions: 501, Entropy: 0.000\n",
      "Episode 965, Reward: -501.000, End reward: -501.000, Loss: 297.928, Grad mean: 0.000009, Grad max: 0.076, Actions: 501, Entropy: 0.000\n",
      "Episode 970, Reward: -501.000, End reward: -501.000, Loss: 301.838, Grad mean: 0.000009, Grad max: 0.076, Actions: 501, Entropy: 0.000\n",
      "Episode 975, Reward: -501.000, End reward: -501.000, Loss: 313.238, Grad mean: 0.000009, Grad max: 0.076, Actions: 501, Entropy: 0.000\n",
      "Episode 980, Reward: -501.000, End reward: -501.000, Loss: 323.464, Grad mean: 0.000009, Grad max: 0.077, Actions: 501, Entropy: 0.000\n",
      "Episode 985, Reward: -501.000, End reward: -501.000, Loss: 299.884, Grad mean: 0.000009, Grad max: 0.076, Actions: 501, Entropy: 0.000\n",
      "Episode 990, Reward: -501.000, End reward: -501.000, Loss: 302.292, Grad mean: 0.000009, Grad max: 0.077, Actions: 501, Entropy: 0.000\n",
      "Episode 995, Reward: -501.000, End reward: -501.000, Loss: 312.127, Grad mean: 0.000009, Grad max: 0.076, Actions: 501, Entropy: 0.000\n",
      "Episode 1000, Reward: -501.000, End reward: -501.000, Loss: 297.213, Grad mean: 0.000009, Grad max: 0.077, Actions: 501, Entropy: 0.000\n",
      "Episode 1005, Reward: -501.000, End reward: -501.000, Loss: 298.340, Grad mean: 0.000009, Grad max: 0.077, Actions: 501, Entropy: 0.000\n",
      "Episode 1010, Reward: -501.000, End reward: -501.000, Loss: 298.777, Grad mean: 0.000009, Grad max: 0.077, Actions: 501, Entropy: 0.000\n",
      "Episode 1015, Reward: -501.000, End reward: -501.000, Loss: 297.124, Grad mean: 0.000009, Grad max: 0.077, Actions: 501, Entropy: 0.000\n",
      "Episode 1020, Reward: -501.000, End reward: -501.000, Loss: 299.342, Grad mean: 0.000009, Grad max: 0.077, Actions: 501, Entropy: 0.000\n",
      "Episode 1025, Reward: -501.000, End reward: -501.000, Loss: 295.728, Grad mean: 0.000009, Grad max: 0.077, Actions: 501, Entropy: 0.000\n",
      "Episode 1030, Reward: -501.000, End reward: -501.000, Loss: 297.336, Grad mean: 0.000009, Grad max: 0.077, Actions: 501, Entropy: 0.000\n",
      "Episode 1035, Reward: -501.000, End reward: -501.000, Loss: 300.716, Grad mean: 0.000009, Grad max: 0.077, Actions: 501, Entropy: 0.000\n",
      "Episode 1040, Reward: -501.000, End reward: -501.000, Loss: 323.041, Grad mean: 0.000009, Grad max: 0.077, Actions: 501, Entropy: 0.000\n",
      "Episode 1045, Reward: -501.000, End reward: -501.000, Loss: 296.945, Grad mean: 0.000009, Grad max: 0.077, Actions: 501, Entropy: 0.000\n",
      "Episode 1050, Reward: -501.000, End reward: -501.000, Loss: 298.444, Grad mean: 0.000009, Grad max: 0.077, Actions: 501, Entropy: 0.000\n",
      "Episode 1055, Reward: -501.000, End reward: -501.000, Loss: 297.425, Grad mean: 0.000009, Grad max: 0.077, Actions: 501, Entropy: 0.000\n",
      "Episode 1060, Reward: -501.000, End reward: -501.000, Loss: 300.265, Grad mean: 0.000010, Grad max: 0.077, Actions: 501, Entropy: 0.000\n",
      "Episode 1065, Reward: -501.000, End reward: -501.000, Loss: 312.451, Grad mean: 0.000009, Grad max: 0.077, Actions: 501, Entropy: 0.000\n",
      "Episode 1070, Reward: -501.000, End reward: -501.000, Loss: 298.007, Grad mean: 0.000009, Grad max: 0.077, Actions: 501, Entropy: 0.000\n",
      "Episode 1075, Reward: -501.000, End reward: -501.000, Loss: 300.073, Grad mean: 0.000009, Grad max: 0.077, Actions: 501, Entropy: 0.000\n",
      "Episode 1080, Reward: -501.000, End reward: -501.000, Loss: 295.473, Grad mean: 0.000008, Grad max: 0.078, Actions: 501, Entropy: 0.000\n",
      "Episode 1085, Reward: -501.000, End reward: -501.000, Loss: 309.476, Grad mean: 0.000009, Grad max: 0.078, Actions: 501, Entropy: 0.000\n",
      "Episode 1090, Reward: -501.000, End reward: -501.000, Loss: 302.317, Grad mean: 0.000009, Grad max: 0.077, Actions: 501, Entropy: 0.000\n",
      "Episode 1095, Reward: -501.000, End reward: -501.000, Loss: 299.574, Grad mean: 0.000009, Grad max: 0.077, Actions: 501, Entropy: 0.000\n",
      "Episode 1100, Reward: -501.000, End reward: -501.000, Loss: 301.331, Grad mean: 0.000009, Grad max: 0.078, Actions: 501, Entropy: 0.000\n",
      "Episode 1105, Reward: -501.000, End reward: -501.000, Loss: 305.042, Grad mean: 0.000009, Grad max: 0.077, Actions: 501, Entropy: 0.000\n",
      "Episode 1110, Reward: -501.000, End reward: -501.000, Loss: 301.133, Grad mean: 0.000009, Grad max: 0.078, Actions: 501, Entropy: 0.000\n",
      "Episode 1115, Reward: -501.000, End reward: -501.000, Loss: 297.925, Grad mean: 0.000009, Grad max: 0.077, Actions: 501, Entropy: 0.000\n",
      "Episode 1120, Reward: -501.000, End reward: -501.000, Loss: 299.782, Grad mean: 0.000009, Grad max: 0.077, Actions: 501, Entropy: 0.000\n",
      "Episode 1125, Reward: -501.000, End reward: -501.000, Loss: 304.421, Grad mean: 0.000009, Grad max: 0.078, Actions: 501, Entropy: 0.000\n",
      "Episode 1130, Reward: -501.000, End reward: -501.000, Loss: 301.551, Grad mean: 0.000009, Grad max: 0.078, Actions: 501, Entropy: 0.000\n",
      "Episode 1135, Reward: -501.000, End reward: -501.000, Loss: 297.483, Grad mean: 0.000009, Grad max: 0.078, Actions: 501, Entropy: 0.000\n",
      "Episode 1140, Reward: -501.000, End reward: -501.000, Loss: 301.277, Grad mean: 0.000009, Grad max: 0.078, Actions: 501, Entropy: 0.000\n",
      "Episode 1145, Reward: -501.000, End reward: -501.000, Loss: 304.747, Grad mean: 0.000009, Grad max: 0.078, Actions: 501, Entropy: 0.000\n",
      "Episode 1150, Reward: -501.000, End reward: -501.000, Loss: 306.284, Grad mean: 0.000009, Grad max: 0.079, Actions: 501, Entropy: 0.000\n",
      "Episode 1155, Reward: -501.000, End reward: -501.000, Loss: 297.612, Grad mean: 0.000009, Grad max: 0.078, Actions: 501, Entropy: 0.000\n",
      "Episode 1160, Reward: -501.000, End reward: -501.000, Loss: 299.048, Grad mean: 0.000009, Grad max: 0.078, Actions: 501, Entropy: 0.000\n",
      "Episode 1165, Reward: -501.000, End reward: -501.000, Loss: 308.476, Grad mean: 0.000009, Grad max: 0.079, Actions: 501, Entropy: 0.000\n",
      "Episode 1170, Reward: -501.000, End reward: -501.000, Loss: 304.360, Grad mean: 0.000009, Grad max: 0.078, Actions: 501, Entropy: 0.000\n",
      "Episode 1175, Reward: -501.000, End reward: -501.000, Loss: 299.370, Grad mean: 0.000009, Grad max: 0.079, Actions: 501, Entropy: 0.000\n",
      "Episode 1180, Reward: -501.000, End reward: -501.000, Loss: 301.951, Grad mean: 0.000009, Grad max: 0.079, Actions: 501, Entropy: 0.000\n",
      "Episode 1185, Reward: -501.000, End reward: -501.000, Loss: 312.608, Grad mean: 0.000009, Grad max: 0.079, Actions: 501, Entropy: 0.000\n",
      "Episode 1190, Reward: -501.000, End reward: -501.000, Loss: 301.831, Grad mean: 0.000009, Grad max: 0.079, Actions: 501, Entropy: 0.000\n",
      "Episode 1195, Reward: -501.000, End reward: -501.000, Loss: 297.453, Grad mean: 0.000009, Grad max: 0.079, Actions: 501, Entropy: 0.000\n",
      "Episode 1200, Reward: -501.000, End reward: -501.000, Loss: 298.623, Grad mean: 0.000009, Grad max: 0.079, Actions: 501, Entropy: 0.000\n",
      "Episode 1205, Reward: -501.000, End reward: -501.000, Loss: 309.346, Grad mean: 0.000009, Grad max: 0.080, Actions: 501, Entropy: 0.000\n",
      "Episode 1210, Reward: -501.000, End reward: -501.000, Loss: 300.683, Grad mean: 0.000009, Grad max: 0.079, Actions: 501, Entropy: 0.000\n",
      "Episode 1215, Reward: -501.000, End reward: -501.000, Loss: 299.559, Grad mean: 0.000009, Grad max: 0.079, Actions: 501, Entropy: 0.000\n",
      "Episode 1220, Reward: -501.000, End reward: -501.000, Loss: 296.602, Grad mean: 0.000009, Grad max: 0.079, Actions: 501, Entropy: 0.000\n",
      "Episode 1225, Reward: -501.000, End reward: -501.000, Loss: 301.918, Grad mean: 0.000009, Grad max: 0.078, Actions: 501, Entropy: 0.000\n",
      "Episode 1230, Reward: -501.000, End reward: -501.000, Loss: 300.974, Grad mean: 0.000010, Grad max: 0.078, Actions: 501, Entropy: 0.000\n",
      "Episode 1235, Reward: -501.000, End reward: -501.000, Loss: 305.193, Grad mean: 0.000010, Grad max: 0.090, Actions: 501, Entropy: 0.000\n",
      "Episode 1240, Reward: -501.000, End reward: -501.000, Loss: 315.164, Grad mean: 0.000009, Grad max: 0.100, Actions: 501, Entropy: 0.000\n",
      "Episode 1245, Reward: -501.000, End reward: -501.000, Loss: 299.626, Grad mean: 0.000010, Grad max: 0.088, Actions: 501, Entropy: 0.000\n",
      "Episode 1250, Reward: -501.000, End reward: -501.000, Loss: 304.697, Grad mean: 0.000009, Grad max: 0.082, Actions: 501, Entropy: 0.000\n",
      "Episode 1255, Reward: -501.000, End reward: -501.000, Loss: 297.955, Grad mean: 0.000010, Grad max: 0.079, Actions: 501, Entropy: 0.000\n",
      "Episode 1260, Reward: -501.000, End reward: -501.000, Loss: 299.972, Grad mean: 0.000009, Grad max: 0.079, Actions: 501, Entropy: 0.000\n",
      "Episode 1265, Reward: -501.000, End reward: -501.000, Loss: 300.263, Grad mean: 0.000009, Grad max: 0.080, Actions: 501, Entropy: 0.000\n",
      "Episode 1270, Reward: -501.000, End reward: -501.000, Loss: 297.566, Grad mean: 0.000009, Grad max: 0.080, Actions: 501, Entropy: 0.000\n",
      "Episode 1275, Reward: -501.000, End reward: -501.000, Loss: 297.891, Grad mean: 0.000011, Grad max: 0.263, Actions: 501, Entropy: 0.000\n",
      "Episode 1280, Reward: -501.000, End reward: -501.000, Loss: 298.525, Grad mean: 0.000009, Grad max: 0.080, Actions: 501, Entropy: 0.000\n",
      "Episode 1285, Reward: -501.000, End reward: -501.000, Loss: 296.993, Grad mean: 0.000009, Grad max: 0.080, Actions: 501, Entropy: 0.000\n",
      "Episode 1290, Reward: -501.000, End reward: -501.000, Loss: 298.486, Grad mean: 0.000009, Grad max: 0.080, Actions: 501, Entropy: 0.000\n",
      "Episode 1295, Reward: -501.000, End reward: -501.000, Loss: 298.766, Grad mean: 0.000009, Grad max: 0.080, Actions: 501, Entropy: 0.000\n",
      "Episode 1300, Reward: -501.000, End reward: -501.000, Loss: 297.141, Grad mean: 0.000009, Grad max: 0.080, Actions: 501, Entropy: 0.000\n",
      "Episode 1305, Reward: -501.000, End reward: -501.000, Loss: 302.107, Grad mean: 0.000009, Grad max: 0.080, Actions: 501, Entropy: 0.000\n",
      "Episode 1310, Reward: -501.000, End reward: -501.000, Loss: 297.441, Grad mean: 0.000009, Grad max: 0.080, Actions: 501, Entropy: 0.000\n",
      "Episode 1315, Reward: -501.000, End reward: -501.000, Loss: 297.229, Grad mean: 0.000009, Grad max: 0.080, Actions: 501, Entropy: 0.000\n",
      "Episode 1320, Reward: -501.000, End reward: -501.000, Loss: 298.891, Grad mean: 0.000009, Grad max: 0.080, Actions: 501, Entropy: 0.000\n",
      "Episode 1325, Reward: -501.000, End reward: -501.000, Loss: 298.639, Grad mean: 0.000009, Grad max: 0.080, Actions: 501, Entropy: 0.000\n",
      "Episode 1330, Reward: -501.000, End reward: -501.000, Loss: 297.356, Grad mean: 0.000009, Grad max: 0.081, Actions: 501, Entropy: 0.000\n",
      "Episode 1335, Reward: -501.000, End reward: -501.000, Loss: 298.661, Grad mean: 0.000009, Grad max: 0.081, Actions: 501, Entropy: 0.000\n",
      "Episode 1340, Reward: -501.000, End reward: -501.000, Loss: 296.193, Grad mean: 0.000010, Grad max: 0.080, Actions: 501, Entropy: 0.000\n",
      "Episode 1345, Reward: -501.000, End reward: -501.000, Loss: 306.432, Grad mean: 0.000009, Grad max: 0.081, Actions: 501, Entropy: 0.000\n",
      "Episode 1350, Reward: -501.000, End reward: -501.000, Loss: 302.787, Grad mean: 0.000009, Grad max: 0.080, Actions: 501, Entropy: 0.000\n",
      "Episode 1355, Reward: -501.000, End reward: -501.000, Loss: 315.181, Grad mean: 0.000009, Grad max: 0.081, Actions: 501, Entropy: 0.000\n",
      "Episode 1360, Reward: -501.000, End reward: -501.000, Loss: 321.278, Grad mean: 0.000009, Grad max: 0.080, Actions: 501, Entropy: 0.000\n",
      "Episode 1365, Reward: -501.000, End reward: -501.000, Loss: 305.269, Grad mean: 0.000009, Grad max: 0.082, Actions: 501, Entropy: 0.000\n",
      "Episode 1370, Reward: -501.000, End reward: -501.000, Loss: 296.296, Grad mean: 0.000009, Grad max: 0.081, Actions: 501, Entropy: 0.000\n",
      "Episode 1375, Reward: -501.000, End reward: -501.000, Loss: 307.036, Grad mean: 0.000009, Grad max: 0.081, Actions: 501, Entropy: 0.000\n",
      "Episode 1380, Reward: -501.000, End reward: -501.000, Loss: 298.816, Grad mean: 0.000009, Grad max: 0.082, Actions: 501, Entropy: 0.000\n",
      "Episode 1385, Reward: -501.000, End reward: -501.000, Loss: 297.861, Grad mean: 0.000009, Grad max: 0.082, Actions: 501, Entropy: 0.000\n",
      "Episode 1390, Reward: -501.000, End reward: -501.000, Loss: 298.270, Grad mean: 0.000009, Grad max: 0.082, Actions: 501, Entropy: 0.000\n",
      "Episode 1395, Reward: -501.000, End reward: -501.000, Loss: 305.765, Grad mean: 0.000009, Grad max: 0.082, Actions: 501, Entropy: 0.000\n",
      "Episode 1400, Reward: -501.000, End reward: -501.000, Loss: 304.399, Grad mean: 0.000009, Grad max: 0.082, Actions: 501, Entropy: 0.000\n",
      "Episode 1405, Reward: -501.000, End reward: -501.000, Loss: 300.291, Grad mean: 0.000008, Grad max: 0.082, Actions: 501, Entropy: 0.000\n",
      "Episode 1410, Reward: -501.000, End reward: -501.000, Loss: 300.141, Grad mean: 0.000008, Grad max: 0.083, Actions: 501, Entropy: 0.000\n",
      "Episode 1415, Reward: -501.000, End reward: -501.000, Loss: 300.461, Grad mean: 0.000008, Grad max: 0.083, Actions: 501, Entropy: 0.000\n",
      "Episode 1420, Reward: -501.000, End reward: -501.000, Loss: 308.541, Grad mean: 0.000008, Grad max: 0.083, Actions: 501, Entropy: 0.000\n",
      "Episode 1425, Reward: -501.000, End reward: -501.000, Loss: 300.321, Grad mean: 0.000008, Grad max: 0.082, Actions: 501, Entropy: 0.000\n",
      "Episode 1430, Reward: -501.000, End reward: -501.000, Loss: 307.670, Grad mean: 0.000008, Grad max: 0.083, Actions: 501, Entropy: 0.000\n",
      "Episode 1435, Reward: -501.000, End reward: -501.000, Loss: 297.102, Grad mean: 0.000008, Grad max: 0.083, Actions: 501, Entropy: 0.000\n",
      "Episode 1440, Reward: -501.000, End reward: -501.000, Loss: 298.118, Grad mean: 0.000008, Grad max: 0.083, Actions: 501, Entropy: 0.000\n",
      "Episode 1445, Reward: -501.000, End reward: -501.000, Loss: 306.594, Grad mean: 0.000008, Grad max: 0.083, Actions: 501, Entropy: 0.000\n",
      "Episode 1450, Reward: -501.000, End reward: -501.000, Loss: 311.256, Grad mean: 0.000008, Grad max: 0.083, Actions: 501, Entropy: 0.000\n",
      "Episode 1455, Reward: -501.000, End reward: -501.000, Loss: 299.870, Grad mean: 0.000011, Grad max: 0.081, Actions: 501, Entropy: 0.000\n",
      "Episode 1460, Reward: -501.000, End reward: -501.000, Loss: 297.873, Grad mean: 0.000009, Grad max: 0.083, Actions: 501, Entropy: 0.000\n",
      "Episode 1465, Reward: -501.000, End reward: -501.000, Loss: 298.243, Grad mean: 0.000008, Grad max: 0.084, Actions: 501, Entropy: 0.000\n",
      "Episode 1470, Reward: -501.000, End reward: -501.000, Loss: 299.521, Grad mean: 0.000008, Grad max: 0.084, Actions: 501, Entropy: 0.000\n",
      "Episode 1475, Reward: -501.000, End reward: -501.000, Loss: 298.420, Grad mean: 0.000008, Grad max: 0.084, Actions: 501, Entropy: 0.000\n",
      "Episode 1480, Reward: -501.000, End reward: -501.000, Loss: 301.271, Grad mean: 0.000008, Grad max: 0.084, Actions: 501, Entropy: 0.000\n",
      "Episode 1485, Reward: -501.000, End reward: -501.000, Loss: 295.842, Grad mean: 0.000008, Grad max: 0.084, Actions: 501, Entropy: 0.000\n",
      "Episode 1490, Reward: -501.000, End reward: -501.000, Loss: 297.554, Grad mean: 0.000008, Grad max: 0.084, Actions: 501, Entropy: 0.000\n",
      "Episode 1495, Reward: -501.000, End reward: -501.000, Loss: 311.782, Grad mean: 0.000008, Grad max: 0.083, Actions: 501, Entropy: 0.000\n",
      "Episode 1500, Reward: -501.000, End reward: -501.000, Loss: 297.076, Grad mean: 0.000008, Grad max: 0.084, Actions: 501, Entropy: 0.000\n",
      "Episode 1505, Reward: -501.000, End reward: -501.000, Loss: 305.532, Grad mean: 0.000008, Grad max: 0.084, Actions: 501, Entropy: 0.000\n",
      "Episode 1510, Reward: -501.000, End reward: -501.000, Loss: 314.411, Grad mean: 0.000008, Grad max: 0.084, Actions: 501, Entropy: 0.000\n",
      "Episode 1515, Reward: -501.000, End reward: -501.000, Loss: 324.760, Grad mean: 0.000008, Grad max: 0.085, Actions: 501, Entropy: 0.000\n",
      "Episode 1520, Reward: -501.000, End reward: -501.000, Loss: 304.944, Grad mean: 0.000008, Grad max: 0.084, Actions: 501, Entropy: 0.000\n",
      "Episode 1525, Reward: -501.000, End reward: -501.000, Loss: 297.923, Grad mean: 0.000008, Grad max: 0.084, Actions: 501, Entropy: 0.000\n",
      "Episode 1530, Reward: -501.000, End reward: -501.000, Loss: 299.264, Grad mean: 0.000008, Grad max: 0.085, Actions: 501, Entropy: 0.000\n",
      "Episode 1535, Reward: -501.000, End reward: -501.000, Loss: 297.012, Grad mean: 0.000008, Grad max: 0.084, Actions: 501, Entropy: 0.000\n",
      "Episode 1540, Reward: -501.000, End reward: -501.000, Loss: 297.407, Grad mean: 0.000008, Grad max: 0.084, Actions: 501, Entropy: 0.000\n",
      "Episode 1545, Reward: -501.000, End reward: -501.000, Loss: 297.765, Grad mean: 0.000008, Grad max: 0.084, Actions: 501, Entropy: 0.000\n",
      "Episode 1550, Reward: -501.000, End reward: -501.000, Loss: 295.548, Grad mean: 0.000009, Grad max: 0.084, Actions: 501, Entropy: 0.000\n",
      "Episode 1555, Reward: -501.000, End reward: -501.000, Loss: 304.773, Grad mean: 0.000008, Grad max: 0.085, Actions: 501, Entropy: 0.000\n",
      "Episode 1560, Reward: -501.000, End reward: -501.000, Loss: 295.667, Grad mean: 0.000008, Grad max: 0.085, Actions: 501, Entropy: 0.000\n",
      "Episode 1565, Reward: -501.000, End reward: -501.000, Loss: 296.790, Grad mean: 0.000009, Grad max: 0.084, Actions: 501, Entropy: 0.000\n",
      "Episode 1570, Reward: -501.000, End reward: -501.000, Loss: 305.487, Grad mean: 0.000008, Grad max: 0.085, Actions: 501, Entropy: 0.000\n",
      "Episode 1575, Reward: -501.000, End reward: -501.000, Loss: 308.811, Grad mean: 0.000008, Grad max: 0.084, Actions: 501, Entropy: 0.000\n",
      "Episode 1580, Reward: -501.000, End reward: -501.000, Loss: 298.848, Grad mean: 0.000008, Grad max: 0.084, Actions: 501, Entropy: 0.000\n",
      "Episode 1585, Reward: -501.000, End reward: -501.000, Loss: 295.771, Grad mean: 0.000008, Grad max: 0.085, Actions: 501, Entropy: 0.000\n",
      "Episode 1590, Reward: -501.000, End reward: -501.000, Loss: 295.634, Grad mean: 0.000008, Grad max: 0.085, Actions: 501, Entropy: 0.000\n",
      "Episode 1595, Reward: -501.000, End reward: -501.000, Loss: 295.608, Grad mean: 0.000008, Grad max: 0.085, Actions: 501, Entropy: 0.000\n",
      "Episode 1600, Reward: -501.000, End reward: -501.000, Loss: 297.827, Grad mean: 0.000009, Grad max: 0.082, Actions: 501, Entropy: 0.000\n",
      "Episode 1605, Reward: -501.000, End reward: -501.000, Loss: 315.286, Grad mean: 0.000008, Grad max: 0.084, Actions: 501, Entropy: 0.000\n",
      "Episode 1610, Reward: -501.000, End reward: -501.000, Loss: 312.001, Grad mean: 0.000008, Grad max: 0.088, Actions: 501, Entropy: 0.000\n",
      "Episode 1615, Reward: -501.000, End reward: -501.000, Loss: 303.643, Grad mean: 0.000008, Grad max: 0.091, Actions: 501, Entropy: 0.000\n",
      "Episode 1620, Reward: -501.000, End reward: -501.000, Loss: 297.905, Grad mean: 0.000009, Grad max: 0.100, Actions: 501, Entropy: 0.000\n",
      "Episode 1625, Reward: -501.000, End reward: -501.000, Loss: 298.711, Grad mean: 0.000008, Grad max: 0.099, Actions: 501, Entropy: 0.000\n",
      "Episode 1630, Reward: -501.000, End reward: -501.000, Loss: 298.020, Grad mean: 0.000008, Grad max: 0.094, Actions: 501, Entropy: 0.000\n",
      "Episode 1635, Reward: -501.000, End reward: -501.000, Loss: 303.037, Grad mean: 0.000008, Grad max: 0.097, Actions: 501, Entropy: 0.000\n",
      "Episode 1640, Reward: -501.000, End reward: -501.000, Loss: 297.306, Grad mean: 0.000008, Grad max: 0.092, Actions: 501, Entropy: 0.000\n",
      "Episode 1645, Reward: -501.000, End reward: -501.000, Loss: 297.364, Grad mean: 0.000008, Grad max: 0.090, Actions: 501, Entropy: 0.000\n",
      "Episode 1650, Reward: -501.000, End reward: -501.000, Loss: 299.319, Grad mean: 0.000008, Grad max: 0.091, Actions: 501, Entropy: 0.000\n",
      "Episode 1655, Reward: -501.000, End reward: -501.000, Loss: 301.214, Grad mean: 0.000008, Grad max: 0.091, Actions: 501, Entropy: 0.000\n",
      "Episode 1660, Reward: -501.000, End reward: -501.000, Loss: 298.037, Grad mean: 0.000008, Grad max: 0.087, Actions: 501, Entropy: 0.000\n",
      "Episode 1665, Reward: -501.000, End reward: -501.000, Loss: 299.704, Grad mean: 0.000008, Grad max: 0.085, Actions: 501, Entropy: 0.000\n",
      "Episode 1670, Reward: -501.000, End reward: -501.000, Loss: 302.408, Grad mean: 0.000008, Grad max: 0.086, Actions: 501, Entropy: 0.000\n",
      "Episode 1675, Reward: -501.000, End reward: -501.000, Loss: 302.311, Grad mean: 0.000008, Grad max: 0.086, Actions: 501, Entropy: 0.000\n",
      "Episode 1680, Reward: -501.000, End reward: -501.000, Loss: 297.842, Grad mean: 0.000008, Grad max: 0.086, Actions: 501, Entropy: 0.000\n",
      "Episode 1685, Reward: -501.000, End reward: -501.000, Loss: 305.240, Grad mean: 0.000008, Grad max: 0.085, Actions: 501, Entropy: 0.000\n",
      "Episode 1690, Reward: -501.000, End reward: -501.000, Loss: 308.237, Grad mean: 0.000008, Grad max: 0.086, Actions: 501, Entropy: 0.000\n",
      "Episode 1695, Reward: -501.000, End reward: -501.000, Loss: 296.912, Grad mean: 0.000007, Grad max: 0.086, Actions: 501, Entropy: 0.000\n",
      "Episode 1700, Reward: -501.000, End reward: -501.000, Loss: 321.873, Grad mean: 0.000008, Grad max: 0.085, Actions: 501, Entropy: 0.000\n",
      "Episode 1705, Reward: -501.000, End reward: -501.000, Loss: 312.046, Grad mean: 0.000008, Grad max: 0.086, Actions: 501, Entropy: 0.000\n",
      "Episode 1710, Reward: -501.000, End reward: -501.000, Loss: 296.908, Grad mean: 0.000008, Grad max: 0.086, Actions: 501, Entropy: 0.000\n",
      "Episode 1715, Reward: -501.000, End reward: -501.000, Loss: 296.844, Grad mean: 0.000008, Grad max: 0.086, Actions: 501, Entropy: 0.000\n",
      "Episode 1720, Reward: -501.000, End reward: -501.000, Loss: 298.850, Grad mean: 0.000008, Grad max: 0.087, Actions: 501, Entropy: 0.000\n",
      "Episode 1725, Reward: -501.000, End reward: -501.000, Loss: 295.710, Grad mean: 0.000008, Grad max: 0.087, Actions: 501, Entropy: 0.000\n",
      "Episode 1730, Reward: -501.000, End reward: -501.000, Loss: 305.674, Grad mean: 0.000008, Grad max: 0.087, Actions: 501, Entropy: 0.000\n",
      "Episode 1735, Reward: -501.000, End reward: -501.000, Loss: 296.033, Grad mean: 0.000011, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 1740, Reward: -501.000, End reward: -501.000, Loss: 296.985, Grad mean: 0.000008, Grad max: 0.087, Actions: 501, Entropy: 0.000\n",
      "Episode 1745, Reward: -501.000, End reward: -501.000, Loss: 298.134, Grad mean: 0.000008, Grad max: 0.086, Actions: 501, Entropy: 0.000\n",
      "Episode 1750, Reward: -501.000, End reward: -501.000, Loss: 297.454, Grad mean: 0.000008, Grad max: 0.086, Actions: 501, Entropy: 0.000\n",
      "Episode 1755, Reward: -501.000, End reward: -501.000, Loss: 295.788, Grad mean: 0.000008, Grad max: 0.086, Actions: 501, Entropy: 0.000\n",
      "Episode 1760, Reward: -501.000, End reward: -501.000, Loss: 300.399, Grad mean: 0.000008, Grad max: 0.087, Actions: 501, Entropy: 0.000\n",
      "Episode 1765, Reward: -501.000, End reward: -501.000, Loss: 311.236, Grad mean: 0.000008, Grad max: 0.087, Actions: 501, Entropy: 0.000\n",
      "Episode 1770, Reward: -501.000, End reward: -501.000, Loss: 297.583, Grad mean: 0.000010, Grad max: 0.128, Actions: 501, Entropy: 0.000\n",
      "Episode 1775, Reward: -501.000, End reward: -501.000, Loss: 305.587, Grad mean: 0.000008, Grad max: 0.087, Actions: 501, Entropy: 0.000\n",
      "Episode 1780, Reward: -501.000, End reward: -501.000, Loss: 297.779, Grad mean: 0.000008, Grad max: 0.087, Actions: 501, Entropy: 0.000\n",
      "Episode 1785, Reward: -501.000, End reward: -501.000, Loss: 297.097, Grad mean: 0.000008, Grad max: 0.087, Actions: 501, Entropy: 0.000\n",
      "Episode 1790, Reward: -501.000, End reward: -501.000, Loss: 300.441, Grad mean: 0.000008, Grad max: 0.087, Actions: 501, Entropy: 0.000\n",
      "Episode 1795, Reward: -501.000, End reward: -501.000, Loss: 301.717, Grad mean: 0.000008, Grad max: 0.087, Actions: 501, Entropy: 0.000\n",
      "Episode 1800, Reward: -501.000, End reward: -501.000, Loss: 303.810, Grad mean: 0.000008, Grad max: 0.088, Actions: 501, Entropy: 0.000\n",
      "Episode 1805, Reward: -501.000, End reward: -501.000, Loss: 303.099, Grad mean: 0.000008, Grad max: 0.088, Actions: 501, Entropy: 0.000\n",
      "Episode 1810, Reward: -501.000, End reward: -501.000, Loss: 298.279, Grad mean: 0.000008, Grad max: 0.087, Actions: 501, Entropy: 0.000\n",
      "Episode 1815, Reward: -501.000, End reward: -501.000, Loss: 300.186, Grad mean: 0.000008, Grad max: 0.088, Actions: 501, Entropy: 0.000\n",
      "Episode 1820, Reward: -501.000, End reward: -501.000, Loss: 303.646, Grad mean: 0.000008, Grad max: 0.088, Actions: 501, Entropy: 0.000\n",
      "Episode 1825, Reward: -501.000, End reward: -501.000, Loss: 296.103, Grad mean: 0.000008, Grad max: 0.088, Actions: 501, Entropy: 0.000\n",
      "Episode 1830, Reward: -501.000, End reward: -501.000, Loss: 296.307, Grad mean: 0.000008, Grad max: 0.088, Actions: 501, Entropy: 0.000\n",
      "Episode 1835, Reward: -501.000, End reward: -501.000, Loss: 305.972, Grad mean: 0.000008, Grad max: 0.088, Actions: 501, Entropy: 0.000\n",
      "Episode 1840, Reward: -501.000, End reward: -501.000, Loss: 300.905, Grad mean: 0.000008, Grad max: 0.089, Actions: 501, Entropy: 0.000\n",
      "Episode 1845, Reward: -501.000, End reward: -501.000, Loss: 297.472, Grad mean: 0.000008, Grad max: 0.088, Actions: 501, Entropy: 0.000\n",
      "Episode 1850, Reward: -501.000, End reward: -501.000, Loss: 299.255, Grad mean: 0.000008, Grad max: 0.088, Actions: 501, Entropy: 0.000\n",
      "Episode 1855, Reward: -501.000, End reward: -501.000, Loss: 304.644, Grad mean: 0.000008, Grad max: 0.089, Actions: 501, Entropy: 0.000\n",
      "Episode 1860, Reward: -501.000, End reward: -501.000, Loss: 301.892, Grad mean: 0.000008, Grad max: 0.088, Actions: 501, Entropy: 0.000\n",
      "Episode 1865, Reward: -501.000, End reward: -501.000, Loss: 298.549, Grad mean: 0.000008, Grad max: 0.089, Actions: 501, Entropy: 0.000\n",
      "Episode 1870, Reward: -501.000, End reward: -501.000, Loss: 298.489, Grad mean: 0.000008, Grad max: 0.089, Actions: 501, Entropy: 0.000\n",
      "Episode 1875, Reward: -501.000, End reward: -501.000, Loss: 314.047, Grad mean: 0.000008, Grad max: 0.089, Actions: 501, Entropy: 0.000\n",
      "Episode 1880, Reward: -501.000, End reward: -501.000, Loss: 302.960, Grad mean: 0.000008, Grad max: 0.089, Actions: 501, Entropy: 0.000\n",
      "Episode 1885, Reward: -501.000, End reward: -501.000, Loss: 297.615, Grad mean: 0.000008, Grad max: 0.089, Actions: 501, Entropy: 0.000\n",
      "Episode 1890, Reward: -501.000, End reward: -501.000, Loss: 301.564, Grad mean: 0.000008, Grad max: 0.089, Actions: 501, Entropy: 0.000\n",
      "Episode 1895, Reward: -501.000, End reward: -501.000, Loss: 312.618, Grad mean: 0.000008, Grad max: 0.090, Actions: 501, Entropy: 0.000\n",
      "Episode 1900, Reward: -501.000, End reward: -501.000, Loss: 298.333, Grad mean: 0.000007, Grad max: 0.090, Actions: 501, Entropy: 0.000\n",
      "Episode 1905, Reward: -501.000, End reward: -501.000, Loss: 322.197, Grad mean: 0.000008, Grad max: 0.089, Actions: 501, Entropy: 0.000\n",
      "Episode 1910, Reward: -501.000, End reward: -501.000, Loss: 312.762, Grad mean: 0.000007, Grad max: 0.091, Actions: 501, Entropy: 0.000\n",
      "Episode 1915, Reward: -501.000, End reward: -501.000, Loss: 299.534, Grad mean: 0.000007, Grad max: 0.090, Actions: 501, Entropy: 0.000\n",
      "Episode 1920, Reward: -501.000, End reward: -501.000, Loss: 301.578, Grad mean: 0.000007, Grad max: 0.090, Actions: 501, Entropy: 0.000\n",
      "Episode 1925, Reward: -501.000, End reward: -501.000, Loss: 298.597, Grad mean: 0.000007, Grad max: 0.091, Actions: 501, Entropy: 0.000\n",
      "Episode 1930, Reward: -501.000, End reward: -501.000, Loss: 298.308, Grad mean: 0.000008, Grad max: 0.090, Actions: 501, Entropy: 0.000\n",
      "Episode 1935, Reward: -501.000, End reward: -501.000, Loss: 295.439, Grad mean: 0.000007, Grad max: 0.091, Actions: 501, Entropy: 0.000\n",
      "Episode 1940, Reward: -501.000, End reward: -501.000, Loss: 304.317, Grad mean: 0.000007, Grad max: 0.091, Actions: 501, Entropy: 0.000\n",
      "Episode 1945, Reward: -501.000, End reward: -501.000, Loss: 318.533, Grad mean: 0.000007, Grad max: 0.090, Actions: 501, Entropy: 0.000\n",
      "Episode 1950, Reward: -501.000, End reward: -501.000, Loss: 306.619, Grad mean: 0.000007, Grad max: 0.092, Actions: 501, Entropy: 0.000\n",
      "Episode 1955, Reward: -501.000, End reward: -501.000, Loss: 307.999, Grad mean: 0.000007, Grad max: 0.091, Actions: 501, Entropy: 0.000\n",
      "Episode 1960, Reward: -501.000, End reward: -501.000, Loss: 318.695, Grad mean: 0.000007, Grad max: 0.092, Actions: 501, Entropy: 0.000\n",
      "Episode 1965, Reward: -501.000, End reward: -501.000, Loss: 301.841, Grad mean: 0.000007, Grad max: 0.091, Actions: 501, Entropy: 0.000\n",
      "Episode 1970, Reward: -501.000, End reward: -501.000, Loss: 297.207, Grad mean: 0.000017, Grad max: 0.161, Actions: 501, Entropy: 0.000\n",
      "Episode 1975, Reward: -501.000, End reward: -501.000, Loss: 300.658, Grad mean: 0.000007, Grad max: 0.093, Actions: 501, Entropy: 0.000\n",
      "Episode 1980, Reward: -501.000, End reward: -501.000, Loss: 301.250, Grad mean: 0.000007, Grad max: 0.093, Actions: 501, Entropy: 0.000\n",
      "Episode 1985, Reward: -501.000, End reward: -501.000, Loss: 299.902, Grad mean: 0.000010, Grad max: 0.129, Actions: 501, Entropy: 0.000\n",
      "Episode 1990, Reward: -501.000, End reward: -501.000, Loss: 298.968, Grad mean: 0.000013, Grad max: 0.414, Actions: 501, Entropy: 0.000\n",
      "Episode 1995, Reward: -501.000, End reward: -501.000, Loss: 297.904, Grad mean: 0.000007, Grad max: 0.092, Actions: 501, Entropy: 0.000\n",
      "Episode 2000, Reward: -501.000, End reward: -501.000, Loss: 298.453, Grad mean: 0.000007, Grad max: 0.092, Actions: 501, Entropy: 0.000\n",
      "Episode 2005, Reward: -501.000, End reward: -501.000, Loss: 302.699, Grad mean: 0.000007, Grad max: 0.090, Actions: 501, Entropy: 0.000\n",
      "Episode 2010, Reward: -501.000, End reward: -501.000, Loss: 300.007, Grad mean: 0.000007, Grad max: 0.087, Actions: 501, Entropy: 0.000\n",
      "Episode 2015, Reward: -501.000, End reward: -501.000, Loss: 297.443, Grad mean: 0.000007, Grad max: 0.086, Actions: 501, Entropy: 0.000\n",
      "Episode 2020, Reward: -501.000, End reward: -501.000, Loss: 297.979, Grad mean: 0.000007, Grad max: 0.093, Actions: 501, Entropy: 0.000\n",
      "Episode 2025, Reward: -501.000, End reward: -501.000, Loss: 302.422, Grad mean: 0.000007, Grad max: 0.094, Actions: 501, Entropy: 0.000\n",
      "Episode 2030, Reward: -501.000, End reward: -501.000, Loss: 301.925, Grad mean: 0.000007, Grad max: 0.093, Actions: 501, Entropy: 0.000\n",
      "Episode 2035, Reward: -501.000, End reward: -501.000, Loss: 296.679, Grad mean: 0.000007, Grad max: 0.094, Actions: 501, Entropy: 0.000\n",
      "Episode 2040, Reward: -501.000, End reward: -501.000, Loss: 297.411, Grad mean: 0.000007, Grad max: 0.094, Actions: 501, Entropy: 0.000\n",
      "Episode 2045, Reward: -501.000, End reward: -501.000, Loss: 300.582, Grad mean: 0.000007, Grad max: 0.094, Actions: 501, Entropy: 0.000\n",
      "Episode 2050, Reward: -501.000, End reward: -501.000, Loss: 298.680, Grad mean: 0.000007, Grad max: 0.094, Actions: 501, Entropy: 0.000\n",
      "Episode 2055, Reward: -501.000, End reward: -501.000, Loss: 297.099, Grad mean: 0.000007, Grad max: 0.094, Actions: 501, Entropy: 0.000\n",
      "Episode 2060, Reward: -501.000, End reward: -501.000, Loss: 301.499, Grad mean: 0.000007, Grad max: 0.094, Actions: 501, Entropy: 0.000\n",
      "Episode 2065, Reward: -501.000, End reward: -501.000, Loss: 297.344, Grad mean: 0.000007, Grad max: 0.094, Actions: 501, Entropy: 0.000\n",
      "Episode 2070, Reward: -501.000, End reward: -501.000, Loss: 295.501, Grad mean: 0.000007, Grad max: 0.094, Actions: 501, Entropy: 0.000\n",
      "Episode 2075, Reward: -501.000, End reward: -501.000, Loss: 302.030, Grad mean: 0.000007, Grad max: 0.094, Actions: 501, Entropy: 0.000\n",
      "Episode 2080, Reward: -501.000, End reward: -501.000, Loss: 296.791, Grad mean: 0.000007, Grad max: 0.094, Actions: 501, Entropy: 0.000\n",
      "Episode 2085, Reward: -501.000, End reward: -501.000, Loss: 299.109, Grad mean: 0.000007, Grad max: 0.094, Actions: 501, Entropy: 0.000\n",
      "Episode 2090, Reward: -501.000, End reward: -501.000, Loss: 307.862, Grad mean: 0.000007, Grad max: 0.095, Actions: 501, Entropy: 0.000\n",
      "Episode 2095, Reward: -501.000, End reward: -501.000, Loss: 296.126, Grad mean: 0.000007, Grad max: 0.095, Actions: 501, Entropy: 0.000\n",
      "Episode 2100, Reward: -501.000, End reward: -501.000, Loss: 303.474, Grad mean: 0.000007, Grad max: 0.095, Actions: 501, Entropy: 0.000\n",
      "Episode 2105, Reward: -501.000, End reward: -501.000, Loss: 301.286, Grad mean: 0.000007, Grad max: 0.095, Actions: 501, Entropy: 0.000\n",
      "Episode 2110, Reward: -501.000, End reward: -501.000, Loss: 296.329, Grad mean: 0.000007, Grad max: 0.095, Actions: 501, Entropy: 0.000\n",
      "Episode 2115, Reward: -501.000, End reward: -501.000, Loss: 300.027, Grad mean: 0.000007, Grad max: 0.095, Actions: 501, Entropy: 0.000\n",
      "Episode 2120, Reward: -501.000, End reward: -501.000, Loss: 299.295, Grad mean: 0.000007, Grad max: 0.096, Actions: 501, Entropy: 0.000\n",
      "Episode 2125, Reward: -501.000, End reward: -501.000, Loss: 302.898, Grad mean: 0.000007, Grad max: 0.095, Actions: 501, Entropy: 0.000\n",
      "Episode 2130, Reward: -501.000, End reward: -501.000, Loss: 302.314, Grad mean: 0.000006, Grad max: 0.096, Actions: 501, Entropy: 0.000\n",
      "Episode 2135, Reward: -501.000, End reward: -501.000, Loss: 296.964, Grad mean: 0.000007, Grad max: 0.095, Actions: 501, Entropy: 0.000\n",
      "Episode 2140, Reward: -501.000, End reward: -501.000, Loss: 298.940, Grad mean: 0.000007, Grad max: 0.095, Actions: 501, Entropy: 0.000\n",
      "Episode 2145, Reward: -501.000, End reward: -501.000, Loss: 304.219, Grad mean: 0.000006, Grad max: 0.096, Actions: 501, Entropy: 0.000\n",
      "Episode 2150, Reward: -501.000, End reward: -501.000, Loss: 306.453, Grad mean: 0.000007, Grad max: 0.095, Actions: 501, Entropy: 0.000\n",
      "Episode 2155, Reward: -501.000, End reward: -501.000, Loss: 298.616, Grad mean: 0.000006, Grad max: 0.096, Actions: 501, Entropy: 0.000\n",
      "Episode 2160, Reward: -501.000, End reward: -501.000, Loss: 301.430, Grad mean: 0.000006, Grad max: 0.096, Actions: 501, Entropy: 0.000\n",
      "Episode 2165, Reward: -501.000, End reward: -501.000, Loss: 300.690, Grad mean: 0.000007, Grad max: 0.096, Actions: 501, Entropy: 0.000\n",
      "Episode 2170, Reward: -501.000, End reward: -501.000, Loss: 303.403, Grad mean: 0.000006, Grad max: 0.096, Actions: 501, Entropy: 0.000\n",
      "Episode 2175, Reward: -501.000, End reward: -501.000, Loss: 297.670, Grad mean: 0.000006, Grad max: 0.096, Actions: 501, Entropy: 0.000\n",
      "Episode 2180, Reward: -501.000, End reward: -501.000, Loss: 298.473, Grad mean: 0.000006, Grad max: 0.096, Actions: 501, Entropy: 0.000\n",
      "Episode 2185, Reward: -501.000, End reward: -501.000, Loss: 298.156, Grad mean: 0.000006, Grad max: 0.096, Actions: 501, Entropy: 0.000\n",
      "Episode 2190, Reward: -501.000, End reward: -501.000, Loss: 300.399, Grad mean: 0.000007, Grad max: 0.096, Actions: 501, Entropy: 0.000\n",
      "Episode 2195, Reward: -501.000, End reward: -501.000, Loss: 300.965, Grad mean: 0.000007, Grad max: 0.096, Actions: 501, Entropy: 0.000\n",
      "Episode 2200, Reward: -501.000, End reward: -501.000, Loss: 299.766, Grad mean: 0.000007, Grad max: 0.096, Actions: 501, Entropy: 0.000\n",
      "Episode 2205, Reward: -501.000, End reward: -501.000, Loss: 297.267, Grad mean: 0.000007, Grad max: 0.096, Actions: 501, Entropy: 0.000\n",
      "Episode 2210, Reward: -501.000, End reward: -501.000, Loss: 296.346, Grad mean: 0.000007, Grad max: 0.096, Actions: 501, Entropy: 0.000\n",
      "Episode 2215, Reward: -501.000, End reward: -501.000, Loss: 300.836, Grad mean: 0.000007, Grad max: 0.096, Actions: 501, Entropy: 0.000\n",
      "Episode 2220, Reward: -501.000, End reward: -501.000, Loss: 296.303, Grad mean: 0.000008, Grad max: 0.096, Actions: 501, Entropy: 0.000\n",
      "Episode 2225, Reward: -501.000, End reward: -501.000, Loss: 295.196, Grad mean: 0.000007, Grad max: 0.096, Actions: 501, Entropy: 0.000\n",
      "Episode 2230, Reward: -501.000, End reward: -501.000, Loss: 303.729, Grad mean: 0.000007, Grad max: 0.096, Actions: 501, Entropy: 0.000\n",
      "Episode 2235, Reward: -501.000, End reward: -501.000, Loss: 305.650, Grad mean: 0.000007, Grad max: 0.095, Actions: 501, Entropy: 0.000\n",
      "Episode 2240, Reward: -501.000, End reward: -501.000, Loss: 298.652, Grad mean: 0.000007, Grad max: 0.096, Actions: 501, Entropy: 0.000\n",
      "Episode 2245, Reward: -501.000, End reward: -501.000, Loss: 298.103, Grad mean: 0.000007, Grad max: 0.096, Actions: 501, Entropy: 0.000\n",
      "Episode 2250, Reward: -501.000, End reward: -501.000, Loss: 298.659, Grad mean: 0.000007, Grad max: 0.096, Actions: 501, Entropy: 0.000\n",
      "Episode 2255, Reward: -501.000, End reward: -501.000, Loss: 297.422, Grad mean: 0.000006, Grad max: 0.096, Actions: 501, Entropy: 0.000\n",
      "Episode 2260, Reward: -501.000, End reward: -501.000, Loss: 306.517, Grad mean: 0.000007, Grad max: 0.097, Actions: 501, Entropy: 0.000\n",
      "Episode 2265, Reward: -501.000, End reward: -501.000, Loss: 300.406, Grad mean: 0.000007, Grad max: 0.096, Actions: 501, Entropy: 0.000\n",
      "Episode 2270, Reward: -501.000, End reward: -501.000, Loss: 298.025, Grad mean: 0.000007, Grad max: 0.097, Actions: 501, Entropy: 0.000\n",
      "Episode 2275, Reward: -501.000, End reward: -501.000, Loss: 301.769, Grad mean: 0.000007, Grad max: 0.096, Actions: 501, Entropy: 0.000\n",
      "Episode 2280, Reward: -501.000, End reward: -501.000, Loss: 301.030, Grad mean: 0.000007, Grad max: 0.097, Actions: 501, Entropy: 0.000\n",
      "Episode 2285, Reward: -501.000, End reward: -501.000, Loss: 301.081, Grad mean: 0.000007, Grad max: 0.096, Actions: 501, Entropy: 0.000\n",
      "Episode 2290, Reward: -501.000, End reward: -501.000, Loss: 299.832, Grad mean: 0.000007, Grad max: 0.097, Actions: 501, Entropy: 0.000\n",
      "Episode 2295, Reward: -501.000, End reward: -501.000, Loss: 300.332, Grad mean: 0.000007, Grad max: 0.097, Actions: 501, Entropy: 0.000\n",
      "Episode 2300, Reward: -501.000, End reward: -501.000, Loss: 298.370, Grad mean: 0.000007, Grad max: 0.097, Actions: 501, Entropy: 0.000\n",
      "Episode 2305, Reward: -501.000, End reward: -501.000, Loss: 303.591, Grad mean: 0.000007, Grad max: 0.097, Actions: 501, Entropy: 0.000\n",
      "Episode 2310, Reward: -501.000, End reward: -501.000, Loss: 300.544, Grad mean: 0.000007, Grad max: 0.098, Actions: 501, Entropy: 0.000\n",
      "Episode 2315, Reward: -501.000, End reward: -501.000, Loss: 298.100, Grad mean: 0.000007, Grad max: 0.097, Actions: 501, Entropy: 0.000\n",
      "Episode 2320, Reward: -501.000, End reward: -501.000, Loss: 302.032, Grad mean: 0.000007, Grad max: 0.098, Actions: 501, Entropy: 0.000\n",
      "Episode 2325, Reward: -501.000, End reward: -501.000, Loss: 302.603, Grad mean: 0.000007, Grad max: 0.097, Actions: 501, Entropy: 0.000\n",
      "Episode 2330, Reward: -501.000, End reward: -501.000, Loss: 297.365, Grad mean: 0.000007, Grad max: 0.098, Actions: 501, Entropy: 0.000\n",
      "Episode 2335, Reward: -501.000, End reward: -501.000, Loss: 303.131, Grad mean: 0.000007, Grad max: 0.097, Actions: 501, Entropy: 0.000\n",
      "Episode 2340, Reward: -501.000, End reward: -501.000, Loss: 302.358, Grad mean: 0.000007, Grad max: 0.098, Actions: 501, Entropy: 0.000\n",
      "Episode 2345, Reward: -501.000, End reward: -501.000, Loss: 297.577, Grad mean: 0.000007, Grad max: 0.098, Actions: 501, Entropy: 0.000\n",
      "Episode 2350, Reward: -501.000, End reward: -501.000, Loss: 298.012, Grad mean: 0.000007, Grad max: 0.098, Actions: 501, Entropy: 0.000\n",
      "Episode 2355, Reward: -501.000, End reward: -501.000, Loss: 298.094, Grad mean: 0.000006, Grad max: 0.099, Actions: 501, Entropy: 0.000\n",
      "Episode 2360, Reward: -501.000, End reward: -501.000, Loss: 301.995, Grad mean: 0.000006, Grad max: 0.099, Actions: 501, Entropy: 0.000\n",
      "Episode 2365, Reward: -501.000, End reward: -501.000, Loss: 301.802, Grad mean: 0.000006, Grad max: 0.100, Actions: 501, Entropy: 0.000\n",
      "Episode 2370, Reward: -501.000, End reward: -501.000, Loss: 300.978, Grad mean: 0.000006, Grad max: 0.099, Actions: 501, Entropy: 0.000\n",
      "Episode 2375, Reward: -501.000, End reward: -501.000, Loss: 297.178, Grad mean: 0.000006, Grad max: 0.100, Actions: 501, Entropy: 0.000\n",
      "Episode 2380, Reward: -501.000, End reward: -501.000, Loss: 300.769, Grad mean: 0.000006, Grad max: 0.100, Actions: 501, Entropy: 0.000\n",
      "Episode 2385, Reward: -501.000, End reward: -501.000, Loss: 301.326, Grad mean: 0.000006, Grad max: 0.099, Actions: 501, Entropy: 0.000\n",
      "Episode 2390, Reward: -501.000, End reward: -501.000, Loss: 304.871, Grad mean: 0.000006, Grad max: 0.100, Actions: 501, Entropy: 0.000\n",
      "Episode 2395, Reward: -501.000, End reward: -501.000, Loss: 297.687, Grad mean: 0.000009, Grad max: 0.159, Actions: 501, Entropy: 0.000\n",
      "Episode 2400, Reward: -501.000, End reward: -501.000, Loss: 298.725, Grad mean: 0.000006, Grad max: 0.099, Actions: 501, Entropy: 0.000\n",
      "Episode 2405, Reward: -501.000, End reward: -501.000, Loss: 299.353, Grad mean: 0.000006, Grad max: 0.099, Actions: 501, Entropy: 0.000\n",
      "Episode 2410, Reward: -501.000, End reward: -501.000, Loss: 298.335, Grad mean: 0.000013, Grad max: 0.230, Actions: 501, Entropy: 0.000\n",
      "Episode 2415, Reward: -501.000, End reward: -501.000, Loss: 297.753, Grad mean: 0.000006, Grad max: 0.100, Actions: 501, Entropy: 0.000\n",
      "Episode 2420, Reward: -501.000, End reward: -501.000, Loss: 295.565, Grad mean: 0.000006, Grad max: 0.100, Actions: 501, Entropy: 0.000\n",
      "Episode 2425, Reward: -501.000, End reward: -501.000, Loss: 298.372, Grad mean: 0.000006, Grad max: 0.100, Actions: 501, Entropy: 0.000\n",
      "Episode 2430, Reward: -501.000, End reward: -501.000, Loss: 302.038, Grad mean: 0.000006, Grad max: 0.100, Actions: 501, Entropy: 0.000\n",
      "Episode 2435, Reward: -501.000, End reward: -501.000, Loss: 298.381, Grad mean: 0.000006, Grad max: 0.100, Actions: 501, Entropy: 0.000\n",
      "Episode 2440, Reward: -501.000, End reward: -501.000, Loss: 299.746, Grad mean: 0.000006, Grad max: 0.100, Actions: 501, Entropy: 0.000\n",
      "Episode 2445, Reward: -501.000, End reward: -501.000, Loss: 299.112, Grad mean: 0.000006, Grad max: 0.101, Actions: 501, Entropy: 0.000\n",
      "Episode 2450, Reward: -501.000, End reward: -501.000, Loss: 309.083, Grad mean: 0.000006, Grad max: 0.100, Actions: 501, Entropy: 0.000\n",
      "Episode 2455, Reward: -501.000, End reward: -501.000, Loss: 318.470, Grad mean: 0.000006, Grad max: 0.101, Actions: 501, Entropy: 0.000\n",
      "Episode 2460, Reward: -501.000, End reward: -501.000, Loss: 300.444, Grad mean: 0.000006, Grad max: 0.100, Actions: 501, Entropy: 0.000\n",
      "Episode 2465, Reward: -501.000, End reward: -501.000, Loss: 295.783, Grad mean: 0.000006, Grad max: 0.100, Actions: 501, Entropy: 0.000\n",
      "Episode 2470, Reward: -501.000, End reward: -501.000, Loss: 297.728, Grad mean: 0.000006, Grad max: 0.101, Actions: 501, Entropy: 0.000\n",
      "Episode 2475, Reward: -501.000, End reward: -501.000, Loss: 297.894, Grad mean: 0.000006, Grad max: 0.101, Actions: 501, Entropy: 0.000\n",
      "Episode 2480, Reward: -501.000, End reward: -501.000, Loss: 301.359, Grad mean: 0.000006, Grad max: 0.101, Actions: 501, Entropy: 0.000\n",
      "Episode 2485, Reward: -501.000, End reward: -501.000, Loss: 304.910, Grad mean: 0.000006, Grad max: 0.100, Actions: 501, Entropy: 0.000\n",
      "Episode 2490, Reward: -501.000, End reward: -501.000, Loss: 298.471, Grad mean: 0.000006, Grad max: 0.101, Actions: 501, Entropy: 0.000\n",
      "Episode 2495, Reward: -501.000, End reward: -501.000, Loss: 297.791, Grad mean: 0.000006, Grad max: 0.100, Actions: 501, Entropy: 0.000\n",
      "Episode 2500, Reward: -501.000, End reward: -501.000, Loss: 301.451, Grad mean: 0.000006, Grad max: 0.101, Actions: 501, Entropy: 0.000\n",
      "Episode 2505, Reward: -501.000, End reward: -501.000, Loss: 300.146, Grad mean: 0.000006, Grad max: 0.100, Actions: 501, Entropy: 0.000\n",
      "Episode 2510, Reward: -501.000, End reward: -501.000, Loss: 297.738, Grad mean: 0.000007, Grad max: 0.100, Actions: 501, Entropy: 0.000\n",
      "Episode 2515, Reward: -501.000, End reward: -501.000, Loss: 297.645, Grad mean: 0.000006, Grad max: 0.100, Actions: 501, Entropy: 0.000\n",
      "Episode 2520, Reward: -501.000, End reward: -501.000, Loss: 296.731, Grad mean: 0.000006, Grad max: 0.100, Actions: 501, Entropy: 0.000\n",
      "Episode 2525, Reward: -501.000, End reward: -501.000, Loss: 297.280, Grad mean: 0.000007, Grad max: 0.100, Actions: 501, Entropy: 0.000\n",
      "Episode 2530, Reward: -501.000, End reward: -501.000, Loss: 297.817, Grad mean: 0.000007, Grad max: 0.100, Actions: 501, Entropy: 0.000\n",
      "Episode 2535, Reward: -501.000, End reward: -501.000, Loss: 297.364, Grad mean: 0.000007, Grad max: 0.100, Actions: 501, Entropy: 0.000\n",
      "Episode 2540, Reward: -501.000, End reward: -501.000, Loss: 297.880, Grad mean: 0.000006, Grad max: 0.100, Actions: 501, Entropy: 0.000\n",
      "Episode 2545, Reward: -501.000, End reward: -501.000, Loss: 303.677, Grad mean: 0.000006, Grad max: 0.100, Actions: 501, Entropy: 0.000\n",
      "Episode 2550, Reward: -501.000, End reward: -501.000, Loss: 308.899, Grad mean: 0.000006, Grad max: 0.101, Actions: 501, Entropy: 0.000\n",
      "Episode 2555, Reward: -501.000, End reward: -501.000, Loss: 299.309, Grad mean: 0.000006, Grad max: 0.101, Actions: 501, Entropy: 0.000\n",
      "Episode 2560, Reward: -501.000, End reward: -501.000, Loss: 297.016, Grad mean: 0.000006, Grad max: 0.101, Actions: 501, Entropy: 0.000\n",
      "Episode 2565, Reward: -501.000, End reward: -501.000, Loss: 297.547, Grad mean: 0.000006, Grad max: 0.101, Actions: 501, Entropy: 0.000\n",
      "Episode 2570, Reward: -501.000, End reward: -501.000, Loss: 305.241, Grad mean: 0.000006, Grad max: 0.100, Actions: 501, Entropy: 0.000\n",
      "Episode 2575, Reward: -501.000, End reward: -501.000, Loss: 301.777, Grad mean: 0.000006, Grad max: 0.100, Actions: 501, Entropy: 0.000\n",
      "Episode 2580, Reward: -501.000, End reward: -501.000, Loss: 300.197, Grad mean: 0.000006, Grad max: 0.100, Actions: 501, Entropy: 0.000\n",
      "Episode 2585, Reward: -501.000, End reward: -501.000, Loss: 298.431, Grad mean: 0.000006, Grad max: 0.100, Actions: 501, Entropy: 0.000\n",
      "Episode 2590, Reward: -501.000, End reward: -501.000, Loss: 299.137, Grad mean: 0.000006, Grad max: 0.100, Actions: 501, Entropy: 0.000\n",
      "Episode 2595, Reward: -501.000, End reward: -501.000, Loss: 306.233, Grad mean: 0.000006, Grad max: 0.101, Actions: 501, Entropy: 0.000\n",
      "Episode 2600, Reward: -501.000, End reward: -501.000, Loss: 297.544, Grad mean: 0.000006, Grad max: 0.101, Actions: 501, Entropy: 0.000\n",
      "Episode 2605, Reward: -501.000, End reward: -501.000, Loss: 297.154, Grad mean: 0.000006, Grad max: 0.101, Actions: 501, Entropy: 0.000\n",
      "Episode 2610, Reward: -501.000, End reward: -501.000, Loss: 295.957, Grad mean: 0.000006, Grad max: 0.102, Actions: 501, Entropy: 0.000\n",
      "Episode 2615, Reward: -501.000, End reward: -501.000, Loss: 300.088, Grad mean: 0.000006, Grad max: 0.102, Actions: 501, Entropy: 0.000\n",
      "Episode 2620, Reward: -501.000, End reward: -501.000, Loss: 298.401, Grad mean: 0.000006, Grad max: 0.102, Actions: 501, Entropy: 0.000\n",
      "Episode 2625, Reward: -501.000, End reward: -501.000, Loss: 298.596, Grad mean: 0.000006, Grad max: 0.101, Actions: 501, Entropy: 0.000\n",
      "Episode 2630, Reward: -501.000, End reward: -501.000, Loss: 297.445, Grad mean: 0.000006, Grad max: 0.102, Actions: 501, Entropy: 0.000\n",
      "Episode 2635, Reward: -501.000, End reward: -501.000, Loss: 297.715, Grad mean: 0.000006, Grad max: 0.102, Actions: 501, Entropy: 0.000\n",
      "Episode 2640, Reward: -501.000, End reward: -501.000, Loss: 298.703, Grad mean: 0.000007, Grad max: 0.100, Actions: 501, Entropy: 0.000\n",
      "Episode 2645, Reward: -501.000, End reward: -501.000, Loss: 297.236, Grad mean: 0.000006, Grad max: 0.099, Actions: 501, Entropy: 0.000\n",
      "Episode 2650, Reward: -501.000, End reward: -501.000, Loss: 297.216, Grad mean: 0.000006, Grad max: 0.102, Actions: 501, Entropy: 0.000\n",
      "Episode 2655, Reward: -501.000, End reward: -501.000, Loss: 297.489, Grad mean: 0.000006, Grad max: 0.102, Actions: 501, Entropy: 0.000\n",
      "Episode 2660, Reward: -501.000, End reward: -501.000, Loss: 297.552, Grad mean: 0.000006, Grad max: 0.102, Actions: 501, Entropy: 0.000\n",
      "Episode 2665, Reward: -501.000, End reward: -501.000, Loss: 299.297, Grad mean: 0.000006, Grad max: 0.102, Actions: 501, Entropy: 0.000\n",
      "Episode 2670, Reward: -501.000, End reward: -501.000, Loss: 306.667, Grad mean: 0.000006, Grad max: 0.103, Actions: 501, Entropy: 0.000\n",
      "Episode 2675, Reward: -501.000, End reward: -501.000, Loss: 299.516, Grad mean: 0.000006, Grad max: 0.103, Actions: 501, Entropy: 0.000\n",
      "Episode 2680, Reward: -501.000, End reward: -501.000, Loss: 299.471, Grad mean: 0.000006, Grad max: 0.102, Actions: 501, Entropy: 0.000\n",
      "Episode 2685, Reward: -501.000, End reward: -501.000, Loss: 302.104, Grad mean: 0.000006, Grad max: 0.103, Actions: 501, Entropy: 0.000\n",
      "Episode 2690, Reward: -501.000, End reward: -501.000, Loss: 299.620, Grad mean: 0.000006, Grad max: 0.103, Actions: 501, Entropy: 0.000\n",
      "Episode 2695, Reward: -501.000, End reward: -501.000, Loss: 295.466, Grad mean: 0.000006, Grad max: 0.102, Actions: 501, Entropy: 0.000\n",
      "Episode 2700, Reward: -501.000, End reward: -501.000, Loss: 297.724, Grad mean: 0.000006, Grad max: 0.103, Actions: 501, Entropy: 0.000\n",
      "Episode 2705, Reward: -501.000, End reward: -501.000, Loss: 300.464, Grad mean: 0.000006, Grad max: 0.103, Actions: 501, Entropy: 0.000\n",
      "Episode 2710, Reward: -501.000, End reward: -501.000, Loss: 297.151, Grad mean: 0.000006, Grad max: 0.103, Actions: 501, Entropy: 0.000\n",
      "Episode 2715, Reward: -501.000, End reward: -501.000, Loss: 297.590, Grad mean: 0.000006, Grad max: 0.103, Actions: 501, Entropy: 0.000\n",
      "Episode 2720, Reward: -501.000, End reward: -501.000, Loss: 299.365, Grad mean: 0.000007, Grad max: 0.102, Actions: 501, Entropy: 0.000\n",
      "Episode 2725, Reward: -501.000, End reward: -501.000, Loss: 297.132, Grad mean: 0.000006, Grad max: 0.103, Actions: 501, Entropy: 0.000\n",
      "Episode 2730, Reward: -501.000, End reward: -501.000, Loss: 297.695, Grad mean: 0.000006, Grad max: 0.103, Actions: 501, Entropy: 0.000\n",
      "Episode 2735, Reward: -501.000, End reward: -501.000, Loss: 301.845, Grad mean: 0.000006, Grad max: 0.103, Actions: 501, Entropy: 0.000\n",
      "Episode 2740, Reward: -501.000, End reward: -501.000, Loss: 298.404, Grad mean: 0.000006, Grad max: 0.103, Actions: 501, Entropy: 0.000\n",
      "Episode 2745, Reward: -501.000, End reward: -501.000, Loss: 295.649, Grad mean: 0.000006, Grad max: 0.103, Actions: 501, Entropy: 0.000\n",
      "Episode 2750, Reward: -501.000, End reward: -501.000, Loss: 297.247, Grad mean: 0.000006, Grad max: 0.104, Actions: 501, Entropy: 0.000\n",
      "Episode 2755, Reward: -501.000, End reward: -501.000, Loss: 298.036, Grad mean: 0.000006, Grad max: 0.104, Actions: 501, Entropy: 0.000\n",
      "Episode 2760, Reward: -501.000, End reward: -501.000, Loss: 299.537, Grad mean: 0.000006, Grad max: 0.105, Actions: 501, Entropy: 0.000\n",
      "Episode 2765, Reward: -501.000, End reward: -501.000, Loss: 296.237, Grad mean: 0.000006, Grad max: 0.105, Actions: 501, Entropy: 0.000\n",
      "Episode 2770, Reward: -501.000, End reward: -501.000, Loss: 302.122, Grad mean: 0.000006, Grad max: 0.105, Actions: 501, Entropy: 0.000\n",
      "Episode 2775, Reward: -501.000, End reward: -501.000, Loss: 297.728, Grad mean: 0.000006, Grad max: 0.105, Actions: 501, Entropy: 0.000\n",
      "Episode 2780, Reward: -501.000, End reward: -501.000, Loss: 297.293, Grad mean: 0.000006, Grad max: 0.105, Actions: 501, Entropy: 0.000\n",
      "Episode 2785, Reward: -501.000, End reward: -501.000, Loss: 297.148, Grad mean: 0.000006, Grad max: 0.103, Actions: 501, Entropy: 0.000\n",
      "Episode 2790, Reward: -501.000, End reward: -501.000, Loss: 294.916, Grad mean: 0.000006, Grad max: 0.105, Actions: 501, Entropy: 0.000\n",
      "Episode 2795, Reward: -501.000, End reward: -501.000, Loss: 296.400, Grad mean: 0.000006, Grad max: 0.105, Actions: 501, Entropy: 0.000\n",
      "Episode 2800, Reward: -501.000, End reward: -501.000, Loss: 295.672, Grad mean: 0.000006, Grad max: 0.105, Actions: 501, Entropy: 0.000\n",
      "Episode 2805, Reward: -501.000, End reward: -501.000, Loss: 298.615, Grad mean: 0.000006, Grad max: 0.105, Actions: 501, Entropy: 0.000\n",
      "Episode 2810, Reward: -501.000, End reward: -501.000, Loss: 301.535, Grad mean: 0.000006, Grad max: 0.104, Actions: 501, Entropy: 0.000\n",
      "Episode 2815, Reward: -501.000, End reward: -501.000, Loss: 301.874, Grad mean: 0.000006, Grad max: 0.105, Actions: 501, Entropy: 0.000\n",
      "Episode 2820, Reward: -501.000, End reward: -501.000, Loss: 324.250, Grad mean: 0.000006, Grad max: 0.104, Actions: 501, Entropy: 0.000\n",
      "Episode 2825, Reward: -501.000, End reward: -501.000, Loss: 297.457, Grad mean: 0.000006, Grad max: 0.105, Actions: 501, Entropy: 0.000\n",
      "Episode 2830, Reward: -501.000, End reward: -501.000, Loss: 299.297, Grad mean: 0.000006, Grad max: 0.105, Actions: 501, Entropy: 0.000\n",
      "Episode 2835, Reward: -501.000, End reward: -501.000, Loss: 301.120, Grad mean: 0.000006, Grad max: 0.104, Actions: 501, Entropy: 0.000\n",
      "Episode 2840, Reward: -501.000, End reward: -501.000, Loss: 301.958, Grad mean: 0.000006, Grad max: 0.105, Actions: 501, Entropy: 0.000\n",
      "Episode 2845, Reward: -501.000, End reward: -501.000, Loss: 297.559, Grad mean: 0.000006, Grad max: 0.105, Actions: 501, Entropy: 0.000\n",
      "Episode 2850, Reward: -501.000, End reward: -501.000, Loss: 298.435, Grad mean: 0.000006, Grad max: 0.105, Actions: 501, Entropy: 0.000\n",
      "Episode 2855, Reward: -501.000, End reward: -501.000, Loss: 296.134, Grad mean: 0.000006, Grad max: 0.106, Actions: 501, Entropy: 0.000\n",
      "Episode 2860, Reward: -501.000, End reward: -501.000, Loss: 297.050, Grad mean: 0.000006, Grad max: 0.106, Actions: 501, Entropy: 0.000\n",
      "Episode 2865, Reward: -501.000, End reward: -501.000, Loss: 296.242, Grad mean: 0.000006, Grad max: 0.106, Actions: 501, Entropy: 0.000\n",
      "Episode 2870, Reward: -501.000, End reward: -501.000, Loss: 295.766, Grad mean: 0.000006, Grad max: 0.105, Actions: 501, Entropy: 0.000\n",
      "Episode 2875, Reward: -501.000, End reward: -501.000, Loss: 300.021, Grad mean: 0.000006, Grad max: 0.106, Actions: 501, Entropy: 0.000\n",
      "Episode 2880, Reward: -501.000, End reward: -501.000, Loss: 303.840, Grad mean: 0.000006, Grad max: 0.106, Actions: 501, Entropy: 0.000\n",
      "Episode 2885, Reward: -501.000, End reward: -501.000, Loss: 296.167, Grad mean: 0.000006, Grad max: 0.106, Actions: 501, Entropy: 0.000\n",
      "Episode 2890, Reward: -501.000, End reward: -501.000, Loss: 300.330, Grad mean: 0.000006, Grad max: 0.106, Actions: 501, Entropy: 0.000\n",
      "Episode 2895, Reward: -501.000, End reward: -501.000, Loss: 297.518, Grad mean: 0.000006, Grad max: 0.106, Actions: 501, Entropy: 0.000\n",
      "Episode 2900, Reward: -501.000, End reward: -501.000, Loss: 298.372, Grad mean: 0.000006, Grad max: 0.106, Actions: 501, Entropy: 0.000\n",
      "Episode 2905, Reward: -501.000, End reward: -501.000, Loss: 297.800, Grad mean: 0.000006, Grad max: 0.106, Actions: 501, Entropy: 0.000\n",
      "Episode 2910, Reward: -501.000, End reward: -501.000, Loss: 299.199, Grad mean: 0.000006, Grad max: 0.106, Actions: 501, Entropy: 0.000\n",
      "Episode 2915, Reward: -501.000, End reward: -501.000, Loss: 299.071, Grad mean: 0.000006, Grad max: 0.106, Actions: 501, Entropy: 0.000\n",
      "Episode 2920, Reward: -501.000, End reward: -501.000, Loss: 301.009, Grad mean: 0.000006, Grad max: 0.106, Actions: 501, Entropy: 0.000\n",
      "Episode 2925, Reward: -501.000, End reward: -501.000, Loss: 301.438, Grad mean: 0.000006, Grad max: 0.107, Actions: 501, Entropy: 0.000\n",
      "Episode 2930, Reward: -501.000, End reward: -501.000, Loss: 301.500, Grad mean: 0.000006, Grad max: 0.106, Actions: 501, Entropy: 0.000\n",
      "Episode 2935, Reward: -501.000, End reward: -501.000, Loss: 298.618, Grad mean: 0.000006, Grad max: 0.106, Actions: 501, Entropy: 0.000\n",
      "Episode 2940, Reward: -501.000, End reward: -501.000, Loss: 300.267, Grad mean: 0.000006, Grad max: 0.107, Actions: 501, Entropy: 0.000\n",
      "Episode 2945, Reward: -501.000, End reward: -501.000, Loss: 300.451, Grad mean: 0.000006, Grad max: 0.106, Actions: 501, Entropy: 0.000\n",
      "Episode 2950, Reward: -501.000, End reward: -501.000, Loss: 299.943, Grad mean: 0.000006, Grad max: 0.107, Actions: 501, Entropy: 0.000\n",
      "Episode 2955, Reward: -501.000, End reward: -501.000, Loss: 297.405, Grad mean: 0.000006, Grad max: 0.106, Actions: 501, Entropy: 0.000\n",
      "Episode 2960, Reward: -501.000, End reward: -501.000, Loss: 296.440, Grad mean: 0.000006, Grad max: 0.106, Actions: 501, Entropy: 0.000\n",
      "Episode 2965, Reward: -501.000, End reward: -501.000, Loss: 306.015, Grad mean: 0.000006, Grad max: 0.107, Actions: 501, Entropy: 0.000\n",
      "Episode 2970, Reward: -501.000, End reward: -501.000, Loss: 297.854, Grad mean: 0.000006, Grad max: 0.106, Actions: 501, Entropy: 0.000\n",
      "Episode 2975, Reward: -501.000, End reward: -501.000, Loss: 301.503, Grad mean: 0.000006, Grad max: 0.107, Actions: 501, Entropy: 0.000\n",
      "Episode 2980, Reward: -501.000, End reward: -501.000, Loss: 304.619, Grad mean: 0.000006, Grad max: 0.106, Actions: 501, Entropy: 0.000\n",
      "Episode 2985, Reward: -501.000, End reward: -501.000, Loss: 307.652, Grad mean: 0.000006, Grad max: 0.107, Actions: 501, Entropy: 0.000\n",
      "Episode 2990, Reward: -501.000, End reward: -501.000, Loss: 300.246, Grad mean: 0.000006, Grad max: 0.106, Actions: 501, Entropy: 0.000\n",
      "Episode 2995, Reward: -501.000, End reward: -501.000, Loss: 305.278, Grad mean: 0.000006, Grad max: 0.107, Actions: 501, Entropy: 0.000\n",
      "Episode 3000, Reward: -501.000, End reward: -501.000, Loss: 306.740, Grad mean: 0.000006, Grad max: 0.106, Actions: 501, Entropy: 0.000\n",
      "Episode 3005, Reward: -501.000, End reward: -501.000, Loss: 298.237, Grad mean: 0.000006, Grad max: 0.107, Actions: 501, Entropy: 0.000\n",
      "Episode 3010, Reward: -501.000, End reward: -501.000, Loss: 298.710, Grad mean: 0.000006, Grad max: 0.107, Actions: 501, Entropy: 0.000\n",
      "Episode 3015, Reward: -501.000, End reward: -501.000, Loss: 299.127, Grad mean: 0.000006, Grad max: 0.107, Actions: 501, Entropy: 0.000\n",
      "Episode 3020, Reward: -501.000, End reward: -501.000, Loss: 298.438, Grad mean: 0.000006, Grad max: 0.107, Actions: 501, Entropy: 0.000\n",
      "Episode 3025, Reward: -501.000, End reward: -501.000, Loss: 301.514, Grad mean: 0.000006, Grad max: 0.108, Actions: 501, Entropy: 0.000\n",
      "Episode 3030, Reward: -501.000, End reward: -501.000, Loss: 297.256, Grad mean: 0.000006, Grad max: 0.107, Actions: 501, Entropy: 0.000\n",
      "Episode 3035, Reward: -501.000, End reward: -501.000, Loss: 298.682, Grad mean: 0.000006, Grad max: 0.107, Actions: 501, Entropy: 0.000\n",
      "Episode 3040, Reward: -501.000, End reward: -501.000, Loss: 298.010, Grad mean: 0.000006, Grad max: 0.107, Actions: 501, Entropy: 0.000\n",
      "Episode 3045, Reward: -501.000, End reward: -501.000, Loss: 298.108, Grad mean: 0.000006, Grad max: 0.108, Actions: 501, Entropy: 0.000\n",
      "Episode 3050, Reward: -501.000, End reward: -501.000, Loss: 297.452, Grad mean: 0.000006, Grad max: 0.108, Actions: 501, Entropy: 0.000\n",
      "Episode 3055, Reward: -501.000, End reward: -501.000, Loss: 297.353, Grad mean: 0.000006, Grad max: 0.108, Actions: 501, Entropy: 0.000\n",
      "Episode 3060, Reward: -501.000, End reward: -501.000, Loss: 298.397, Grad mean: 0.000006, Grad max: 0.107, Actions: 501, Entropy: 0.000\n",
      "Episode 3065, Reward: -501.000, End reward: -501.000, Loss: 298.917, Grad mean: 0.000006, Grad max: 0.107, Actions: 501, Entropy: 0.000\n",
      "Episode 3070, Reward: -501.000, End reward: -501.000, Loss: 298.299, Grad mean: 0.000006, Grad max: 0.108, Actions: 501, Entropy: 0.000\n",
      "Episode 3075, Reward: -501.000, End reward: -501.000, Loss: 300.101, Grad mean: 0.000006, Grad max: 0.107, Actions: 501, Entropy: 0.000\n",
      "Episode 3080, Reward: -501.000, End reward: -501.000, Loss: 297.286, Grad mean: 0.000006, Grad max: 0.107, Actions: 501, Entropy: 0.000\n",
      "Episode 3085, Reward: -501.000, End reward: -501.000, Loss: 298.383, Grad mean: 0.000006, Grad max: 0.108, Actions: 501, Entropy: 0.000\n",
      "Episode 3090, Reward: -501.000, End reward: -501.000, Loss: 296.524, Grad mean: 0.000006, Grad max: 0.108, Actions: 501, Entropy: 0.000\n",
      "Episode 3095, Reward: -501.000, End reward: -501.000, Loss: 298.652, Grad mean: 0.000006, Grad max: 0.108, Actions: 501, Entropy: 0.000\n",
      "Episode 3100, Reward: -501.000, End reward: -501.000, Loss: 295.837, Grad mean: 0.000006, Grad max: 0.107, Actions: 501, Entropy: 0.000\n",
      "Episode 3105, Reward: -501.000, End reward: -501.000, Loss: 298.171, Grad mean: 0.000006, Grad max: 0.107, Actions: 501, Entropy: 0.000\n",
      "Episode 3110, Reward: -501.000, End reward: -501.000, Loss: 299.120, Grad mean: 0.000006, Grad max: 0.108, Actions: 501, Entropy: 0.000\n",
      "Episode 3115, Reward: -501.000, End reward: -501.000, Loss: 310.456, Grad mean: 0.000006, Grad max: 0.109, Actions: 501, Entropy: 0.000\n",
      "Episode 3120, Reward: -501.000, End reward: -501.000, Loss: 314.447, Grad mean: 0.000006, Grad max: 0.108, Actions: 501, Entropy: 0.000\n",
      "Episode 3125, Reward: -501.000, End reward: -501.000, Loss: 301.415, Grad mean: 0.000006, Grad max: 0.109, Actions: 501, Entropy: 0.000\n",
      "Episode 3130, Reward: -501.000, End reward: -501.000, Loss: 297.718, Grad mean: 0.000006, Grad max: 0.108, Actions: 501, Entropy: 0.000\n",
      "Episode 3135, Reward: -501.000, End reward: -501.000, Loss: 299.256, Grad mean: 0.000006, Grad max: 0.108, Actions: 501, Entropy: 0.000\n",
      "Episode 3140, Reward: -501.000, End reward: -501.000, Loss: 298.030, Grad mean: 0.000006, Grad max: 0.109, Actions: 501, Entropy: 0.000\n",
      "Episode 3145, Reward: -501.000, End reward: -501.000, Loss: 295.923, Grad mean: 0.000006, Grad max: 0.109, Actions: 501, Entropy: 0.000\n",
      "Episode 3150, Reward: -501.000, End reward: -501.000, Loss: 298.064, Grad mean: 0.000006, Grad max: 0.108, Actions: 501, Entropy: 0.000\n",
      "Episode 3155, Reward: -501.000, End reward: -501.000, Loss: 302.083, Grad mean: 0.000006, Grad max: 0.108, Actions: 501, Entropy: 0.000\n",
      "Episode 3160, Reward: -501.000, End reward: -501.000, Loss: 297.737, Grad mean: 0.000006, Grad max: 0.108, Actions: 501, Entropy: 0.000\n",
      "Episode 3165, Reward: -501.000, End reward: -501.000, Loss: 297.803, Grad mean: 0.000006, Grad max: 0.109, Actions: 501, Entropy: 0.000\n",
      "Episode 3170, Reward: -501.000, End reward: -501.000, Loss: 298.523, Grad mean: 0.000006, Grad max: 0.109, Actions: 501, Entropy: 0.000\n",
      "Episode 3175, Reward: -501.000, End reward: -501.000, Loss: 295.511, Grad mean: 0.000006, Grad max: 0.109, Actions: 501, Entropy: 0.000\n",
      "Episode 3180, Reward: -501.000, End reward: -501.000, Loss: 297.510, Grad mean: 0.000006, Grad max: 0.108, Actions: 501, Entropy: 0.000\n",
      "Episode 3185, Reward: -501.000, End reward: -501.000, Loss: 298.434, Grad mean: 0.000006, Grad max: 0.109, Actions: 501, Entropy: 0.000\n",
      "Episode 3190, Reward: -501.000, End reward: -501.000, Loss: 297.554, Grad mean: 0.000006, Grad max: 0.109, Actions: 501, Entropy: 0.000\n",
      "Episode 3195, Reward: -501.000, End reward: -501.000, Loss: 298.198, Grad mean: 0.000006, Grad max: 0.109, Actions: 501, Entropy: 0.000\n",
      "Episode 3200, Reward: -501.000, End reward: -501.000, Loss: 299.435, Grad mean: 0.000006, Grad max: 0.109, Actions: 501, Entropy: 0.000\n",
      "Episode 3205, Reward: -501.000, End reward: -501.000, Loss: 298.027, Grad mean: 0.000006, Grad max: 0.109, Actions: 501, Entropy: 0.000\n",
      "Episode 3210, Reward: -501.000, End reward: -501.000, Loss: 298.061, Grad mean: 0.000006, Grad max: 0.109, Actions: 501, Entropy: 0.000\n",
      "Episode 3215, Reward: -501.000, End reward: -501.000, Loss: 299.128, Grad mean: 0.000006, Grad max: 0.109, Actions: 501, Entropy: 0.000\n",
      "Episode 3220, Reward: -501.000, End reward: -501.000, Loss: 298.158, Grad mean: 0.000006, Grad max: 0.109, Actions: 501, Entropy: 0.000\n",
      "Episode 3225, Reward: -501.000, End reward: -501.000, Loss: 295.293, Grad mean: 0.000006, Grad max: 0.109, Actions: 501, Entropy: 0.000\n",
      "Episode 3230, Reward: -501.000, End reward: -501.000, Loss: 300.977, Grad mean: 0.000006, Grad max: 0.109, Actions: 501, Entropy: 0.000\n",
      "Episode 3235, Reward: -501.000, End reward: -501.000, Loss: 297.231, Grad mean: 0.000006, Grad max: 0.107, Actions: 501, Entropy: 0.000\n",
      "Episode 3240, Reward: -501.000, End reward: -501.000, Loss: 297.112, Grad mean: 0.000006, Grad max: 0.109, Actions: 501, Entropy: 0.000\n",
      "Episode 3245, Reward: -501.000, End reward: -501.000, Loss: 297.684, Grad mean: 0.000006, Grad max: 0.109, Actions: 501, Entropy: 0.000\n",
      "Episode 3250, Reward: -501.000, End reward: -501.000, Loss: 298.402, Grad mean: 0.000006, Grad max: 0.109, Actions: 501, Entropy: 0.000\n",
      "Episode 3255, Reward: -501.000, End reward: -501.000, Loss: 295.503, Grad mean: 0.000006, Grad max: 0.109, Actions: 501, Entropy: 0.000\n",
      "Episode 3260, Reward: -501.000, End reward: -501.000, Loss: 298.097, Grad mean: 0.000006, Grad max: 0.110, Actions: 501, Entropy: 0.000\n",
      "Episode 3265, Reward: -501.000, End reward: -501.000, Loss: 295.194, Grad mean: 0.000006, Grad max: 0.109, Actions: 501, Entropy: 0.000\n",
      "Episode 3270, Reward: -501.000, End reward: -501.000, Loss: 300.123, Grad mean: 0.000006, Grad max: 0.110, Actions: 501, Entropy: 0.000\n",
      "Episode 3275, Reward: -501.000, End reward: -501.000, Loss: 297.328, Grad mean: 0.000006, Grad max: 0.110, Actions: 501, Entropy: 0.000\n",
      "Episode 3280, Reward: -501.000, End reward: -501.000, Loss: 294.967, Grad mean: 0.000007, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 3285, Reward: -501.000, End reward: -501.000, Loss: 298.233, Grad mean: 0.000006, Grad max: 0.110, Actions: 501, Entropy: 0.000\n",
      "Episode 3290, Reward: -501.000, End reward: -501.000, Loss: 295.604, Grad mean: 0.000006, Grad max: 0.110, Actions: 501, Entropy: 0.000\n",
      "Episode 3295, Reward: -501.000, End reward: -501.000, Loss: 295.109, Grad mean: 0.000006, Grad max: 0.110, Actions: 501, Entropy: 0.000\n",
      "Episode 3300, Reward: -501.000, End reward: -501.000, Loss: 299.846, Grad mean: 0.000006, Grad max: 0.110, Actions: 501, Entropy: 0.000\n",
      "Episode 3305, Reward: -501.000, End reward: -501.000, Loss: 297.297, Grad mean: 0.000007, Grad max: 0.108, Actions: 501, Entropy: 0.000\n",
      "Episode 3310, Reward: -501.000, End reward: -501.000, Loss: 296.623, Grad mean: 0.000006, Grad max: 0.111, Actions: 501, Entropy: 0.000\n",
      "Episode 3315, Reward: -501.000, End reward: -501.000, Loss: 295.331, Grad mean: 0.000006, Grad max: 0.106, Actions: 501, Entropy: 0.000\n",
      "Episode 3320, Reward: -501.000, End reward: -501.000, Loss: 299.962, Grad mean: 0.000006, Grad max: 0.110, Actions: 501, Entropy: 0.000\n",
      "Episode 3325, Reward: -501.000, End reward: -501.000, Loss: 295.007, Grad mean: 0.000006, Grad max: 0.110, Actions: 501, Entropy: 0.000\n",
      "Episode 3330, Reward: -501.000, End reward: -501.000, Loss: 297.422, Grad mean: 0.000006, Grad max: 0.110, Actions: 501, Entropy: 0.000\n",
      "Episode 3335, Reward: -501.000, End reward: -501.000, Loss: 295.294, Grad mean: 0.000006, Grad max: 0.110, Actions: 501, Entropy: 0.000\n",
      "Episode 3340, Reward: -501.000, End reward: -501.000, Loss: 301.790, Grad mean: 0.000006, Grad max: 0.111, Actions: 501, Entropy: 0.000\n",
      "Episode 3345, Reward: -501.000, End reward: -501.000, Loss: 294.758, Grad mean: 0.000006, Grad max: 0.111, Actions: 501, Entropy: 0.000\n",
      "Episode 3350, Reward: -501.000, End reward: -501.000, Loss: 296.741, Grad mean: 0.000006, Grad max: 0.111, Actions: 501, Entropy: 0.000\n",
      "Episode 3355, Reward: -501.000, End reward: -501.000, Loss: 294.709, Grad mean: 0.000006, Grad max: 0.111, Actions: 501, Entropy: 0.000\n",
      "Episode 3360, Reward: -501.000, End reward: -501.000, Loss: 296.625, Grad mean: 0.000006, Grad max: 0.111, Actions: 501, Entropy: 0.000\n",
      "Episode 3365, Reward: -501.000, End reward: -501.000, Loss: 297.591, Grad mean: 0.000006, Grad max: 0.111, Actions: 501, Entropy: 0.000\n",
      "Episode 3370, Reward: -501.000, End reward: -501.000, Loss: 295.488, Grad mean: 0.000006, Grad max: 0.111, Actions: 501, Entropy: 0.000\n",
      "Episode 3375, Reward: -501.000, End reward: -501.000, Loss: 298.934, Grad mean: 0.000006, Grad max: 0.111, Actions: 501, Entropy: 0.000\n",
      "Episode 3380, Reward: -501.000, End reward: -501.000, Loss: 297.325, Grad mean: 0.000006, Grad max: 0.111, Actions: 501, Entropy: 0.000\n",
      "Episode 3385, Reward: -501.000, End reward: -501.000, Loss: 296.914, Grad mean: 0.000006, Grad max: 0.111, Actions: 501, Entropy: 0.000\n",
      "Episode 3390, Reward: -501.000, End reward: -501.000, Loss: 297.961, Grad mean: 0.000006, Grad max: 0.111, Actions: 501, Entropy: 0.000\n",
      "Episode 3395, Reward: -501.000, End reward: -501.000, Loss: 299.469, Grad mean: 0.000006, Grad max: 0.112, Actions: 501, Entropy: 0.000\n",
      "Episode 3400, Reward: -501.000, End reward: -501.000, Loss: 299.827, Grad mean: 0.000006, Grad max: 0.111, Actions: 501, Entropy: 0.000\n",
      "Episode 3405, Reward: -501.000, End reward: -501.000, Loss: 298.121, Grad mean: 0.000006, Grad max: 0.112, Actions: 501, Entropy: 0.000\n",
      "Episode 3410, Reward: -501.000, End reward: -501.000, Loss: 298.313, Grad mean: 0.000006, Grad max: 0.112, Actions: 501, Entropy: 0.000\n",
      "Episode 3415, Reward: -501.000, End reward: -501.000, Loss: 298.723, Grad mean: 0.000006, Grad max: 0.111, Actions: 501, Entropy: 0.000\n",
      "Episode 3420, Reward: -501.000, End reward: -501.000, Loss: 297.234, Grad mean: 0.000005, Grad max: 0.112, Actions: 501, Entropy: 0.000\n",
      "Episode 3425, Reward: -501.000, End reward: -501.000, Loss: 295.814, Grad mean: 0.000006, Grad max: 0.112, Actions: 501, Entropy: 0.000\n",
      "Episode 3430, Reward: -501.000, End reward: -501.000, Loss: 295.114, Grad mean: 0.000010, Grad max: 0.120, Actions: 501, Entropy: 0.000\n",
      "Episode 3435, Reward: -501.000, End reward: -501.000, Loss: 301.055, Grad mean: 0.000006, Grad max: 0.112, Actions: 501, Entropy: 0.000\n",
      "Episode 3440, Reward: -501.000, End reward: -501.000, Loss: 297.360, Grad mean: 0.000006, Grad max: 0.112, Actions: 501, Entropy: 0.000\n",
      "Episode 3445, Reward: -501.000, End reward: -501.000, Loss: 297.901, Grad mean: 0.000006, Grad max: 0.112, Actions: 501, Entropy: 0.000\n",
      "Episode 3450, Reward: -501.000, End reward: -501.000, Loss: 297.238, Grad mean: 0.000006, Grad max: 0.111, Actions: 501, Entropy: 0.000\n",
      "Episode 3455, Reward: -501.000, End reward: -501.000, Loss: 297.088, Grad mean: 0.000006, Grad max: 0.111, Actions: 501, Entropy: 0.000\n",
      "Episode 3460, Reward: -501.000, End reward: -501.000, Loss: 298.645, Grad mean: 0.000006, Grad max: 0.112, Actions: 501, Entropy: 0.000\n",
      "Episode 3465, Reward: -501.000, End reward: -501.000, Loss: 299.057, Grad mean: 0.000006, Grad max: 0.112, Actions: 501, Entropy: 0.000\n",
      "Episode 3470, Reward: -501.000, End reward: -501.000, Loss: 299.223, Grad mean: 0.000006, Grad max: 0.112, Actions: 501, Entropy: 0.000\n",
      "Episode 3475, Reward: -501.000, End reward: -501.000, Loss: 298.494, Grad mean: 0.000006, Grad max: 0.112, Actions: 501, Entropy: 0.000\n",
      "Episode 3480, Reward: -501.000, End reward: -501.000, Loss: 297.970, Grad mean: 0.000006, Grad max: 0.113, Actions: 501, Entropy: 0.000\n",
      "Episode 3485, Reward: -501.000, End reward: -501.000, Loss: 299.981, Grad mean: 0.000008, Grad max: 0.141, Actions: 501, Entropy: 0.000\n",
      "Episode 3490, Reward: -501.000, End reward: -501.000, Loss: 298.727, Grad mean: 0.000006, Grad max: 0.112, Actions: 501, Entropy: 0.000\n",
      "Episode 3495, Reward: -501.000, End reward: -501.000, Loss: 299.356, Grad mean: 0.000006, Grad max: 0.113, Actions: 501, Entropy: 0.000\n",
      "Episode 3500, Reward: -501.000, End reward: -501.000, Loss: 302.659, Grad mean: 0.000006, Grad max: 0.113, Actions: 501, Entropy: 0.000\n",
      "Episode 3505, Reward: -501.000, End reward: -501.000, Loss: 300.137, Grad mean: 0.000006, Grad max: 0.113, Actions: 501, Entropy: 0.000\n",
      "Episode 3510, Reward: -501.000, End reward: -501.000, Loss: 297.600, Grad mean: 0.000006, Grad max: 0.113, Actions: 501, Entropy: 0.000\n",
      "Episode 3515, Reward: -501.000, End reward: -501.000, Loss: 299.026, Grad mean: 0.000006, Grad max: 0.114, Actions: 501, Entropy: 0.000\n",
      "Episode 3520, Reward: -501.000, End reward: -501.000, Loss: 298.342, Grad mean: 0.000006, Grad max: 0.113, Actions: 501, Entropy: 0.000\n",
      "Episode 3525, Reward: -501.000, End reward: -501.000, Loss: 297.427, Grad mean: 0.000006, Grad max: 0.113, Actions: 501, Entropy: 0.000\n",
      "Episode 3530, Reward: -501.000, End reward: -501.000, Loss: 301.502, Grad mean: 0.000006, Grad max: 0.113, Actions: 501, Entropy: 0.000\n",
      "Episode 3535, Reward: -501.000, End reward: -501.000, Loss: 308.842, Grad mean: 0.000006, Grad max: 0.114, Actions: 501, Entropy: 0.000\n",
      "Episode 3540, Reward: -501.000, End reward: -501.000, Loss: 312.183, Grad mean: 0.000006, Grad max: 0.112, Actions: 501, Entropy: 0.000\n",
      "Episode 3545, Reward: -501.000, End reward: -501.000, Loss: 298.337, Grad mean: 0.000006, Grad max: 0.113, Actions: 501, Entropy: 0.000\n",
      "Episode 3550, Reward: -501.000, End reward: -501.000, Loss: 297.853, Grad mean: 0.000006, Grad max: 0.113, Actions: 501, Entropy: 0.000\n",
      "Episode 3555, Reward: -501.000, End reward: -501.000, Loss: 298.887, Grad mean: 0.000006, Grad max: 0.113, Actions: 501, Entropy: 0.000\n",
      "Episode 3560, Reward: -501.000, End reward: -501.000, Loss: 297.047, Grad mean: 0.000006, Grad max: 0.113, Actions: 501, Entropy: 0.000\n",
      "Episode 3565, Reward: -501.000, End reward: -501.000, Loss: 302.124, Grad mean: 0.000006, Grad max: 0.113, Actions: 501, Entropy: 0.000\n",
      "Episode 3570, Reward: -501.000, End reward: -501.000, Loss: 297.078, Grad mean: 0.000006, Grad max: 0.113, Actions: 501, Entropy: 0.000\n",
      "Episode 3575, Reward: -501.000, End reward: -501.000, Loss: 302.925, Grad mean: 0.000006, Grad max: 0.114, Actions: 501, Entropy: 0.000\n",
      "Episode 3580, Reward: -501.000, End reward: -501.000, Loss: 303.080, Grad mean: 0.000006, Grad max: 0.113, Actions: 501, Entropy: 0.000\n",
      "Episode 3585, Reward: -501.000, End reward: -501.000, Loss: 295.574, Grad mean: 0.000006, Grad max: 0.113, Actions: 501, Entropy: 0.000\n",
      "Episode 3590, Reward: -501.000, End reward: -501.000, Loss: 298.840, Grad mean: 0.000006, Grad max: 0.113, Actions: 501, Entropy: 0.000\n",
      "Episode 3595, Reward: -501.000, End reward: -501.000, Loss: 298.261, Grad mean: 0.000006, Grad max: 0.113, Actions: 501, Entropy: 0.000\n",
      "Episode 3600, Reward: -501.000, End reward: -501.000, Loss: 298.367, Grad mean: 0.000005, Grad max: 0.114, Actions: 501, Entropy: 0.000\n",
      "Episode 3605, Reward: -501.000, End reward: -501.000, Loss: 297.024, Grad mean: 0.000006, Grad max: 0.114, Actions: 501, Entropy: 0.000\n",
      "Episode 3610, Reward: -501.000, End reward: -501.000, Loss: 295.872, Grad mean: 0.000006, Grad max: 0.114, Actions: 501, Entropy: 0.000\n",
      "Episode 3615, Reward: -501.000, End reward: -501.000, Loss: 299.861, Grad mean: 0.000006, Grad max: 0.114, Actions: 501, Entropy: 0.000\n",
      "Episode 3620, Reward: -501.000, End reward: -501.000, Loss: 295.104, Grad mean: 0.000006, Grad max: 0.114, Actions: 501, Entropy: 0.000\n",
      "Episode 3625, Reward: -501.000, End reward: -501.000, Loss: 298.432, Grad mean: 0.000006, Grad max: 0.114, Actions: 501, Entropy: 0.000\n",
      "Episode 3630, Reward: -501.000, End reward: -501.000, Loss: 298.690, Grad mean: 0.000005, Grad max: 0.114, Actions: 501, Entropy: 0.000\n",
      "Episode 3635, Reward: -501.000, End reward: -501.000, Loss: 297.701, Grad mean: 0.000005, Grad max: 0.114, Actions: 501, Entropy: 0.000\n",
      "Episode 3640, Reward: -501.000, End reward: -501.000, Loss: 297.438, Grad mean: 0.000006, Grad max: 0.113, Actions: 501, Entropy: 0.000\n",
      "Episode 3645, Reward: -501.000, End reward: -501.000, Loss: 298.174, Grad mean: 0.000005, Grad max: 0.114, Actions: 501, Entropy: 0.000\n",
      "Episode 3650, Reward: -501.000, End reward: -501.000, Loss: 298.503, Grad mean: 0.000006, Grad max: 0.114, Actions: 501, Entropy: 0.000\n",
      "Episode 3655, Reward: -501.000, End reward: -501.000, Loss: 296.708, Grad mean: 0.000005, Grad max: 0.114, Actions: 501, Entropy: 0.000\n",
      "Episode 3660, Reward: -501.000, End reward: -501.000, Loss: 298.273, Grad mean: 0.000006, Grad max: 0.114, Actions: 501, Entropy: 0.000\n",
      "Episode 3665, Reward: -501.000, End reward: -501.000, Loss: 297.625, Grad mean: 0.000005, Grad max: 0.115, Actions: 501, Entropy: 0.000\n",
      "Episode 3670, Reward: -501.000, End reward: -501.000, Loss: 297.111, Grad mean: 0.000005, Grad max: 0.114, Actions: 501, Entropy: 0.000\n",
      "Episode 3675, Reward: -501.000, End reward: -501.000, Loss: 296.972, Grad mean: 0.000005, Grad max: 0.115, Actions: 501, Entropy: 0.000\n",
      "Episode 3680, Reward: -501.000, End reward: -501.000, Loss: 297.722, Grad mean: 0.000006, Grad max: 0.114, Actions: 501, Entropy: 0.000\n",
      "Episode 3685, Reward: -501.000, End reward: -501.000, Loss: 295.231, Grad mean: 0.000006, Grad max: 0.115, Actions: 501, Entropy: 0.000\n",
      "Episode 3690, Reward: -501.000, End reward: -501.000, Loss: 296.332, Grad mean: 0.000005, Grad max: 0.115, Actions: 501, Entropy: 0.000\n",
      "Episode 3695, Reward: -501.000, End reward: -501.000, Loss: 298.126, Grad mean: 0.000005, Grad max: 0.115, Actions: 501, Entropy: 0.000\n",
      "Episode 3700, Reward: -501.000, End reward: -501.000, Loss: 297.585, Grad mean: 0.000006, Grad max: 0.115, Actions: 501, Entropy: 0.000\n",
      "Episode 3705, Reward: -501.000, End reward: -501.000, Loss: 298.664, Grad mean: 0.000005, Grad max: 0.115, Actions: 501, Entropy: 0.000\n",
      "Episode 3710, Reward: -501.000, End reward: -501.000, Loss: 297.124, Grad mean: 0.000006, Grad max: 0.115, Actions: 501, Entropy: 0.000\n",
      "Episode 3715, Reward: -501.000, End reward: -501.000, Loss: 297.952, Grad mean: 0.000005, Grad max: 0.115, Actions: 501, Entropy: 0.000\n",
      "Episode 3720, Reward: -501.000, End reward: -501.000, Loss: 297.118, Grad mean: 0.000007, Grad max: 0.112, Actions: 501, Entropy: 0.000\n",
      "Episode 3725, Reward: -501.000, End reward: -501.000, Loss: 298.120, Grad mean: 0.000006, Grad max: 0.115, Actions: 501, Entropy: 0.000\n",
      "Episode 3730, Reward: -501.000, End reward: -501.000, Loss: 296.912, Grad mean: 0.000006, Grad max: 0.111, Actions: 501, Entropy: 0.000\n",
      "Episode 3735, Reward: -501.000, End reward: -501.000, Loss: 300.033, Grad mean: 0.000005, Grad max: 0.116, Actions: 501, Entropy: 0.000\n",
      "Episode 3740, Reward: -501.000, End reward: -501.000, Loss: 297.620, Grad mean: 0.000006, Grad max: 0.115, Actions: 501, Entropy: 0.000\n",
      "Episode 3745, Reward: -501.000, End reward: -501.000, Loss: 298.806, Grad mean: 0.000005, Grad max: 0.116, Actions: 501, Entropy: 0.000\n",
      "Episode 3750, Reward: -501.000, End reward: -501.000, Loss: 296.713, Grad mean: 0.000005, Grad max: 0.116, Actions: 501, Entropy: 0.000\n",
      "Episode 3755, Reward: -501.000, End reward: -501.000, Loss: 298.921, Grad mean: 0.000005, Grad max: 0.116, Actions: 501, Entropy: 0.000\n",
      "Episode 3760, Reward: -501.000, End reward: -501.000, Loss: 297.389, Grad mean: 0.000006, Grad max: 0.142, Actions: 501, Entropy: 0.000\n",
      "Episode 3765, Reward: -501.000, End reward: -501.000, Loss: 298.288, Grad mean: 0.000005, Grad max: 0.116, Actions: 501, Entropy: 0.000\n",
      "Episode 3770, Reward: -501.000, End reward: -501.000, Loss: 297.116, Grad mean: 0.000006, Grad max: 0.116, Actions: 501, Entropy: 0.000\n",
      "Episode 3775, Reward: -501.000, End reward: -501.000, Loss: 296.130, Grad mean: 0.000006, Grad max: 0.116, Actions: 501, Entropy: 0.000\n",
      "Episode 3780, Reward: -501.000, End reward: -501.000, Loss: 298.160, Grad mean: 0.000005, Grad max: 0.116, Actions: 501, Entropy: 0.000\n",
      "Episode 3785, Reward: -501.000, End reward: -501.000, Loss: 297.181, Grad mean: 0.000006, Grad max: 0.116, Actions: 501, Entropy: 0.000\n",
      "Episode 3790, Reward: -501.000, End reward: -501.000, Loss: 299.832, Grad mean: 0.000006, Grad max: 0.116, Actions: 501, Entropy: 0.000\n",
      "Episode 3795, Reward: -501.000, End reward: -501.000, Loss: 298.329, Grad mean: 0.000006, Grad max: 0.115, Actions: 501, Entropy: 0.000\n",
      "Episode 3800, Reward: -501.000, End reward: -501.000, Loss: 306.399, Grad mean: 0.000006, Grad max: 0.116, Actions: 501, Entropy: 0.000\n",
      "Episode 3805, Reward: -501.000, End reward: -501.000, Loss: 295.193, Grad mean: 0.000006, Grad max: 0.115, Actions: 501, Entropy: 0.000\n",
      "Episode 3810, Reward: -501.000, End reward: -501.000, Loss: 296.919, Grad mean: 0.000006, Grad max: 0.116, Actions: 501, Entropy: 0.000\n",
      "Episode 3815, Reward: -501.000, End reward: -501.000, Loss: 304.367, Grad mean: 0.000006, Grad max: 0.116, Actions: 501, Entropy: 0.000\n",
      "Episode 3820, Reward: -501.000, End reward: -501.000, Loss: 299.367, Grad mean: 0.000006, Grad max: 0.116, Actions: 501, Entropy: 0.000\n",
      "Episode 3825, Reward: -501.000, End reward: -501.000, Loss: 299.287, Grad mean: 0.000006, Grad max: 0.116, Actions: 501, Entropy: 0.000\n",
      "Episode 3830, Reward: -501.000, End reward: -501.000, Loss: 301.412, Grad mean: 0.000006, Grad max: 0.116, Actions: 501, Entropy: 0.000\n",
      "Episode 3835, Reward: -501.000, End reward: -501.000, Loss: 299.153, Grad mean: 0.000006, Grad max: 0.116, Actions: 501, Entropy: 0.000\n",
      "Episode 3840, Reward: -501.000, End reward: -501.000, Loss: 301.361, Grad mean: 0.000006, Grad max: 0.117, Actions: 501, Entropy: 0.000\n",
      "Episode 3845, Reward: -501.000, End reward: -501.000, Loss: 296.229, Grad mean: 0.000006, Grad max: 0.116, Actions: 501, Entropy: 0.000\n",
      "Episode 3850, Reward: -501.000, End reward: -501.000, Loss: 299.746, Grad mean: 0.000006, Grad max: 0.116, Actions: 501, Entropy: 0.000\n",
      "Episode 3855, Reward: -501.000, End reward: -501.000, Loss: 302.124, Grad mean: 0.000005, Grad max: 0.117, Actions: 501, Entropy: 0.000\n",
      "Episode 3860, Reward: -501.000, End reward: -501.000, Loss: 299.514, Grad mean: 0.000006, Grad max: 0.116, Actions: 501, Entropy: 0.000\n",
      "Episode 3865, Reward: -501.000, End reward: -501.000, Loss: 299.098, Grad mean: 0.000005, Grad max: 0.117, Actions: 501, Entropy: 0.000\n",
      "Episode 3870, Reward: -501.000, End reward: -501.000, Loss: 302.427, Grad mean: 0.000005, Grad max: 0.117, Actions: 501, Entropy: 0.000\n",
      "Episode 3875, Reward: -501.000, End reward: -501.000, Loss: 301.545, Grad mean: 0.000006, Grad max: 0.116, Actions: 501, Entropy: 0.000\n",
      "Episode 3880, Reward: -501.000, End reward: -501.000, Loss: 299.419, Grad mean: 0.000006, Grad max: 0.116, Actions: 501, Entropy: 0.000\n",
      "Episode 3885, Reward: -501.000, End reward: -501.000, Loss: 297.385, Grad mean: 0.000006, Grad max: 0.116, Actions: 501, Entropy: 0.000\n",
      "Episode 3890, Reward: -501.000, End reward: -501.000, Loss: 299.663, Grad mean: 0.000006, Grad max: 0.116, Actions: 501, Entropy: 0.000\n",
      "Episode 3895, Reward: -501.000, End reward: -501.000, Loss: 297.049, Grad mean: 0.000005, Grad max: 0.117, Actions: 501, Entropy: 0.000\n",
      "Episode 3900, Reward: -501.000, End reward: -501.000, Loss: 297.419, Grad mean: 0.000007, Grad max: 0.108, Actions: 501, Entropy: 0.000\n",
      "Episode 3905, Reward: -501.000, End reward: -501.000, Loss: 297.720, Grad mean: 0.000005, Grad max: 0.117, Actions: 501, Entropy: 0.000\n",
      "Episode 3910, Reward: -501.000, End reward: -501.000, Loss: 296.145, Grad mean: 0.000006, Grad max: 0.116, Actions: 501, Entropy: 0.000\n",
      "Episode 3915, Reward: -501.000, End reward: -501.000, Loss: 298.570, Grad mean: 0.000005, Grad max: 0.117, Actions: 501, Entropy: 0.000\n",
      "Episode 3920, Reward: -501.000, End reward: -501.000, Loss: 299.876, Grad mean: 0.000005, Grad max: 0.117, Actions: 501, Entropy: 0.000\n",
      "Episode 3925, Reward: -501.000, End reward: -501.000, Loss: 301.324, Grad mean: 0.000005, Grad max: 0.117, Actions: 501, Entropy: 0.000\n",
      "Episode 3930, Reward: -501.000, End reward: -501.000, Loss: 300.515, Grad mean: 0.000005, Grad max: 0.118, Actions: 501, Entropy: 0.000\n",
      "Episode 3935, Reward: -501.000, End reward: -501.000, Loss: 297.776, Grad mean: 0.000005, Grad max: 0.118, Actions: 501, Entropy: 0.000\n",
      "Episode 3940, Reward: -501.000, End reward: -501.000, Loss: 295.836, Grad mean: 0.000005, Grad max: 0.118, Actions: 501, Entropy: 0.000\n",
      "Episode 3945, Reward: -501.000, End reward: -501.000, Loss: 298.486, Grad mean: 0.000006, Grad max: 0.115, Actions: 501, Entropy: 0.000\n",
      "Episode 3950, Reward: -501.000, End reward: -501.000, Loss: 301.584, Grad mean: 0.000005, Grad max: 0.118, Actions: 501, Entropy: 0.000\n",
      "Episode 3955, Reward: -501.000, End reward: -501.000, Loss: 303.721, Grad mean: 0.000005, Grad max: 0.119, Actions: 501, Entropy: 0.000\n",
      "Episode 3960, Reward: -501.000, End reward: -501.000, Loss: 295.725, Grad mean: 0.000005, Grad max: 0.118, Actions: 501, Entropy: 0.000\n",
      "Episode 3965, Reward: -501.000, End reward: -501.000, Loss: 298.522, Grad mean: 0.000005, Grad max: 0.119, Actions: 501, Entropy: 0.000\n",
      "Episode 3970, Reward: -501.000, End reward: -501.000, Loss: 305.074, Grad mean: 0.000005, Grad max: 0.118, Actions: 501, Entropy: 0.000\n",
      "Episode 3975, Reward: -501.000, End reward: -501.000, Loss: 296.193, Grad mean: 0.000005, Grad max: 0.119, Actions: 501, Entropy: 0.000\n",
      "Episode 3980, Reward: -501.000, End reward: -501.000, Loss: 295.334, Grad mean: 0.000005, Grad max: 0.119, Actions: 501, Entropy: 0.000\n",
      "Episode 3985, Reward: -501.000, End reward: -501.000, Loss: 298.556, Grad mean: 0.000005, Grad max: 0.119, Actions: 501, Entropy: 0.000\n",
      "Episode 3990, Reward: -501.000, End reward: -501.000, Loss: 298.959, Grad mean: 0.000005, Grad max: 0.119, Actions: 501, Entropy: 0.000\n",
      "Episode 3995, Reward: -501.000, End reward: -501.000, Loss: 300.123, Grad mean: 0.000005, Grad max: 0.119, Actions: 501, Entropy: 0.000\n",
      "Episode 4000, Reward: -501.000, End reward: -501.000, Loss: 297.775, Grad mean: 0.000005, Grad max: 0.119, Actions: 501, Entropy: 0.000\n",
      "Episode 4005, Reward: -501.000, End reward: -501.000, Loss: 298.138, Grad mean: 0.000005, Grad max: 0.119, Actions: 501, Entropy: 0.000\n",
      "Episode 4010, Reward: -501.000, End reward: -501.000, Loss: 298.031, Grad mean: 0.000005, Grad max: 0.119, Actions: 501, Entropy: 0.000\n",
      "Episode 4015, Reward: -501.000, End reward: -501.000, Loss: 296.423, Grad mean: 0.000005, Grad max: 0.118, Actions: 501, Entropy: 0.000\n",
      "Episode 4020, Reward: -501.000, End reward: -501.000, Loss: 297.436, Grad mean: 0.000005, Grad max: 0.119, Actions: 501, Entropy: 0.000\n",
      "Episode 4025, Reward: -501.000, End reward: -501.000, Loss: 297.104, Grad mean: 0.000005, Grad max: 0.118, Actions: 501, Entropy: 0.000\n",
      "Episode 4030, Reward: -501.000, End reward: -501.000, Loss: 298.641, Grad mean: 0.000005, Grad max: 0.119, Actions: 501, Entropy: 0.000\n",
      "Episode 4035, Reward: -501.000, End reward: -501.000, Loss: 299.003, Grad mean: 0.000005, Grad max: 0.119, Actions: 501, Entropy: 0.000\n",
      "Episode 4040, Reward: -501.000, End reward: -501.000, Loss: 299.177, Grad mean: 0.000005, Grad max: 0.119, Actions: 501, Entropy: 0.000\n",
      "Episode 4045, Reward: -501.000, End reward: -501.000, Loss: 301.006, Grad mean: 0.000005, Grad max: 0.118, Actions: 501, Entropy: 0.000\n",
      "Episode 4050, Reward: -501.000, End reward: -501.000, Loss: 297.464, Grad mean: 0.000005, Grad max: 0.119, Actions: 501, Entropy: 0.000\n",
      "Episode 4055, Reward: -501.000, End reward: -501.000, Loss: 304.927, Grad mean: 0.000005, Grad max: 0.120, Actions: 501, Entropy: 0.000\n",
      "Episode 4060, Reward: -501.000, End reward: -501.000, Loss: 301.223, Grad mean: 0.000005, Grad max: 0.120, Actions: 501, Entropy: 0.000\n",
      "Episode 4065, Reward: -501.000, End reward: -501.000, Loss: 299.754, Grad mean: 0.000005, Grad max: 0.120, Actions: 501, Entropy: 0.000\n",
      "Episode 4070, Reward: -501.000, End reward: -501.000, Loss: 301.726, Grad mean: 0.000005, Grad max: 0.120, Actions: 501, Entropy: 0.000\n",
      "Episode 4075, Reward: -501.000, End reward: -501.000, Loss: 298.052, Grad mean: 0.000005, Grad max: 0.121, Actions: 501, Entropy: 0.000\n",
      "Episode 4080, Reward: -501.000, End reward: -501.000, Loss: 299.769, Grad mean: 0.000005, Grad max: 0.120, Actions: 501, Entropy: 0.000\n",
      "Episode 4085, Reward: -501.000, End reward: -501.000, Loss: 303.309, Grad mean: 0.000005, Grad max: 0.121, Actions: 501, Entropy: 0.000\n",
      "Episode 4090, Reward: -501.000, End reward: -501.000, Loss: 298.216, Grad mean: 0.000005, Grad max: 0.120, Actions: 501, Entropy: 0.000\n",
      "Episode 4095, Reward: -501.000, End reward: -501.000, Loss: 295.969, Grad mean: 0.000005, Grad max: 0.121, Actions: 501, Entropy: 0.000\n",
      "Episode 4100, Reward: -501.000, End reward: -501.000, Loss: 298.569, Grad mean: 0.000005, Grad max: 0.121, Actions: 501, Entropy: 0.000\n",
      "Episode 4105, Reward: -501.000, End reward: -501.000, Loss: 307.744, Grad mean: 0.000005, Grad max: 0.120, Actions: 501, Entropy: 0.000\n",
      "Episode 4110, Reward: -501.000, End reward: -501.000, Loss: 300.272, Grad mean: 0.000005, Grad max: 0.121, Actions: 501, Entropy: 0.000\n",
      "Episode 4115, Reward: -501.000, End reward: -501.000, Loss: 296.801, Grad mean: 0.000005, Grad max: 0.120, Actions: 501, Entropy: 0.000\n",
      "Episode 4120, Reward: -501.000, End reward: -501.000, Loss: 298.972, Grad mean: 0.000005, Grad max: 0.120, Actions: 501, Entropy: 0.000\n",
      "Episode 4125, Reward: -501.000, End reward: -501.000, Loss: 304.202, Grad mean: 0.000005, Grad max: 0.121, Actions: 501, Entropy: 0.000\n",
      "Episode 4130, Reward: -501.000, End reward: -501.000, Loss: 303.396, Grad mean: 0.000005, Grad max: 0.120, Actions: 501, Entropy: 0.000\n",
      "Episode 4135, Reward: -501.000, End reward: -501.000, Loss: 301.798, Grad mean: 0.000005, Grad max: 0.121, Actions: 501, Entropy: 0.000\n",
      "Episode 4140, Reward: -501.000, End reward: -501.000, Loss: 304.204, Grad mean: 0.000005, Grad max: 0.120, Actions: 501, Entropy: 0.000\n",
      "Episode 4145, Reward: -501.000, End reward: -501.000, Loss: 308.343, Grad mean: 0.000005, Grad max: 0.122, Actions: 501, Entropy: 0.000\n",
      "Episode 4150, Reward: -501.000, End reward: -501.000, Loss: 302.885, Grad mean: 0.000005, Grad max: 0.121, Actions: 501, Entropy: 0.000\n",
      "Episode 4155, Reward: -501.000, End reward: -501.000, Loss: 297.290, Grad mean: 0.000005, Grad max: 0.121, Actions: 501, Entropy: 0.000\n",
      "Episode 4160, Reward: -501.000, End reward: -501.000, Loss: 299.547, Grad mean: 0.000005, Grad max: 0.121, Actions: 501, Entropy: 0.000\n",
      "Episode 4165, Reward: -501.000, End reward: -501.000, Loss: 298.036, Grad mean: 0.000005, Grad max: 0.121, Actions: 501, Entropy: 0.000\n",
      "Episode 4170, Reward: -501.000, End reward: -501.000, Loss: 297.292, Grad mean: 0.000005, Grad max: 0.121, Actions: 501, Entropy: 0.000\n",
      "Episode 4175, Reward: -501.000, End reward: -501.000, Loss: 295.533, Grad mean: 0.000005, Grad max: 0.121, Actions: 501, Entropy: 0.000\n",
      "Episode 4180, Reward: -501.000, End reward: -501.000, Loss: 297.501, Grad mean: 0.000005, Grad max: 0.121, Actions: 501, Entropy: 0.000\n",
      "Episode 4185, Reward: -501.000, End reward: -501.000, Loss: 297.753, Grad mean: 0.000005, Grad max: 0.121, Actions: 501, Entropy: 0.000\n",
      "Episode 4190, Reward: -501.000, End reward: -501.000, Loss: 297.384, Grad mean: 0.000005, Grad max: 0.121, Actions: 501, Entropy: 0.000\n",
      "Episode 4195, Reward: -501.000, End reward: -501.000, Loss: 296.770, Grad mean: 0.000005, Grad max: 0.121, Actions: 501, Entropy: 0.000\n",
      "Episode 4200, Reward: -501.000, End reward: -501.000, Loss: 296.460, Grad mean: 0.000005, Grad max: 0.120, Actions: 501, Entropy: 0.000\n",
      "Episode 4205, Reward: -501.000, End reward: -501.000, Loss: 300.488, Grad mean: 0.000005, Grad max: 0.121, Actions: 501, Entropy: 0.000\n",
      "Episode 4210, Reward: -501.000, End reward: -501.000, Loss: 302.764, Grad mean: 0.000005, Grad max: 0.120, Actions: 501, Entropy: 0.000\n",
      "Episode 4215, Reward: -501.000, End reward: -501.000, Loss: 310.672, Grad mean: 0.000005, Grad max: 0.121, Actions: 501, Entropy: 0.000\n",
      "Episode 4220, Reward: -501.000, End reward: -501.000, Loss: 297.384, Grad mean: 0.000005, Grad max: 0.120, Actions: 501, Entropy: 0.000\n",
      "Episode 4225, Reward: -501.000, End reward: -501.000, Loss: 300.504, Grad mean: 0.000005, Grad max: 0.120, Actions: 501, Entropy: 0.000\n",
      "Episode 4230, Reward: -501.000, End reward: -501.000, Loss: 298.813, Grad mean: 0.000005, Grad max: 0.121, Actions: 501, Entropy: 0.000\n",
      "Episode 4235, Reward: -501.000, End reward: -501.000, Loss: 296.979, Grad mean: 0.000005, Grad max: 0.121, Actions: 501, Entropy: 0.000\n",
      "Episode 4240, Reward: -501.000, End reward: -501.000, Loss: 303.337, Grad mean: 0.000005, Grad max: 0.121, Actions: 501, Entropy: 0.000\n",
      "Episode 4245, Reward: -501.000, End reward: -501.000, Loss: 308.158, Grad mean: 0.000005, Grad max: 0.120, Actions: 501, Entropy: 0.000\n",
      "Episode 4250, Reward: -501.000, End reward: -501.000, Loss: 308.938, Grad mean: 0.000005, Grad max: 0.121, Actions: 501, Entropy: 0.000\n",
      "Episode 4255, Reward: -501.000, End reward: -501.000, Loss: 300.764, Grad mean: 0.000005, Grad max: 0.121, Actions: 501, Entropy: 0.000\n",
      "Episode 4260, Reward: -501.000, End reward: -501.000, Loss: 295.736, Grad mean: 0.000005, Grad max: 0.121, Actions: 501, Entropy: 0.000\n",
      "Episode 4265, Reward: -501.000, End reward: -501.000, Loss: 295.358, Grad mean: 0.000005, Grad max: 0.121, Actions: 501, Entropy: 0.000\n",
      "Episode 4270, Reward: -501.000, End reward: -501.000, Loss: 300.923, Grad mean: 0.000005, Grad max: 0.122, Actions: 501, Entropy: 0.000\n",
      "Episode 4275, Reward: -501.000, End reward: -501.000, Loss: 302.822, Grad mean: 0.000005, Grad max: 0.121, Actions: 501, Entropy: 0.000\n",
      "Episode 4280, Reward: -501.000, End reward: -501.000, Loss: 312.596, Grad mean: 0.000005, Grad max: 0.122, Actions: 501, Entropy: 0.000\n",
      "Episode 4285, Reward: -501.000, End reward: -501.000, Loss: 304.566, Grad mean: 0.000005, Grad max: 0.121, Actions: 501, Entropy: 0.000\n",
      "Episode 4290, Reward: -501.000, End reward: -501.000, Loss: 307.902, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4295, Reward: -501.000, End reward: -501.000, Loss: 299.083, Grad mean: 0.000005, Grad max: 0.122, Actions: 501, Entropy: 0.000\n",
      "Episode 4300, Reward: -501.000, End reward: -501.000, Loss: 300.945, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4305, Reward: -501.000, End reward: -501.000, Loss: 308.784, Grad mean: 0.000005, Grad max: 0.122, Actions: 501, Entropy: 0.000\n",
      "Episode 4310, Reward: -501.000, End reward: -501.000, Loss: 299.511, Grad mean: 0.000008, Grad max: 0.110, Actions: 501, Entropy: 0.000\n",
      "Episode 4315, Reward: -501.000, End reward: -501.000, Loss: 299.961, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4320, Reward: -501.000, End reward: -501.000, Loss: 298.688, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4325, Reward: -501.000, End reward: -501.000, Loss: 303.616, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4330, Reward: -501.000, End reward: -501.000, Loss: 296.059, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4335, Reward: -501.000, End reward: -501.000, Loss: 298.564, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4340, Reward: -501.000, End reward: -501.000, Loss: 304.312, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4345, Reward: -501.000, End reward: -501.000, Loss: 298.277, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4350, Reward: -501.000, End reward: -501.000, Loss: 297.496, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4355, Reward: -501.000, End reward: -501.000, Loss: 298.025, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4360, Reward: -501.000, End reward: -501.000, Loss: 297.708, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4365, Reward: -501.000, End reward: -501.000, Loss: 295.659, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4370, Reward: -501.000, End reward: -501.000, Loss: 297.281, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4375, Reward: -501.000, End reward: -501.000, Loss: 304.472, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4380, Reward: -501.000, End reward: -501.000, Loss: 311.253, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4385, Reward: -501.000, End reward: -501.000, Loss: 299.659, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4390, Reward: -501.000, End reward: -501.000, Loss: 297.379, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4395, Reward: -501.000, End reward: -501.000, Loss: 299.605, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4400, Reward: -501.000, End reward: -501.000, Loss: 297.124, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4405, Reward: -501.000, End reward: -501.000, Loss: 295.487, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4410, Reward: -501.000, End reward: -501.000, Loss: 298.005, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4415, Reward: -501.000, End reward: -501.000, Loss: 300.295, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4420, Reward: -501.000, End reward: -501.000, Loss: 295.588, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4425, Reward: -501.000, End reward: -501.000, Loss: 298.768, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4430, Reward: -501.000, End reward: -501.000, Loss: 298.987, Grad mean: 0.000006, Grad max: 0.121, Actions: 501, Entropy: 0.000\n",
      "Episode 4435, Reward: -501.000, End reward: -501.000, Loss: 299.297, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4440, Reward: -501.000, End reward: -501.000, Loss: 304.603, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4445, Reward: -501.000, End reward: -501.000, Loss: 303.629, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4450, Reward: -501.000, End reward: -501.000, Loss: 299.717, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4455, Reward: -501.000, End reward: -501.000, Loss: 312.753, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4460, Reward: -501.000, End reward: -501.000, Loss: 313.115, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4465, Reward: -501.000, End reward: -501.000, Loss: 301.089, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4470, Reward: -501.000, End reward: -501.000, Loss: 297.094, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4475, Reward: -501.000, End reward: -501.000, Loss: 299.094, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4480, Reward: -501.000, End reward: -501.000, Loss: 299.973, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4485, Reward: -501.000, End reward: -501.000, Loss: 296.648, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4490, Reward: -501.000, End reward: -501.000, Loss: 299.427, Grad mean: 0.000005, Grad max: 0.122, Actions: 501, Entropy: 0.000\n",
      "Episode 4495, Reward: -501.000, End reward: -501.000, Loss: 298.383, Grad mean: 0.000007, Grad max: 0.117, Actions: 501, Entropy: 0.000\n",
      "Episode 4500, Reward: -501.000, End reward: -501.000, Loss: 299.417, Grad mean: 0.000005, Grad max: 0.122, Actions: 501, Entropy: 0.000\n",
      "Episode 4505, Reward: -501.000, End reward: -501.000, Loss: 296.933, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4510, Reward: -501.000, End reward: -501.000, Loss: 297.215, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4515, Reward: -501.000, End reward: -501.000, Loss: 299.816, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4520, Reward: -501.000, End reward: -501.000, Loss: 297.411, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4525, Reward: -501.000, End reward: -501.000, Loss: 294.985, Grad mean: 0.000006, Grad max: 0.122, Actions: 501, Entropy: 0.000\n",
      "Episode 4530, Reward: -501.000, End reward: -501.000, Loss: 300.436, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4535, Reward: -501.000, End reward: -501.000, Loss: 297.967, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4540, Reward: -501.000, End reward: -501.000, Loss: 301.410, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4545, Reward: -501.000, End reward: -501.000, Loss: 298.764, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4550, Reward: -501.000, End reward: -501.000, Loss: 296.458, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4555, Reward: -501.000, End reward: -501.000, Loss: 295.078, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4560, Reward: -501.000, End reward: -501.000, Loss: 300.258, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4565, Reward: -501.000, End reward: -501.000, Loss: 298.267, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4570, Reward: -501.000, End reward: -501.000, Loss: 295.459, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4575, Reward: -501.000, End reward: -501.000, Loss: 296.402, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4580, Reward: -501.000, End reward: -501.000, Loss: 295.482, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4585, Reward: -501.000, End reward: -501.000, Loss: 298.003, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4590, Reward: -501.000, End reward: -501.000, Loss: 298.821, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4595, Reward: -501.000, End reward: -501.000, Loss: 300.164, Grad mean: 0.000006, Grad max: 0.122, Actions: 501, Entropy: 0.000\n",
      "Episode 4600, Reward: -501.000, End reward: -501.000, Loss: 296.075, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4605, Reward: -501.000, End reward: -501.000, Loss: 298.824, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4610, Reward: -501.000, End reward: -501.000, Loss: 296.894, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4615, Reward: -501.000, End reward: -501.000, Loss: 297.470, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4620, Reward: -501.000, End reward: -501.000, Loss: 298.966, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4625, Reward: -501.000, End reward: -501.000, Loss: 298.663, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4630, Reward: -501.000, End reward: -501.000, Loss: 298.794, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4635, Reward: -501.000, End reward: -501.000, Loss: 296.929, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4640, Reward: -501.000, End reward: -501.000, Loss: 295.587, Grad mean: 0.000008, Grad max: 0.210, Actions: 501, Entropy: 0.000\n",
      "Episode 4645, Reward: -501.000, End reward: -501.000, Loss: 302.519, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4650, Reward: -501.000, End reward: -501.000, Loss: 297.731, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4655, Reward: -501.000, End reward: -501.000, Loss: 296.993, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4660, Reward: -501.000, End reward: -501.000, Loss: 295.493, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4665, Reward: -501.000, End reward: -501.000, Loss: 296.893, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4670, Reward: -501.000, End reward: -501.000, Loss: 299.462, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4675, Reward: -501.000, End reward: -501.000, Loss: 304.655, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4680, Reward: -501.000, End reward: -501.000, Loss: 297.436, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4685, Reward: -501.000, End reward: -501.000, Loss: 303.495, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4690, Reward: -501.000, End reward: -501.000, Loss: 301.235, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4695, Reward: -501.000, End reward: -501.000, Loss: 296.829, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4700, Reward: -501.000, End reward: -501.000, Loss: 296.114, Grad mean: 0.000005, Grad max: 0.123, Actions: 501, Entropy: 0.000\n",
      "Episode 4705, Reward: -501.000, End reward: -501.000, Loss: 300.648, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4710, Reward: -501.000, End reward: -501.000, Loss: 299.867, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4715, Reward: -501.000, End reward: -501.000, Loss: 296.714, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4720, Reward: -501.000, End reward: -501.000, Loss: 296.026, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4725, Reward: -501.000, End reward: -501.000, Loss: 301.267, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4730, Reward: -501.000, End reward: -501.000, Loss: 309.635, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4735, Reward: -501.000, End reward: -501.000, Loss: 296.685, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4740, Reward: -501.000, End reward: -501.000, Loss: 297.023, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4745, Reward: -501.000, End reward: -501.000, Loss: 297.322, Grad mean: 0.000005, Grad max: 0.121, Actions: 501, Entropy: 0.000\n",
      "Episode 4750, Reward: -501.000, End reward: -501.000, Loss: 297.361, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4755, Reward: -501.000, End reward: -501.000, Loss: 300.305, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4760, Reward: -501.000, End reward: -501.000, Loss: 297.822, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4765, Reward: -501.000, End reward: -501.000, Loss: 296.895, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4770, Reward: -501.000, End reward: -501.000, Loss: 297.307, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4775, Reward: -501.000, End reward: -501.000, Loss: 300.536, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4780, Reward: -501.000, End reward: -501.000, Loss: 301.328, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4785, Reward: -501.000, End reward: -501.000, Loss: 299.351, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4790, Reward: -501.000, End reward: -501.000, Loss: 295.316, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4795, Reward: -501.000, End reward: -501.000, Loss: 296.361, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4800, Reward: -501.000, End reward: -501.000, Loss: 303.000, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4805, Reward: -501.000, End reward: -501.000, Loss: 305.142, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4810, Reward: -501.000, End reward: -501.000, Loss: 306.836, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4815, Reward: -501.000, End reward: -501.000, Loss: 299.808, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4820, Reward: -501.000, End reward: -501.000, Loss: 297.056, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4825, Reward: -501.000, End reward: -501.000, Loss: 296.494, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4830, Reward: -501.000, End reward: -501.000, Loss: 295.150, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4835, Reward: -501.000, End reward: -501.000, Loss: 297.336, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4840, Reward: -501.000, End reward: -501.000, Loss: 297.057, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4845, Reward: -501.000, End reward: -501.000, Loss: 299.739, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4850, Reward: -501.000, End reward: -501.000, Loss: 301.672, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4855, Reward: -501.000, End reward: -501.000, Loss: 298.168, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4860, Reward: -501.000, End reward: -501.000, Loss: 296.335, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4865, Reward: -501.000, End reward: -501.000, Loss: 297.855, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4870, Reward: -501.000, End reward: -501.000, Loss: 300.599, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4875, Reward: -501.000, End reward: -501.000, Loss: 300.197, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4880, Reward: -501.000, End reward: -501.000, Loss: 297.028, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4885, Reward: -501.000, End reward: -501.000, Loss: 298.421, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4890, Reward: -501.000, End reward: -501.000, Loss: 300.432, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4895, Reward: -501.000, End reward: -501.000, Loss: 296.959, Grad mean: 0.000005, Grad max: 0.126, Actions: 501, Entropy: 0.000\n",
      "Episode 4900, Reward: -501.000, End reward: -501.000, Loss: 297.425, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4905, Reward: -501.000, End reward: -501.000, Loss: 297.138, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4910, Reward: -501.000, End reward: -501.000, Loss: 299.709, Grad mean: 0.000005, Grad max: 0.126, Actions: 501, Entropy: 0.000\n",
      "Episode 4915, Reward: -501.000, End reward: -501.000, Loss: 297.702, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4920, Reward: -501.000, End reward: -501.000, Loss: 300.007, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4925, Reward: -501.000, End reward: -501.000, Loss: 296.614, Grad mean: 0.000005, Grad max: 0.126, Actions: 501, Entropy: 0.000\n",
      "Episode 4930, Reward: -501.000, End reward: -501.000, Loss: 298.336, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4935, Reward: -501.000, End reward: -501.000, Loss: 298.896, Grad mean: 0.000005, Grad max: 0.126, Actions: 501, Entropy: 0.000\n",
      "Episode 4940, Reward: -501.000, End reward: -501.000, Loss: 296.943, Grad mean: 0.000005, Grad max: 0.126, Actions: 501, Entropy: 0.000\n",
      "Episode 4945, Reward: -501.000, End reward: -501.000, Loss: 301.648, Grad mean: 0.000005, Grad max: 0.126, Actions: 501, Entropy: 0.000\n",
      "Episode 4950, Reward: -501.000, End reward: -501.000, Loss: 298.810, Grad mean: 0.000005, Grad max: 0.126, Actions: 501, Entropy: 0.000\n",
      "Episode 4955, Reward: -501.000, End reward: -501.000, Loss: 303.064, Grad mean: 0.000005, Grad max: 0.126, Actions: 501, Entropy: 0.000\n",
      "Episode 4960, Reward: -501.000, End reward: -501.000, Loss: 295.032, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4965, Reward: -501.000, End reward: -501.000, Loss: 298.533, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4970, Reward: -501.000, End reward: -501.000, Loss: 298.332, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4975, Reward: -501.000, End reward: -501.000, Loss: 296.880, Grad mean: 0.000006, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 4980, Reward: -501.000, End reward: -501.000, Loss: 301.806, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4985, Reward: -501.000, End reward: -501.000, Loss: 307.518, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4990, Reward: -501.000, End reward: -501.000, Loss: 295.560, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 4995, Reward: -501.000, End reward: -501.000, Loss: 298.296, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 5000, Reward: -501.000, End reward: -501.000, Loss: 297.908, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 5005, Reward: -501.000, End reward: -501.000, Loss: 296.719, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 5010, Reward: -501.000, End reward: -501.000, Loss: 294.763, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 5015, Reward: -501.000, End reward: -501.000, Loss: 299.625, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 5020, Reward: -501.000, End reward: -501.000, Loss: 301.286, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 5025, Reward: -501.000, End reward: -501.000, Loss: 302.504, Grad mean: 0.000006, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 5030, Reward: -501.000, End reward: -501.000, Loss: 305.045, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 5035, Reward: -501.000, End reward: -501.000, Loss: 306.762, Grad mean: 0.000005, Grad max: 0.126, Actions: 501, Entropy: 0.000\n",
      "Episode 5040, Reward: -501.000, End reward: -501.000, Loss: 301.070, Grad mean: 0.000005, Grad max: 0.126, Actions: 501, Entropy: 0.000\n",
      "Episode 5045, Reward: -501.000, End reward: -501.000, Loss: 296.882, Grad mean: 0.000005, Grad max: 0.124, Actions: 501, Entropy: 0.000\n",
      "Episode 5050, Reward: -501.000, End reward: -501.000, Loss: 301.242, Grad mean: 0.000005, Grad max: 0.126, Actions: 501, Entropy: 0.000\n",
      "Episode 5055, Reward: -501.000, End reward: -501.000, Loss: 304.437, Grad mean: 0.000005, Grad max: 0.126, Actions: 501, Entropy: 0.000\n",
      "Episode 5060, Reward: -501.000, End reward: -501.000, Loss: 296.786, Grad mean: 0.000005, Grad max: 0.126, Actions: 501, Entropy: 0.000\n",
      "Episode 5065, Reward: -501.000, End reward: -501.000, Loss: 296.314, Grad mean: 0.000005, Grad max: 0.126, Actions: 501, Entropy: 0.000\n",
      "Episode 5070, Reward: -501.000, End reward: -501.000, Loss: 295.372, Grad mean: 0.000005, Grad max: 0.126, Actions: 501, Entropy: 0.000\n",
      "Episode 5075, Reward: -501.000, End reward: -501.000, Loss: 298.182, Grad mean: 0.000005, Grad max: 0.126, Actions: 501, Entropy: 0.000\n",
      "Episode 5080, Reward: -501.000, End reward: -501.000, Loss: 302.785, Grad mean: 0.000005, Grad max: 0.126, Actions: 501, Entropy: 0.000\n",
      "Episode 5085, Reward: -501.000, End reward: -501.000, Loss: 298.899, Grad mean: 0.000005, Grad max: 0.126, Actions: 501, Entropy: 0.000\n",
      "Episode 5090, Reward: -501.000, End reward: -501.000, Loss: 294.912, Grad mean: 0.000005, Grad max: 0.126, Actions: 501, Entropy: 0.000\n",
      "Episode 5095, Reward: -501.000, End reward: -501.000, Loss: 298.016, Grad mean: 0.000005, Grad max: 0.126, Actions: 501, Entropy: 0.000\n",
      "Episode 5100, Reward: -501.000, End reward: -501.000, Loss: 296.547, Grad mean: 0.000005, Grad max: 0.126, Actions: 501, Entropy: 0.000\n",
      "Episode 5105, Reward: -501.000, End reward: -501.000, Loss: 300.869, Grad mean: 0.000005, Grad max: 0.126, Actions: 501, Entropy: 0.000\n",
      "Episode 5110, Reward: -501.000, End reward: -501.000, Loss: 296.882, Grad mean: 0.000005, Grad max: 0.126, Actions: 501, Entropy: 0.000\n",
      "Episode 5115, Reward: -501.000, End reward: -501.000, Loss: 297.346, Grad mean: 0.000005, Grad max: 0.127, Actions: 501, Entropy: 0.000\n",
      "Episode 5120, Reward: -501.000, End reward: -501.000, Loss: 301.989, Grad mean: 0.000005, Grad max: 0.126, Actions: 501, Entropy: 0.000\n",
      "Episode 5125, Reward: -501.000, End reward: -501.000, Loss: 298.900, Grad mean: 0.000005, Grad max: 0.127, Actions: 501, Entropy: 0.000\n",
      "Episode 5130, Reward: -501.000, End reward: -501.000, Loss: 298.131, Grad mean: 0.000005, Grad max: 0.127, Actions: 501, Entropy: 0.000\n",
      "Episode 5135, Reward: -501.000, End reward: -501.000, Loss: 296.348, Grad mean: 0.000005, Grad max: 0.127, Actions: 501, Entropy: 0.000\n",
      "Episode 5140, Reward: -501.000, End reward: -501.000, Loss: 301.130, Grad mean: 0.000005, Grad max: 0.127, Actions: 501, Entropy: 0.000\n",
      "Episode 5145, Reward: -501.000, End reward: -501.000, Loss: 299.432, Grad mean: 0.000005, Grad max: 0.127, Actions: 501, Entropy: 0.000\n",
      "Episode 5150, Reward: -501.000, End reward: -501.000, Loss: 295.478, Grad mean: 0.000005, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 5155, Reward: -501.000, End reward: -501.000, Loss: 294.772, Grad mean: 0.000006, Grad max: 0.125, Actions: 501, Entropy: 0.000\n",
      "Episode 5160, Reward: -501.000, End reward: -501.000, Loss: 294.575, Grad mean: 0.000005, Grad max: 0.127, Actions: 501, Entropy: 0.000\n",
      "Episode 5165, Reward: -501.000, End reward: -501.000, Loss: 294.891, Grad mean: 0.000005, Grad max: 0.127, Actions: 501, Entropy: 0.000\n",
      "Episode 5170, Reward: -501.000, End reward: -501.000, Loss: 297.637, Grad mean: 0.000005, Grad max: 0.127, Actions: 501, Entropy: 0.000\n",
      "Episode 5175, Reward: -501.000, End reward: -501.000, Loss: 296.644, Grad mean: 0.000005, Grad max: 0.127, Actions: 501, Entropy: 0.000\n",
      "Episode 5180, Reward: -501.000, End reward: -501.000, Loss: 295.833, Grad mean: 0.000006, Grad max: 0.158, Actions: 501, Entropy: 0.000\n",
      "Episode 5185, Reward: -501.000, End reward: -501.000, Loss: 298.040, Grad mean: 0.000005, Grad max: 0.127, Actions: 501, Entropy: 0.000\n",
      "Episode 5190, Reward: -501.000, End reward: -501.000, Loss: 299.409, Grad mean: 0.000005, Grad max: 0.127, Actions: 501, Entropy: 0.000\n",
      "Episode 5195, Reward: -501.000, End reward: -501.000, Loss: 297.865, Grad mean: 0.000005, Grad max: 0.127, Actions: 501, Entropy: 0.000\n",
      "Episode 5200, Reward: -501.000, End reward: -501.000, Loss: 296.664, Grad mean: 0.000005, Grad max: 0.127, Actions: 501, Entropy: 0.000\n",
      "Episode 5205, Reward: -501.000, End reward: -501.000, Loss: 298.850, Grad mean: 0.000005, Grad max: 0.127, Actions: 501, Entropy: 0.000\n",
      "Episode 5210, Reward: -501.000, End reward: -501.000, Loss: 301.394, Grad mean: 0.000005, Grad max: 0.127, Actions: 501, Entropy: 0.000\n",
      "Episode 5215, Reward: -501.000, End reward: -501.000, Loss: 299.314, Grad mean: 0.000005, Grad max: 0.127, Actions: 501, Entropy: 0.000\n",
      "Episode 5220, Reward: -501.000, End reward: -501.000, Loss: 299.469, Grad mean: 0.000005, Grad max: 0.128, Actions: 501, Entropy: 0.000\n",
      "Episode 5225, Reward: -501.000, End reward: -501.000, Loss: 297.051, Grad mean: 0.000005, Grad max: 0.127, Actions: 501, Entropy: 0.000\n",
      "Episode 5230, Reward: -501.000, End reward: -501.000, Loss: 294.741, Grad mean: 0.000005, Grad max: 0.128, Actions: 501, Entropy: 0.000\n",
      "Episode 5235, Reward: -501.000, End reward: -501.000, Loss: 296.551, Grad mean: 0.000005, Grad max: 0.128, Actions: 501, Entropy: 0.000\n",
      "Episode 5240, Reward: -501.000, End reward: -501.000, Loss: 298.255, Grad mean: 0.000005, Grad max: 0.127, Actions: 501, Entropy: 0.000\n",
      "Episode 5245, Reward: -501.000, End reward: -501.000, Loss: 304.391, Grad mean: 0.000005, Grad max: 0.128, Actions: 501, Entropy: 0.000\n",
      "Episode 5250, Reward: -501.000, End reward: -501.000, Loss: 296.603, Grad mean: 0.000005, Grad max: 0.128, Actions: 501, Entropy: 0.000\n",
      "Episode 5255, Reward: -501.000, End reward: -501.000, Loss: 310.637, Grad mean: 0.000005, Grad max: 0.127, Actions: 501, Entropy: 0.000\n",
      "Episode 5260, Reward: -501.000, End reward: -501.000, Loss: 302.909, Grad mean: 0.000005, Grad max: 0.128, Actions: 501, Entropy: 0.000\n",
      "Episode 5265, Reward: -501.000, End reward: -501.000, Loss: 297.746, Grad mean: 0.000005, Grad max: 0.128, Actions: 501, Entropy: 0.000\n",
      "Episode 5270, Reward: -501.000, End reward: -501.000, Loss: 307.084, Grad mean: 0.000005, Grad max: 0.129, Actions: 501, Entropy: 0.000\n",
      "Episode 5275, Reward: -501.000, End reward: -501.000, Loss: 300.374, Grad mean: 0.000005, Grad max: 0.129, Actions: 501, Entropy: 0.000\n",
      "Episode 5280, Reward: -501.000, End reward: -501.000, Loss: 296.565, Grad mean: 0.000005, Grad max: 0.127, Actions: 501, Entropy: 0.000\n",
      "Episode 5285, Reward: -501.000, End reward: -501.000, Loss: 296.798, Grad mean: 0.000006, Grad max: 0.126, Actions: 501, Entropy: 0.000\n",
      "Episode 5290, Reward: -501.000, End reward: -501.000, Loss: 296.658, Grad mean: 0.000005, Grad max: 0.128, Actions: 501, Entropy: 0.000\n",
      "Episode 5295, Reward: -501.000, End reward: -501.000, Loss: 300.387, Grad mean: 0.000005, Grad max: 0.129, Actions: 501, Entropy: 0.000\n",
      "Episode 5300, Reward: -501.000, End reward: -501.000, Loss: 299.515, Grad mean: 0.000005, Grad max: 0.128, Actions: 501, Entropy: 0.000\n",
      "Episode 5305, Reward: -501.000, End reward: -501.000, Loss: 297.019, Grad mean: 0.000005, Grad max: 0.129, Actions: 501, Entropy: 0.000\n",
      "Episode 5310, Reward: -501.000, End reward: -501.000, Loss: 300.866, Grad mean: 0.000005, Grad max: 0.129, Actions: 501, Entropy: 0.000\n",
      "Episode 5315, Reward: -501.000, End reward: -501.000, Loss: 305.803, Grad mean: 0.000005, Grad max: 0.130, Actions: 501, Entropy: 0.000\n",
      "Episode 5320, Reward: -501.000, End reward: -501.000, Loss: 299.541, Grad mean: 0.000005, Grad max: 0.129, Actions: 501, Entropy: 0.000\n",
      "Episode 5325, Reward: -501.000, End reward: -501.000, Loss: 298.882, Grad mean: 0.000005, Grad max: 0.130, Actions: 501, Entropy: 0.000\n",
      "Episode 5330, Reward: -501.000, End reward: -501.000, Loss: 301.505, Grad mean: 0.000005, Grad max: 0.130, Actions: 501, Entropy: 0.000\n",
      "Episode 5335, Reward: -501.000, End reward: -501.000, Loss: 297.595, Grad mean: 0.000005, Grad max: 0.130, Actions: 501, Entropy: 0.000\n",
      "Episode 5340, Reward: -501.000, End reward: -501.000, Loss: 297.576, Grad mean: 0.000005, Grad max: 0.130, Actions: 501, Entropy: 0.000\n",
      "Episode 5345, Reward: -501.000, End reward: -501.000, Loss: 298.259, Grad mean: 0.000005, Grad max: 0.130, Actions: 501, Entropy: 0.000\n",
      "Episode 5350, Reward: -501.000, End reward: -501.000, Loss: 299.897, Grad mean: 0.000005, Grad max: 0.130, Actions: 501, Entropy: 0.000\n",
      "Episode 5355, Reward: -501.000, End reward: -501.000, Loss: 299.855, Grad mean: 0.000005, Grad max: 0.130, Actions: 501, Entropy: 0.000\n",
      "Episode 5360, Reward: -501.000, End reward: -501.000, Loss: 295.788, Grad mean: 0.000005, Grad max: 0.130, Actions: 501, Entropy: 0.000\n",
      "Episode 5365, Reward: -501.000, End reward: -501.000, Loss: 298.427, Grad mean: 0.000005, Grad max: 0.130, Actions: 501, Entropy: 0.000\n",
      "Episode 5370, Reward: -501.000, End reward: -501.000, Loss: 299.766, Grad mean: 0.000005, Grad max: 0.131, Actions: 501, Entropy: 0.000\n",
      "Episode 5375, Reward: -501.000, End reward: -501.000, Loss: 299.509, Grad mean: 0.000005, Grad max: 0.130, Actions: 501, Entropy: 0.000\n",
      "Episode 5380, Reward: -501.000, End reward: -501.000, Loss: 297.438, Grad mean: 0.000005, Grad max: 0.131, Actions: 501, Entropy: 0.000\n",
      "Episode 5385, Reward: -501.000, End reward: -501.000, Loss: 296.302, Grad mean: 0.000005, Grad max: 0.131, Actions: 501, Entropy: 0.000\n",
      "Episode 5390, Reward: -501.000, End reward: -501.000, Loss: 301.176, Grad mean: 0.000005, Grad max: 0.130, Actions: 501, Entropy: 0.000\n",
      "Episode 5395, Reward: -501.000, End reward: -501.000, Loss: 301.688, Grad mean: 0.000005, Grad max: 0.131, Actions: 501, Entropy: 0.000\n",
      "Episode 5400, Reward: -501.000, End reward: -501.000, Loss: 296.788, Grad mean: 0.000005, Grad max: 0.129, Actions: 501, Entropy: 0.000\n",
      "Episode 5405, Reward: -501.000, End reward: -501.000, Loss: 297.100, Grad mean: 0.000005, Grad max: 0.130, Actions: 501, Entropy: 0.000\n",
      "Episode 5410, Reward: -501.000, End reward: -501.000, Loss: 297.108, Grad mean: 0.000005, Grad max: 0.130, Actions: 501, Entropy: 0.000\n",
      "Episode 5415, Reward: -501.000, End reward: -501.000, Loss: 297.473, Grad mean: 0.000005, Grad max: 0.130, Actions: 501, Entropy: 0.000\n",
      "Episode 5420, Reward: -501.000, End reward: -501.000, Loss: 296.998, Grad mean: 0.000005, Grad max: 0.130, Actions: 501, Entropy: 0.000\n",
      "Episode 5425, Reward: -501.000, End reward: -501.000, Loss: 295.116, Grad mean: 0.000005, Grad max: 0.131, Actions: 501, Entropy: 0.000\n",
      "Episode 5430, Reward: -501.000, End reward: -501.000, Loss: 297.966, Grad mean: 0.000005, Grad max: 0.131, Actions: 501, Entropy: 0.000\n",
      "Episode 5435, Reward: -501.000, End reward: -501.000, Loss: 294.980, Grad mean: 0.000005, Grad max: 0.132, Actions: 501, Entropy: 0.000\n",
      "Episode 5440, Reward: -501.000, End reward: -501.000, Loss: 299.113, Grad mean: 0.000005, Grad max: 0.132, Actions: 501, Entropy: 0.000\n",
      "Episode 5445, Reward: -501.000, End reward: -501.000, Loss: 299.624, Grad mean: 0.000005, Grad max: 0.131, Actions: 501, Entropy: 0.000\n",
      "Episode 5450, Reward: -501.000, End reward: -501.000, Loss: 299.830, Grad mean: 0.000005, Grad max: 0.132, Actions: 501, Entropy: 0.000\n",
      "Episode 5455, Reward: -501.000, End reward: -501.000, Loss: 299.448, Grad mean: 0.000005, Grad max: 0.132, Actions: 501, Entropy: 0.000\n",
      "Episode 5460, Reward: -501.000, End reward: -501.000, Loss: 298.144, Grad mean: 0.000005, Grad max: 0.131, Actions: 501, Entropy: 0.000\n",
      "Episode 5465, Reward: -501.000, End reward: -501.000, Loss: 300.899, Grad mean: 0.000005, Grad max: 0.132, Actions: 501, Entropy: 0.000\n",
      "Episode 5470, Reward: -501.000, End reward: -501.000, Loss: 299.281, Grad mean: 0.000005, Grad max: 0.131, Actions: 501, Entropy: 0.000\n",
      "Episode 5475, Reward: -501.000, End reward: -501.000, Loss: 298.069, Grad mean: 0.000005, Grad max: 0.132, Actions: 501, Entropy: 0.000\n",
      "Episode 5480, Reward: -501.000, End reward: -501.000, Loss: 297.837, Grad mean: 0.000005, Grad max: 0.132, Actions: 501, Entropy: 0.000\n",
      "Episode 5485, Reward: -501.000, End reward: -501.000, Loss: 299.087, Grad mean: 0.000005, Grad max: 0.131, Actions: 501, Entropy: 0.000\n",
      "Episode 5490, Reward: -501.000, End reward: -501.000, Loss: 300.266, Grad mean: 0.000005, Grad max: 0.132, Actions: 501, Entropy: 0.000\n",
      "Episode 5495, Reward: -501.000, End reward: -501.000, Loss: 297.641, Grad mean: 0.000005, Grad max: 0.130, Actions: 501, Entropy: 0.000\n",
      "Episode 5500, Reward: -501.000, End reward: -501.000, Loss: 297.702, Grad mean: 0.000005, Grad max: 0.132, Actions: 501, Entropy: 0.000\n",
      "Episode 5505, Reward: -501.000, End reward: -501.000, Loss: 303.104, Grad mean: 0.000005, Grad max: 0.132, Actions: 501, Entropy: 0.000\n",
      "Episode 5510, Reward: -501.000, End reward: -501.000, Loss: 310.198, Grad mean: 0.000005, Grad max: 0.131, Actions: 501, Entropy: 0.000\n",
      "Episode 5515, Reward: -501.000, End reward: -501.000, Loss: 302.986, Grad mean: 0.000005, Grad max: 0.132, Actions: 501, Entropy: 0.000\n",
      "Episode 5520, Reward: -501.000, End reward: -501.000, Loss: 299.191, Grad mean: 0.000005, Grad max: 0.132, Actions: 501, Entropy: 0.000\n",
      "Episode 5525, Reward: -501.000, End reward: -501.000, Loss: 298.341, Grad mean: 0.000005, Grad max: 0.132, Actions: 501, Entropy: 0.000\n",
      "Episode 5530, Reward: -501.000, End reward: -501.000, Loss: 297.807, Grad mean: 0.000005, Grad max: 0.132, Actions: 501, Entropy: 0.000\n",
      "Episode 5535, Reward: -501.000, End reward: -501.000, Loss: 297.091, Grad mean: 0.000005, Grad max: 0.132, Actions: 501, Entropy: 0.000\n",
      "Episode 5540, Reward: -501.000, End reward: -501.000, Loss: 298.705, Grad mean: 0.000005, Grad max: 0.132, Actions: 501, Entropy: 0.000\n",
      "Episode 5545, Reward: -501.000, End reward: -501.000, Loss: 299.510, Grad mean: 0.000005, Grad max: 0.132, Actions: 501, Entropy: 0.000\n",
      "Episode 5550, Reward: -501.000, End reward: -501.000, Loss: 300.208, Grad mean: 0.000005, Grad max: 0.132, Actions: 501, Entropy: 0.000\n",
      "Episode 5555, Reward: -501.000, End reward: -501.000, Loss: 296.488, Grad mean: 0.000005, Grad max: 0.132, Actions: 501, Entropy: 0.000\n",
      "Episode 5560, Reward: -501.000, End reward: -501.000, Loss: 298.407, Grad mean: 0.000005, Grad max: 0.132, Actions: 501, Entropy: 0.000\n",
      "Episode 5565, Reward: -501.000, End reward: -501.000, Loss: 296.690, Grad mean: 0.000005, Grad max: 0.132, Actions: 501, Entropy: 0.000\n",
      "Episode 5570, Reward: -501.000, End reward: -501.000, Loss: 296.570, Grad mean: 0.000005, Grad max: 0.132, Actions: 501, Entropy: 0.000\n",
      "Episode 5575, Reward: -501.000, End reward: -501.000, Loss: 302.242, Grad mean: 0.000006, Grad max: 0.127, Actions: 501, Entropy: 0.000\n",
      "Episode 5580, Reward: -501.000, End reward: -501.000, Loss: 298.072, Grad mean: 0.000005, Grad max: 0.131, Actions: 501, Entropy: 0.000\n",
      "Episode 5585, Reward: -501.000, End reward: -501.000, Loss: 298.111, Grad mean: 0.000005, Grad max: 0.132, Actions: 501, Entropy: 0.000\n",
      "Episode 5590, Reward: -501.000, End reward: -501.000, Loss: 299.093, Grad mean: 0.000005, Grad max: 0.131, Actions: 501, Entropy: 0.000\n",
      "Episode 5595, Reward: -501.000, End reward: -501.000, Loss: 299.399, Grad mean: 0.000005, Grad max: 0.132, Actions: 501, Entropy: 0.000\n",
      "Episode 5600, Reward: -501.000, End reward: -501.000, Loss: 297.937, Grad mean: 0.000005, Grad max: 0.133, Actions: 501, Entropy: 0.000\n",
      "Episode 5605, Reward: -501.000, End reward: -501.000, Loss: 297.025, Grad mean: 0.000005, Grad max: 0.131, Actions: 501, Entropy: 0.000\n",
      "Episode 5610, Reward: -501.000, End reward: -501.000, Loss: 299.375, Grad mean: 0.000005, Grad max: 0.133, Actions: 501, Entropy: 0.000\n",
      "Episode 5615, Reward: -501.000, End reward: -501.000, Loss: 301.712, Grad mean: 0.000005, Grad max: 0.132, Actions: 501, Entropy: 0.000\n",
      "Episode 5620, Reward: -501.000, End reward: -501.000, Loss: 312.816, Grad mean: 0.000005, Grad max: 0.133, Actions: 501, Entropy: 0.000\n",
      "Episode 5625, Reward: -501.000, End reward: -501.000, Loss: 314.046, Grad mean: 0.000005, Grad max: 0.132, Actions: 501, Entropy: 0.000\n",
      "Episode 5630, Reward: -501.000, End reward: -501.000, Loss: 295.958, Grad mean: 0.000005, Grad max: 0.132, Actions: 501, Entropy: 0.000\n",
      "Episode 5635, Reward: -501.000, End reward: -501.000, Loss: 296.645, Grad mean: 0.000005, Grad max: 0.132, Actions: 501, Entropy: 0.000\n",
      "Episode 5640, Reward: -501.000, End reward: -501.000, Loss: 302.611, Grad mean: 0.000005, Grad max: 0.133, Actions: 501, Entropy: 0.000\n",
      "Episode 5645, Reward: -501.000, End reward: -501.000, Loss: 301.264, Grad mean: 0.000005, Grad max: 0.133, Actions: 501, Entropy: 0.000\n",
      "Episode 5650, Reward: -501.000, End reward: -501.000, Loss: 297.170, Grad mean: 0.000004, Grad max: 0.133, Actions: 501, Entropy: 0.000\n",
      "Episode 5655, Reward: -501.000, End reward: -501.000, Loss: 299.101, Grad mean: 0.000005, Grad max: 0.133, Actions: 501, Entropy: 0.000\n",
      "Episode 5660, Reward: -501.000, End reward: -501.000, Loss: 297.153, Grad mean: 0.000005, Grad max: 0.133, Actions: 501, Entropy: 0.000\n",
      "Episode 5665, Reward: -501.000, End reward: -501.000, Loss: 298.149, Grad mean: 0.000005, Grad max: 0.133, Actions: 501, Entropy: 0.000\n",
      "Episode 5670, Reward: -501.000, End reward: -501.000, Loss: 297.422, Grad mean: 0.000005, Grad max: 0.132, Actions: 501, Entropy: 0.000\n",
      "Episode 5675, Reward: -501.000, End reward: -501.000, Loss: 296.653, Grad mean: 0.000005, Grad max: 0.132, Actions: 501, Entropy: 0.000\n",
      "Episode 5680, Reward: -501.000, End reward: -501.000, Loss: 297.870, Grad mean: 0.000005, Grad max: 0.133, Actions: 501, Entropy: 0.000\n",
      "Episode 5685, Reward: -501.000, End reward: -501.000, Loss: 314.624, Grad mean: 0.000005, Grad max: 0.133, Actions: 501, Entropy: 0.000\n",
      "Episode 5690, Reward: -501.000, End reward: -501.000, Loss: 303.909, Grad mean: 0.000005, Grad max: 0.131, Actions: 501, Entropy: 0.000\n",
      "Episode 5695, Reward: -501.000, End reward: -501.000, Loss: 299.767, Grad mean: 0.000005, Grad max: 0.133, Actions: 501, Entropy: 0.000\n",
      "Episode 5700, Reward: -501.000, End reward: -501.000, Loss: 303.595, Grad mean: 0.000005, Grad max: 0.134, Actions: 501, Entropy: 0.000\n",
      "Episode 5705, Reward: -501.000, End reward: -501.000, Loss: 298.065, Grad mean: 0.000005, Grad max: 0.133, Actions: 501, Entropy: 0.000\n",
      "Episode 5710, Reward: -501.000, End reward: -501.000, Loss: 299.709, Grad mean: 0.000005, Grad max: 0.134, Actions: 501, Entropy: 0.000\n",
      "Episode 5715, Reward: -501.000, End reward: -501.000, Loss: 303.814, Grad mean: 0.000005, Grad max: 0.133, Actions: 501, Entropy: 0.000\n",
      "Episode 5720, Reward: -501.000, End reward: -501.000, Loss: 297.264, Grad mean: 0.000005, Grad max: 0.134, Actions: 501, Entropy: 0.000\n",
      "Episode 5725, Reward: -501.000, End reward: -501.000, Loss: 297.011, Grad mean: 0.000005, Grad max: 0.133, Actions: 501, Entropy: 0.000\n",
      "Episode 5730, Reward: -501.000, End reward: -501.000, Loss: 296.874, Grad mean: 0.000004, Grad max: 0.134, Actions: 501, Entropy: 0.000\n",
      "Episode 5735, Reward: -501.000, End reward: -501.000, Loss: 298.474, Grad mean: 0.000005, Grad max: 0.134, Actions: 501, Entropy: 0.000\n",
      "Episode 5740, Reward: -501.000, End reward: -501.000, Loss: 298.671, Grad mean: 0.000006, Grad max: 0.130, Actions: 501, Entropy: 0.000\n",
      "Episode 5745, Reward: -501.000, End reward: -501.000, Loss: 298.091, Grad mean: 0.000004, Grad max: 0.135, Actions: 501, Entropy: 0.000\n",
      "Episode 5750, Reward: -501.000, End reward: -501.000, Loss: 295.681, Grad mean: 0.000004, Grad max: 0.135, Actions: 501, Entropy: 0.000\n",
      "Episode 5755, Reward: -501.000, End reward: -501.000, Loss: 296.979, Grad mean: 0.000004, Grad max: 0.134, Actions: 501, Entropy: 0.000\n",
      "Episode 5760, Reward: -501.000, End reward: -501.000, Loss: 298.300, Grad mean: 0.000005, Grad max: 0.135, Actions: 501, Entropy: 0.000\n",
      "Episode 5765, Reward: -501.000, End reward: -501.000, Loss: 296.943, Grad mean: 0.000004, Grad max: 0.135, Actions: 501, Entropy: 0.000\n",
      "Episode 5770, Reward: -501.000, End reward: -501.000, Loss: 297.310, Grad mean: 0.000004, Grad max: 0.136, Actions: 501, Entropy: 0.000\n",
      "Episode 5775, Reward: -501.000, End reward: -501.000, Loss: 297.362, Grad mean: 0.000005, Grad max: 0.131, Actions: 501, Entropy: 0.000\n",
      "Episode 5780, Reward: -501.000, End reward: -501.000, Loss: 295.741, Grad mean: 0.000004, Grad max: 0.136, Actions: 501, Entropy: 0.000\n",
      "Episode 5785, Reward: -501.000, End reward: -501.000, Loss: 296.349, Grad mean: 0.000004, Grad max: 0.171, Actions: 501, Entropy: 0.000\n",
      "Episode 5790, Reward: -501.000, End reward: -501.000, Loss: 296.461, Grad mean: 0.000004, Grad max: 0.135, Actions: 501, Entropy: 0.000\n",
      "Episode 5795, Reward: -501.000, End reward: -501.000, Loss: 296.572, Grad mean: 0.000004, Grad max: 0.135, Actions: 501, Entropy: 0.000\n",
      "Episode 5800, Reward: -501.000, End reward: -501.000, Loss: 296.662, Grad mean: 0.000005, Grad max: 0.133, Actions: 501, Entropy: 0.000\n",
      "Episode 5805, Reward: -501.000, End reward: -501.000, Loss: 295.772, Grad mean: 0.000005, Grad max: 0.135, Actions: 501, Entropy: 0.000\n",
      "Episode 5810, Reward: -501.000, End reward: -501.000, Loss: 298.421, Grad mean: 0.000004, Grad max: 0.135, Actions: 501, Entropy: 0.000\n",
      "Episode 5815, Reward: -501.000, End reward: -501.000, Loss: 295.077, Grad mean: 0.000005, Grad max: 0.135, Actions: 501, Entropy: 0.000\n",
      "Episode 5820, Reward: -501.000, End reward: -501.000, Loss: 296.868, Grad mean: 0.000005, Grad max: 0.134, Actions: 501, Entropy: 0.000\n",
      "Episode 5825, Reward: -501.000, End reward: -501.000, Loss: 297.380, Grad mean: 0.000005, Grad max: 0.135, Actions: 501, Entropy: 0.000\n",
      "Episode 5830, Reward: -501.000, End reward: -501.000, Loss: 296.793, Grad mean: 0.000005, Grad max: 0.131, Actions: 501, Entropy: 0.000\n",
      "Episode 5835, Reward: -501.000, End reward: -501.000, Loss: 295.723, Grad mean: 0.000005, Grad max: 0.135, Actions: 501, Entropy: 0.000\n",
      "Episode 5840, Reward: -501.000, End reward: -501.000, Loss: 299.267, Grad mean: 0.000005, Grad max: 0.135, Actions: 501, Entropy: 0.000\n",
      "Episode 5845, Reward: -501.000, End reward: -501.000, Loss: 301.082, Grad mean: 0.000005, Grad max: 0.134, Actions: 501, Entropy: 0.000\n",
      "Episode 5850, Reward: -501.000, End reward: -501.000, Loss: 299.002, Grad mean: 0.000004, Grad max: 0.135, Actions: 501, Entropy: 0.000\n",
      "Episode 5855, Reward: -501.000, End reward: -501.000, Loss: 297.734, Grad mean: 0.000004, Grad max: 0.135, Actions: 501, Entropy: 0.000\n",
      "Episode 5860, Reward: -501.000, End reward: -501.000, Loss: 303.643, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 5865, Reward: -501.000, End reward: -501.000, Loss: 300.626, Grad mean: 0.000004, Grad max: 0.136, Actions: 501, Entropy: 0.000\n",
      "Episode 5870, Reward: -501.000, End reward: -501.000, Loss: 299.488, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 5875, Reward: -501.000, End reward: -501.000, Loss: 297.521, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 5880, Reward: -501.000, End reward: -501.000, Loss: 297.324, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 5885, Reward: -501.000, End reward: -501.000, Loss: 300.006, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 5890, Reward: -501.000, End reward: -501.000, Loss: 298.551, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 5895, Reward: -501.000, End reward: -501.000, Loss: 297.302, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 5900, Reward: -501.000, End reward: -501.000, Loss: 300.025, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 5905, Reward: -501.000, End reward: -501.000, Loss: 298.750, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 5910, Reward: -501.000, End reward: -501.000, Loss: 299.636, Grad mean: 0.000004, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 5915, Reward: -501.000, End reward: -501.000, Loss: 297.001, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 5920, Reward: -501.000, End reward: -501.000, Loss: 297.342, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 5925, Reward: -501.000, End reward: -501.000, Loss: 300.043, Grad mean: 0.000004, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 5930, Reward: -501.000, End reward: -501.000, Loss: 298.110, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 5935, Reward: -501.000, End reward: -501.000, Loss: 298.128, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 5940, Reward: -501.000, End reward: -501.000, Loss: 296.901, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 5945, Reward: -501.000, End reward: -501.000, Loss: 295.019, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 5950, Reward: -501.000, End reward: -501.000, Loss: 297.627, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 5955, Reward: -501.000, End reward: -501.000, Loss: 297.206, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 5960, Reward: -501.000, End reward: -501.000, Loss: 296.230, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 5965, Reward: -501.000, End reward: -501.000, Loss: 296.308, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 5970, Reward: -501.000, End reward: -501.000, Loss: 297.834, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 5975, Reward: -501.000, End reward: -501.000, Loss: 298.181, Grad mean: 0.000004, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 5980, Reward: -501.000, End reward: -501.000, Loss: 295.053, Grad mean: 0.000004, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 5985, Reward: -501.000, End reward: -501.000, Loss: 298.966, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 5990, Reward: -501.000, End reward: -501.000, Loss: 297.907, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 5995, Reward: -501.000, End reward: -501.000, Loss: 295.933, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6000, Reward: -501.000, End reward: -501.000, Loss: 297.657, Grad mean: 0.000004, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6005, Reward: -501.000, End reward: -501.000, Loss: 295.828, Grad mean: 0.000004, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6010, Reward: -501.000, End reward: -501.000, Loss: 296.629, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6015, Reward: -501.000, End reward: -501.000, Loss: 297.245, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6020, Reward: -501.000, End reward: -501.000, Loss: 297.312, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6025, Reward: -501.000, End reward: -501.000, Loss: 296.489, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6030, Reward: -501.000, End reward: -501.000, Loss: 297.020, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6035, Reward: -501.000, End reward: -501.000, Loss: 297.480, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6040, Reward: -501.000, End reward: -501.000, Loss: 296.083, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6045, Reward: -501.000, End reward: -501.000, Loss: 295.081, Grad mean: 0.000005, Grad max: 0.136, Actions: 501, Entropy: 0.000\n",
      "Episode 6050, Reward: -501.000, End reward: -501.000, Loss: 296.634, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6055, Reward: -501.000, End reward: -501.000, Loss: 300.154, Grad mean: 0.000005, Grad max: 0.136, Actions: 501, Entropy: 0.000\n",
      "Episode 6060, Reward: -501.000, End reward: -501.000, Loss: 298.830, Grad mean: 0.000005, Grad max: 0.136, Actions: 501, Entropy: 0.000\n",
      "Episode 6065, Reward: -501.000, End reward: -501.000, Loss: 295.946, Grad mean: 0.000004, Grad max: 0.136, Actions: 501, Entropy: 0.000\n",
      "Episode 6070, Reward: -501.000, End reward: -501.000, Loss: 296.582, Grad mean: 0.000004, Grad max: 0.136, Actions: 501, Entropy: 0.000\n",
      "Episode 6075, Reward: -501.000, End reward: -501.000, Loss: 296.739, Grad mean: 0.000004, Grad max: 0.136, Actions: 501, Entropy: 0.000\n",
      "Episode 6080, Reward: -501.000, End reward: -501.000, Loss: 297.014, Grad mean: 0.000005, Grad max: 0.136, Actions: 501, Entropy: 0.000\n",
      "Episode 6085, Reward: -501.000, End reward: -501.000, Loss: 294.511, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6090, Reward: -501.000, End reward: -501.000, Loss: 299.302, Grad mean: 0.000004, Grad max: 0.136, Actions: 501, Entropy: 0.000\n",
      "Episode 6095, Reward: -501.000, End reward: -501.000, Loss: 303.026, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6100, Reward: -501.000, End reward: -501.000, Loss: 299.905, Grad mean: 0.000004, Grad max: 0.136, Actions: 501, Entropy: 0.000\n",
      "Episode 6105, Reward: -501.000, End reward: -501.000, Loss: 301.620, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6110, Reward: -501.000, End reward: -501.000, Loss: 299.859, Grad mean: 0.000004, Grad max: 0.136, Actions: 501, Entropy: 0.000\n",
      "Episode 6115, Reward: -501.000, End reward: -501.000, Loss: 300.389, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6120, Reward: -501.000, End reward: -501.000, Loss: 299.945, Grad mean: 0.000004, Grad max: 0.136, Actions: 501, Entropy: 0.000\n",
      "Episode 6125, Reward: -501.000, End reward: -501.000, Loss: 301.194, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6130, Reward: -501.000, End reward: -501.000, Loss: 298.962, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6135, Reward: -501.000, End reward: -501.000, Loss: 300.101, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6140, Reward: -501.000, End reward: -501.000, Loss: 299.802, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6145, Reward: -501.000, End reward: -501.000, Loss: 298.465, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6150, Reward: -501.000, End reward: -501.000, Loss: 301.404, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6155, Reward: -501.000, End reward: -501.000, Loss: 298.049, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6160, Reward: -501.000, End reward: -501.000, Loss: 300.477, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6165, Reward: -501.000, End reward: -501.000, Loss: 297.216, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6170, Reward: -501.000, End reward: -501.000, Loss: 296.219, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6175, Reward: -501.000, End reward: -501.000, Loss: 298.822, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6180, Reward: -501.000, End reward: -501.000, Loss: 295.545, Grad mean: 0.000004, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6185, Reward: -501.000, End reward: -501.000, Loss: 294.700, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6190, Reward: -501.000, End reward: -501.000, Loss: 299.569, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6195, Reward: -501.000, End reward: -501.000, Loss: 296.563, Grad mean: 0.000004, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6200, Reward: -501.000, End reward: -501.000, Loss: 297.185, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6205, Reward: -501.000, End reward: -501.000, Loss: 297.400, Grad mean: 0.000004, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6210, Reward: -501.000, End reward: -501.000, Loss: 298.102, Grad mean: 0.000004, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6215, Reward: -501.000, End reward: -501.000, Loss: 296.904, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6220, Reward: -501.000, End reward: -501.000, Loss: 295.050, Grad mean: 0.000004, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6225, Reward: -501.000, End reward: -501.000, Loss: 297.666, Grad mean: 0.000004, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6230, Reward: -501.000, End reward: -501.000, Loss: 300.629, Grad mean: 0.000004, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6235, Reward: -501.000, End reward: -501.000, Loss: 296.186, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6240, Reward: -501.000, End reward: -501.000, Loss: 297.834, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6245, Reward: -501.000, End reward: -501.000, Loss: 294.563, Grad mean: 0.000004, Grad max: 0.212, Actions: 501, Entropy: 0.000\n",
      "Episode 6250, Reward: -501.000, End reward: -501.000, Loss: 298.740, Grad mean: 0.000004, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6255, Reward: -501.000, End reward: -501.000, Loss: 297.257, Grad mean: 0.000004, Grad max: 0.136, Actions: 501, Entropy: 0.000\n",
      "Episode 6260, Reward: -501.000, End reward: -501.000, Loss: 294.392, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6265, Reward: -501.000, End reward: -501.000, Loss: 295.607, Grad mean: 0.000004, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6270, Reward: -501.000, End reward: -501.000, Loss: 296.104, Grad mean: 0.000005, Grad max: 0.131, Actions: 501, Entropy: 0.000\n",
      "Episode 6275, Reward: -501.000, End reward: -501.000, Loss: 297.595, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6280, Reward: -501.000, End reward: -501.000, Loss: 298.616, Grad mean: 0.000004, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6285, Reward: -501.000, End reward: -501.000, Loss: 295.678, Grad mean: 0.000004, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6290, Reward: -501.000, End reward: -501.000, Loss: 298.170, Grad mean: 0.000004, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6295, Reward: -501.000, End reward: -501.000, Loss: 298.497, Grad mean: 0.000004, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6300, Reward: -501.000, End reward: -501.000, Loss: 300.151, Grad mean: 0.000004, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6305, Reward: -501.000, End reward: -501.000, Loss: 297.553, Grad mean: 0.000004, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6310, Reward: -501.000, End reward: -501.000, Loss: 296.651, Grad mean: 0.000008, Grad max: 0.250, Actions: 501, Entropy: 0.000\n",
      "Episode 6315, Reward: -501.000, End reward: -501.000, Loss: 297.385, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6320, Reward: -501.000, End reward: -501.000, Loss: 297.233, Grad mean: 0.000005, Grad max: 0.136, Actions: 501, Entropy: 0.000\n",
      "Episode 6325, Reward: -501.000, End reward: -501.000, Loss: 297.895, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6330, Reward: -501.000, End reward: -501.000, Loss: 296.550, Grad mean: 0.000005, Grad max: 0.136, Actions: 501, Entropy: 0.000\n",
      "Episode 6335, Reward: -501.000, End reward: -501.000, Loss: 298.485, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6340, Reward: -501.000, End reward: -501.000, Loss: 295.146, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6345, Reward: -501.000, End reward: -501.000, Loss: 297.685, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6350, Reward: -501.000, End reward: -501.000, Loss: 304.615, Grad mean: 0.000005, Grad max: 0.136, Actions: 501, Entropy: 0.000\n",
      "Episode 6355, Reward: -501.000, End reward: -501.000, Loss: 299.702, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6360, Reward: -501.000, End reward: -501.000, Loss: 301.549, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6365, Reward: -501.000, End reward: -501.000, Loss: 297.262, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6370, Reward: -501.000, End reward: -501.000, Loss: 296.308, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6375, Reward: -501.000, End reward: -501.000, Loss: 296.503, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6380, Reward: -501.000, End reward: -501.000, Loss: 296.191, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6385, Reward: -501.000, End reward: -501.000, Loss: 298.247, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6390, Reward: -501.000, End reward: -501.000, Loss: 295.171, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6395, Reward: -501.000, End reward: -501.000, Loss: 298.925, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6400, Reward: -501.000, End reward: -501.000, Loss: 297.936, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6405, Reward: -501.000, End reward: -501.000, Loss: 296.589, Grad mean: 0.000004, Grad max: 0.133, Actions: 501, Entropy: 0.000\n",
      "Episode 6410, Reward: -501.000, End reward: -501.000, Loss: 294.638, Grad mean: 0.000005, Grad max: 0.136, Actions: 501, Entropy: 0.000\n",
      "Episode 6415, Reward: -501.000, End reward: -501.000, Loss: 297.373, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6420, Reward: -501.000, End reward: -501.000, Loss: 300.984, Grad mean: 0.000004, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6425, Reward: -501.000, End reward: -501.000, Loss: 295.619, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6430, Reward: -501.000, End reward: -501.000, Loss: 296.813, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6435, Reward: -501.000, End reward: -501.000, Loss: 301.346, Grad mean: 0.000004, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6440, Reward: -501.000, End reward: -501.000, Loss: 298.499, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6445, Reward: -501.000, End reward: -501.000, Loss: 297.850, Grad mean: 0.000004, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6450, Reward: -501.000, End reward: -501.000, Loss: 295.241, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6455, Reward: -501.000, End reward: -501.000, Loss: 298.970, Grad mean: 0.000004, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6460, Reward: -501.000, End reward: -501.000, Loss: 300.166, Grad mean: 0.000004, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6465, Reward: -501.000, End reward: -501.000, Loss: 294.363, Grad mean: 0.000005, Grad max: 0.131, Actions: 501, Entropy: 0.000\n",
      "Episode 6470, Reward: -501.000, End reward: -501.000, Loss: 298.833, Grad mean: 0.000004, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6475, Reward: -501.000, End reward: -501.000, Loss: 296.707, Grad mean: 0.000004, Grad max: 0.129, Actions: 501, Entropy: 0.000\n",
      "Episode 6480, Reward: -501.000, End reward: -501.000, Loss: 295.087, Grad mean: 0.000005, Grad max: 0.136, Actions: 501, Entropy: 0.000\n",
      "Episode 6485, Reward: -501.000, End reward: -501.000, Loss: 297.294, Grad mean: 0.000004, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6490, Reward: -501.000, End reward: -501.000, Loss: 299.984, Grad mean: 0.000004, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6495, Reward: -501.000, End reward: -501.000, Loss: 297.022, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6500, Reward: -501.000, End reward: -501.000, Loss: 297.082, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6505, Reward: -501.000, End reward: -501.000, Loss: 297.035, Grad mean: 0.000004, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6510, Reward: -501.000, End reward: -501.000, Loss: 299.226, Grad mean: 0.000005, Grad max: 0.133, Actions: 501, Entropy: 0.000\n",
      "Episode 6515, Reward: -501.000, End reward: -501.000, Loss: 297.416, Grad mean: 0.000004, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6520, Reward: -501.000, End reward: -501.000, Loss: 297.042, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6525, Reward: -501.000, End reward: -501.000, Loss: 297.198, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6530, Reward: -501.000, End reward: -501.000, Loss: 296.672, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6535, Reward: -501.000, End reward: -501.000, Loss: 298.421, Grad mean: 0.000004, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6540, Reward: -501.000, End reward: -501.000, Loss: 299.726, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6545, Reward: -501.000, End reward: -501.000, Loss: 304.914, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6550, Reward: -501.000, End reward: -501.000, Loss: 297.225, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6555, Reward: -501.000, End reward: -501.000, Loss: 300.028, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6560, Reward: -501.000, End reward: -501.000, Loss: 297.594, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6565, Reward: -501.000, End reward: -501.000, Loss: 296.871, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6570, Reward: -501.000, End reward: -501.000, Loss: 299.591, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6575, Reward: -501.000, End reward: -501.000, Loss: 295.581, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6580, Reward: -501.000, End reward: -501.000, Loss: 303.135, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6585, Reward: -501.000, End reward: -501.000, Loss: 307.342, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 6590, Reward: -501.000, End reward: -501.000, Loss: 297.772, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6595, Reward: -501.000, End reward: -501.000, Loss: 297.291, Grad mean: 0.000004, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 6600, Reward: -501.000, End reward: -501.000, Loss: 303.601, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6605, Reward: -501.000, End reward: -501.000, Loss: 296.479, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6610, Reward: -501.000, End reward: -501.000, Loss: 296.542, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6615, Reward: -501.000, End reward: -501.000, Loss: 299.114, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 6620, Reward: -501.000, End reward: -501.000, Loss: 300.213, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6625, Reward: -501.000, End reward: -501.000, Loss: 297.350, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 6630, Reward: -501.000, End reward: -501.000, Loss: 297.646, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6635, Reward: -501.000, End reward: -501.000, Loss: 296.606, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6640, Reward: -501.000, End reward: -501.000, Loss: 297.037, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6645, Reward: -501.000, End reward: -501.000, Loss: 298.793, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6650, Reward: -501.000, End reward: -501.000, Loss: 296.608, Grad mean: 0.000005, Grad max: 0.135, Actions: 501, Entropy: 0.000\n",
      "Episode 6655, Reward: -501.000, End reward: -501.000, Loss: 299.951, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6660, Reward: -501.000, End reward: -501.000, Loss: 301.516, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6665, Reward: -501.000, End reward: -501.000, Loss: 296.652, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6670, Reward: -501.000, End reward: -501.000, Loss: 296.181, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6675, Reward: -501.000, End reward: -501.000, Loss: 296.219, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6680, Reward: -501.000, End reward: -501.000, Loss: 296.666, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6685, Reward: -501.000, End reward: -501.000, Loss: 304.044, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6690, Reward: -501.000, End reward: -501.000, Loss: 296.739, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6695, Reward: -501.000, End reward: -501.000, Loss: 296.243, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6700, Reward: -501.000, End reward: -501.000, Loss: 295.458, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6705, Reward: -501.000, End reward: -501.000, Loss: 297.637, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6710, Reward: -501.000, End reward: -501.000, Loss: 296.952, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6715, Reward: -501.000, End reward: -501.000, Loss: 299.151, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6720, Reward: -501.000, End reward: -501.000, Loss: 296.708, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6725, Reward: -501.000, End reward: -501.000, Loss: 296.295, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6730, Reward: -501.000, End reward: -501.000, Loss: 297.149, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6735, Reward: -501.000, End reward: -501.000, Loss: 296.403, Grad mean: 0.000006, Grad max: 0.129, Actions: 501, Entropy: 0.000\n",
      "Episode 6740, Reward: -501.000, End reward: -501.000, Loss: 298.890, Grad mean: 0.000004, Grad max: 0.198, Actions: 501, Entropy: 0.000\n",
      "Episode 6745, Reward: -501.000, End reward: -501.000, Loss: 296.811, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6750, Reward: -501.000, End reward: -501.000, Loss: 296.684, Grad mean: 0.000006, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 6755, Reward: -501.000, End reward: -501.000, Loss: 300.663, Grad mean: 0.000005, Grad max: 0.136, Actions: 501, Entropy: 0.000\n",
      "Episode 6760, Reward: -501.000, End reward: -501.000, Loss: 296.694, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6765, Reward: -501.000, End reward: -501.000, Loss: 296.697, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6770, Reward: -501.000, End reward: -501.000, Loss: 298.363, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6775, Reward: -501.000, End reward: -501.000, Loss: 297.007, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6780, Reward: -501.000, End reward: -501.000, Loss: 300.276, Grad mean: 0.000006, Grad max: 0.248, Actions: 501, Entropy: 0.000\n",
      "Episode 6785, Reward: -501.000, End reward: -501.000, Loss: 306.159, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6790, Reward: -501.000, End reward: -501.000, Loss: 296.936, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6795, Reward: -501.000, End reward: -501.000, Loss: 295.760, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 6800, Reward: -501.000, End reward: -501.000, Loss: 297.074, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 6805, Reward: -501.000, End reward: -501.000, Loss: 298.456, Grad mean: 0.000004, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 6810, Reward: -501.000, End reward: -501.000, Loss: 300.906, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 6815, Reward: -501.000, End reward: -501.000, Loss: 311.617, Grad mean: 0.000004, Grad max: 0.140, Actions: 501, Entropy: 0.000\n",
      "Episode 6820, Reward: -501.000, End reward: -501.000, Loss: 300.239, Grad mean: 0.000004, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 6825, Reward: -501.000, End reward: -501.000, Loss: 296.542, Grad mean: 0.000004, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 6830, Reward: -501.000, End reward: -501.000, Loss: 299.605, Grad mean: 0.000004, Grad max: 0.140, Actions: 501, Entropy: 0.000\n",
      "Episode 6835, Reward: -501.000, End reward: -501.000, Loss: 298.243, Grad mean: 0.000004, Grad max: 0.140, Actions: 501, Entropy: 0.000\n",
      "Episode 6840, Reward: -501.000, End reward: -501.000, Loss: 299.136, Grad mean: 0.000004, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 6845, Reward: -501.000, End reward: -501.000, Loss: 298.826, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 6850, Reward: -501.000, End reward: -501.000, Loss: 297.206, Grad mean: 0.000004, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 6855, Reward: -501.000, End reward: -501.000, Loss: 296.157, Grad mean: 0.000004, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 6860, Reward: -501.000, End reward: -501.000, Loss: 297.316, Grad mean: 0.000004, Grad max: 0.140, Actions: 501, Entropy: 0.000\n",
      "Episode 6865, Reward: -501.000, End reward: -501.000, Loss: 297.083, Grad mean: 0.000005, Grad max: 0.130, Actions: 501, Entropy: 0.000\n",
      "Episode 6870, Reward: -501.000, End reward: -501.000, Loss: 296.589, Grad mean: 0.000007, Grad max: 0.276, Actions: 501, Entropy: 0.000\n",
      "Episode 6875, Reward: -501.000, End reward: -501.000, Loss: 296.822, Grad mean: 0.000005, Grad max: 0.135, Actions: 501, Entropy: 0.000\n",
      "Episode 6880, Reward: -501.000, End reward: -501.000, Loss: 296.408, Grad mean: 0.000004, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 6885, Reward: -501.000, End reward: -501.000, Loss: 296.168, Grad mean: 0.000006, Grad max: 0.211, Actions: 501, Entropy: 0.000\n",
      "Episode 6890, Reward: -501.000, End reward: -501.000, Loss: 296.742, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 6895, Reward: -501.000, End reward: -501.000, Loss: 296.959, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 6900, Reward: -501.000, End reward: -501.000, Loss: 297.833, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 6905, Reward: -501.000, End reward: -501.000, Loss: 300.689, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 6910, Reward: -501.000, End reward: -501.000, Loss: 302.581, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 6915, Reward: -501.000, End reward: -501.000, Loss: 297.608, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 6920, Reward: -501.000, End reward: -501.000, Loss: 297.669, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 6925, Reward: -501.000, End reward: -501.000, Loss: 297.068, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 6930, Reward: -501.000, End reward: -501.000, Loss: 296.840, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 6935, Reward: -501.000, End reward: -501.000, Loss: 295.024, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 6940, Reward: -501.000, End reward: -501.000, Loss: 297.710, Grad mean: 0.000004, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 6945, Reward: -501.000, End reward: -501.000, Loss: 298.900, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 6950, Reward: -501.000, End reward: -501.000, Loss: 296.827, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 6955, Reward: -501.000, End reward: -501.000, Loss: 294.675, Grad mean: 0.000005, Grad max: 0.136, Actions: 501, Entropy: 0.000\n",
      "Episode 6960, Reward: -501.000, End reward: -501.000, Loss: 297.547, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 6965, Reward: -501.000, End reward: -501.000, Loss: 299.084, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 6970, Reward: -501.000, End reward: -501.000, Loss: 298.507, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 6975, Reward: -501.000, End reward: -501.000, Loss: 307.448, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 6980, Reward: -501.000, End reward: -501.000, Loss: 300.722, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 6985, Reward: -501.000, End reward: -501.000, Loss: 296.519, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 6990, Reward: -501.000, End reward: -501.000, Loss: 297.530, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 6995, Reward: -501.000, End reward: -501.000, Loss: 299.378, Grad mean: 0.000005, Grad max: 0.136, Actions: 501, Entropy: 0.000\n",
      "Episode 7000, Reward: -501.000, End reward: -501.000, Loss: 299.026, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7005, Reward: -501.000, End reward: -501.000, Loss: 295.559, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7010, Reward: -501.000, End reward: -501.000, Loss: 296.390, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7015, Reward: -501.000, End reward: -501.000, Loss: 296.056, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7020, Reward: -501.000, End reward: -501.000, Loss: 306.280, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7025, Reward: -501.000, End reward: -501.000, Loss: 302.096, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7030, Reward: -501.000, End reward: -501.000, Loss: 296.405, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7035, Reward: -501.000, End reward: -501.000, Loss: 296.595, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7040, Reward: -501.000, End reward: -501.000, Loss: 299.841, Grad mean: 0.000004, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 7045, Reward: -501.000, End reward: -501.000, Loss: 294.910, Grad mean: 0.000006, Grad max: 0.130, Actions: 501, Entropy: 0.000\n",
      "Episode 7050, Reward: -501.000, End reward: -501.000, Loss: 299.509, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7055, Reward: -501.000, End reward: -501.000, Loss: 298.062, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7060, Reward: -501.000, End reward: -501.000, Loss: 299.469, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 7065, Reward: -501.000, End reward: -501.000, Loss: 297.338, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7070, Reward: -501.000, End reward: -501.000, Loss: 296.444, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 7075, Reward: -501.000, End reward: -501.000, Loss: 299.255, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7080, Reward: -501.000, End reward: -501.000, Loss: 300.798, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 7085, Reward: -501.000, End reward: -501.000, Loss: 295.454, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7090, Reward: -501.000, End reward: -501.000, Loss: 301.341, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7095, Reward: -501.000, End reward: -501.000, Loss: 296.992, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7100, Reward: -501.000, End reward: -501.000, Loss: 302.543, Grad mean: 0.000005, Grad max: 0.140, Actions: 501, Entropy: 0.000\n",
      "Episode 7105, Reward: -501.000, End reward: -501.000, Loss: 295.377, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7110, Reward: -501.000, End reward: -501.000, Loss: 295.526, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7115, Reward: -501.000, End reward: -501.000, Loss: 296.503, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7120, Reward: -501.000, End reward: -501.000, Loss: 295.577, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7125, Reward: -501.000, End reward: -501.000, Loss: 296.819, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7130, Reward: -501.000, End reward: -501.000, Loss: 303.019, Grad mean: 0.000005, Grad max: 0.140, Actions: 501, Entropy: 0.000\n",
      "Episode 7135, Reward: -501.000, End reward: -501.000, Loss: 295.027, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7140, Reward: -501.000, End reward: -501.000, Loss: 301.463, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7145, Reward: -501.000, End reward: -501.000, Loss: 299.118, Grad mean: 0.000005, Grad max: 0.140, Actions: 501, Entropy: 0.000\n",
      "Episode 7150, Reward: -501.000, End reward: -501.000, Loss: 310.546, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7155, Reward: -501.000, End reward: -501.000, Loss: 298.850, Grad mean: 0.000005, Grad max: 0.140, Actions: 501, Entropy: 0.000\n",
      "Episode 7160, Reward: -501.000, End reward: -501.000, Loss: 296.687, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7165, Reward: -501.000, End reward: -501.000, Loss: 294.910, Grad mean: 0.000005, Grad max: 0.159, Actions: 501, Entropy: 0.000\n",
      "Episode 7170, Reward: -501.000, End reward: -501.000, Loss: 296.447, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 7175, Reward: -501.000, End reward: -501.000, Loss: 298.669, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7180, Reward: -501.000, End reward: -501.000, Loss: 301.864, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7185, Reward: -501.000, End reward: -501.000, Loss: 298.397, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7190, Reward: -501.000, End reward: -501.000, Loss: 297.363, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7195, Reward: -501.000, End reward: -501.000, Loss: 296.651, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7200, Reward: -501.000, End reward: -501.000, Loss: 297.491, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7205, Reward: -501.000, End reward: -501.000, Loss: 304.188, Grad mean: 0.000005, Grad max: 0.140, Actions: 501, Entropy: 0.000\n",
      "Episode 7210, Reward: -501.000, End reward: -501.000, Loss: 305.046, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7215, Reward: -501.000, End reward: -501.000, Loss: 299.666, Grad mean: 0.000005, Grad max: 0.140, Actions: 501, Entropy: 0.000\n",
      "Episode 7220, Reward: -501.000, End reward: -501.000, Loss: 300.233, Grad mean: 0.000005, Grad max: 0.140, Actions: 501, Entropy: 0.000\n",
      "Episode 7225, Reward: -501.000, End reward: -501.000, Loss: 295.701, Grad mean: 0.000004, Grad max: 0.231, Actions: 501, Entropy: 0.000\n",
      "Episode 7230, Reward: -501.000, End reward: -501.000, Loss: 298.457, Grad mean: 0.000005, Grad max: 0.140, Actions: 501, Entropy: 0.000\n",
      "Episode 7235, Reward: -501.000, End reward: -501.000, Loss: 297.820, Grad mean: 0.000004, Grad max: 0.141, Actions: 501, Entropy: 0.000\n",
      "Episode 7240, Reward: -501.000, End reward: -501.000, Loss: 296.063, Grad mean: 0.000005, Grad max: 0.140, Actions: 501, Entropy: 0.000\n",
      "Episode 7245, Reward: -501.000, End reward: -501.000, Loss: 296.724, Grad mean: 0.000005, Grad max: 0.140, Actions: 501, Entropy: 0.000\n",
      "Episode 7250, Reward: -501.000, End reward: -501.000, Loss: 295.263, Grad mean: 0.000005, Grad max: 0.140, Actions: 501, Entropy: 0.000\n",
      "Episode 7255, Reward: -501.000, End reward: -501.000, Loss: 296.209, Grad mean: 0.000005, Grad max: 0.140, Actions: 501, Entropy: 0.000\n",
      "Episode 7260, Reward: -501.000, End reward: -501.000, Loss: 295.367, Grad mean: 0.000005, Grad max: 0.140, Actions: 501, Entropy: 0.000\n",
      "Episode 7265, Reward: -501.000, End reward: -501.000, Loss: 296.542, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7270, Reward: -501.000, End reward: -501.000, Loss: 296.030, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 7275, Reward: -501.000, End reward: -501.000, Loss: 298.483, Grad mean: 0.000005, Grad max: 0.140, Actions: 501, Entropy: 0.000\n",
      "Episode 7280, Reward: -501.000, End reward: -501.000, Loss: 305.219, Grad mean: 0.000005, Grad max: 0.140, Actions: 501, Entropy: 0.000\n",
      "Episode 7285, Reward: -501.000, End reward: -501.000, Loss: 305.177, Grad mean: 0.000005, Grad max: 0.140, Actions: 501, Entropy: 0.000\n",
      "Episode 7290, Reward: -501.000, End reward: -501.000, Loss: 300.218, Grad mean: 0.000005, Grad max: 0.141, Actions: 501, Entropy: 0.000\n",
      "Episode 7295, Reward: -501.000, End reward: -501.000, Loss: 295.948, Grad mean: 0.000004, Grad max: 0.141, Actions: 501, Entropy: 0.000\n",
      "Episode 7300, Reward: -501.000, End reward: -501.000, Loss: 301.511, Grad mean: 0.000004, Grad max: 0.141, Actions: 501, Entropy: 0.000\n",
      "Episode 7305, Reward: -501.000, End reward: -501.000, Loss: 297.263, Grad mean: 0.000005, Grad max: 0.141, Actions: 501, Entropy: 0.000\n",
      "Episode 7310, Reward: -501.000, End reward: -501.000, Loss: 297.260, Grad mean: 0.000005, Grad max: 0.141, Actions: 501, Entropy: 0.000\n",
      "Episode 7315, Reward: -501.000, End reward: -501.000, Loss: 304.426, Grad mean: 0.000004, Grad max: 0.141, Actions: 501, Entropy: 0.000\n",
      "Episode 7320, Reward: -501.000, End reward: -501.000, Loss: 300.469, Grad mean: 0.000005, Grad max: 0.141, Actions: 501, Entropy: 0.000\n",
      "Episode 7325, Reward: -501.000, End reward: -501.000, Loss: 303.984, Grad mean: 0.000004, Grad max: 0.142, Actions: 501, Entropy: 0.000\n",
      "Episode 7330, Reward: -501.000, End reward: -501.000, Loss: 298.786, Grad mean: 0.000004, Grad max: 0.142, Actions: 501, Entropy: 0.000\n",
      "Episode 7335, Reward: -501.000, End reward: -501.000, Loss: 300.103, Grad mean: 0.000004, Grad max: 0.142, Actions: 501, Entropy: 0.000\n",
      "Episode 7340, Reward: -501.000, End reward: -501.000, Loss: 299.826, Grad mean: 0.000004, Grad max: 0.142, Actions: 501, Entropy: 0.000\n",
      "Episode 7345, Reward: -501.000, End reward: -501.000, Loss: 298.138, Grad mean: 0.000004, Grad max: 0.142, Actions: 501, Entropy: 0.000\n",
      "Episode 7350, Reward: -501.000, End reward: -501.000, Loss: 301.169, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7355, Reward: -501.000, End reward: -501.000, Loss: 297.091, Grad mean: 0.000004, Grad max: 0.142, Actions: 501, Entropy: 0.000\n",
      "Episode 7360, Reward: -501.000, End reward: -501.000, Loss: 298.179, Grad mean: 0.000004, Grad max: 0.142, Actions: 501, Entropy: 0.000\n",
      "Episode 7365, Reward: -501.000, End reward: -501.000, Loss: 298.206, Grad mean: 0.000005, Grad max: 0.141, Actions: 501, Entropy: 0.000\n",
      "Episode 7370, Reward: -501.000, End reward: -501.000, Loss: 298.806, Grad mean: 0.000004, Grad max: 0.141, Actions: 501, Entropy: 0.000\n",
      "Episode 7375, Reward: -501.000, End reward: -501.000, Loss: 297.105, Grad mean: 0.000005, Grad max: 0.140, Actions: 501, Entropy: 0.000\n",
      "Episode 7380, Reward: -501.000, End reward: -501.000, Loss: 297.526, Grad mean: 0.000004, Grad max: 0.141, Actions: 501, Entropy: 0.000\n",
      "Episode 7385, Reward: -501.000, End reward: -501.000, Loss: 297.645, Grad mean: 0.000004, Grad max: 0.142, Actions: 501, Entropy: 0.000\n",
      "Episode 7390, Reward: -501.000, End reward: -501.000, Loss: 297.261, Grad mean: 0.000004, Grad max: 0.142, Actions: 501, Entropy: 0.000\n",
      "Episode 7395, Reward: -501.000, End reward: -501.000, Loss: 296.235, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7400, Reward: -501.000, End reward: -501.000, Loss: 297.263, Grad mean: 0.000004, Grad max: 0.141, Actions: 501, Entropy: 0.000\n",
      "Episode 7405, Reward: -501.000, End reward: -501.000, Loss: 298.130, Grad mean: 0.000005, Grad max: 0.142, Actions: 501, Entropy: 0.000\n",
      "Episode 7410, Reward: -501.000, End reward: -501.000, Loss: 297.045, Grad mean: 0.000004, Grad max: 0.140, Actions: 501, Entropy: 0.000\n",
      "Episode 7415, Reward: -501.000, End reward: -501.000, Loss: 297.067, Grad mean: 0.000005, Grad max: 0.142, Actions: 501, Entropy: 0.000\n",
      "Episode 7420, Reward: -501.000, End reward: -501.000, Loss: 297.263, Grad mean: 0.000004, Grad max: 0.142, Actions: 501, Entropy: 0.000\n",
      "Episode 7425, Reward: -501.000, End reward: -501.000, Loss: 294.998, Grad mean: 0.000004, Grad max: 0.142, Actions: 501, Entropy: 0.000\n",
      "Episode 7430, Reward: -501.000, End reward: -501.000, Loss: 296.626, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7435, Reward: -501.000, End reward: -501.000, Loss: 299.068, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7440, Reward: -501.000, End reward: -501.000, Loss: 298.272, Grad mean: 0.000004, Grad max: 0.142, Actions: 501, Entropy: 0.000\n",
      "Episode 7445, Reward: -501.000, End reward: -501.000, Loss: 295.184, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7450, Reward: -501.000, End reward: -501.000, Loss: 296.657, Grad mean: 0.000005, Grad max: 0.141, Actions: 501, Entropy: 0.000\n",
      "Episode 7455, Reward: -501.000, End reward: -501.000, Loss: 296.557, Grad mean: 0.000005, Grad max: 0.142, Actions: 501, Entropy: 0.000\n",
      "Episode 7460, Reward: -501.000, End reward: -501.000, Loss: 297.002, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7465, Reward: -501.000, End reward: -501.000, Loss: 295.809, Grad mean: 0.000005, Grad max: 0.142, Actions: 501, Entropy: 0.000\n",
      "Episode 7470, Reward: -501.000, End reward: -501.000, Loss: 297.347, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7475, Reward: -501.000, End reward: -501.000, Loss: 296.418, Grad mean: 0.000004, Grad max: 0.142, Actions: 501, Entropy: 0.000\n",
      "Episode 7480, Reward: -501.000, End reward: -501.000, Loss: 294.754, Grad mean: 0.000005, Grad max: 0.190, Actions: 501, Entropy: 0.000\n",
      "Episode 7485, Reward: -501.000, End reward: -501.000, Loss: 296.180, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7490, Reward: -501.000, End reward: -501.000, Loss: 299.084, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7495, Reward: -501.000, End reward: -501.000, Loss: 301.523, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 7500, Reward: -501.000, End reward: -501.000, Loss: 296.864, Grad mean: 0.000004, Grad max: 0.142, Actions: 501, Entropy: 0.000\n",
      "Episode 7505, Reward: -501.000, End reward: -501.000, Loss: 296.436, Grad mean: 0.000004, Grad max: 0.292, Actions: 501, Entropy: 0.000\n",
      "Episode 7510, Reward: -501.000, End reward: -501.000, Loss: 295.594, Grad mean: 0.000004, Grad max: 0.141, Actions: 501, Entropy: 0.000\n",
      "Episode 7515, Reward: -501.000, End reward: -501.000, Loss: 297.296, Grad mean: 0.000005, Grad max: 0.139, Actions: 501, Entropy: 0.000\n",
      "Episode 7520, Reward: -501.000, End reward: -501.000, Loss: 295.945, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 7525, Reward: -501.000, End reward: -501.000, Loss: 296.944, Grad mean: 0.000005, Grad max: 0.141, Actions: 501, Entropy: 0.000\n",
      "Episode 7530, Reward: -501.000, End reward: -501.000, Loss: 295.402, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7535, Reward: -501.000, End reward: -501.000, Loss: 297.995, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7540, Reward: -501.000, End reward: -501.000, Loss: 298.947, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7545, Reward: -501.000, End reward: -501.000, Loss: 297.671, Grad mean: 0.000004, Grad max: 0.142, Actions: 501, Entropy: 0.000\n",
      "Episode 7550, Reward: -501.000, End reward: -501.000, Loss: 296.555, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7555, Reward: -501.000, End reward: -501.000, Loss: 296.302, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 7560, Reward: -501.000, End reward: -501.000, Loss: 296.473, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7565, Reward: -501.000, End reward: -501.000, Loss: 297.393, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7570, Reward: -501.000, End reward: -501.000, Loss: 297.904, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7575, Reward: -501.000, End reward: -501.000, Loss: 302.715, Grad mean: 0.000004, Grad max: 0.142, Actions: 501, Entropy: 0.000\n",
      "Episode 7580, Reward: -501.000, End reward: -501.000, Loss: 306.388, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7585, Reward: -501.000, End reward: -501.000, Loss: 301.020, Grad mean: 0.000004, Grad max: 0.142, Actions: 501, Entropy: 0.000\n",
      "Episode 7590, Reward: -501.000, End reward: -501.000, Loss: 299.916, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7595, Reward: -501.000, End reward: -501.000, Loss: 303.853, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7600, Reward: -501.000, End reward: -501.000, Loss: 298.059, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7605, Reward: -501.000, End reward: -501.000, Loss: 301.081, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7610, Reward: -501.000, End reward: -501.000, Loss: 298.090, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7615, Reward: -501.000, End reward: -501.000, Loss: 302.375, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7620, Reward: -501.000, End reward: -501.000, Loss: 304.018, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7625, Reward: -501.000, End reward: -501.000, Loss: 296.551, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7630, Reward: -501.000, End reward: -501.000, Loss: 296.792, Grad mean: 0.000004, Grad max: 0.142, Actions: 501, Entropy: 0.000\n",
      "Episode 7635, Reward: -501.000, End reward: -501.000, Loss: 297.636, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7640, Reward: -501.000, End reward: -501.000, Loss: 297.261, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7645, Reward: -501.000, End reward: -501.000, Loss: 297.410, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7650, Reward: -501.000, End reward: -501.000, Loss: 294.694, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7655, Reward: -501.000, End reward: -501.000, Loss: 298.008, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7660, Reward: -501.000, End reward: -501.000, Loss: 296.673, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7665, Reward: -501.000, End reward: -501.000, Loss: 296.552, Grad mean: 0.000005, Grad max: 0.142, Actions: 501, Entropy: 0.000\n",
      "Episode 7670, Reward: -501.000, End reward: -501.000, Loss: 298.878, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7675, Reward: -501.000, End reward: -501.000, Loss: 298.554, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 7680, Reward: -501.000, End reward: -501.000, Loss: 295.687, Grad mean: 0.000004, Grad max: 0.278, Actions: 501, Entropy: 0.000\n",
      "Episode 7685, Reward: -501.000, End reward: -501.000, Loss: 296.443, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7690, Reward: -501.000, End reward: -501.000, Loss: 296.475, Grad mean: 0.000005, Grad max: 0.136, Actions: 501, Entropy: 0.000\n",
      "Episode 7695, Reward: -501.000, End reward: -501.000, Loss: 299.220, Grad mean: 0.000004, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 7700, Reward: -501.000, End reward: -501.000, Loss: 297.439, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 7705, Reward: -501.000, End reward: -501.000, Loss: 297.057, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 7710, Reward: -501.000, End reward: -501.000, Loss: 294.709, Grad mean: 0.000004, Grad max: 0.168, Actions: 501, Entropy: 0.000\n",
      "Episode 7715, Reward: -501.000, End reward: -501.000, Loss: 297.996, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 7720, Reward: -501.000, End reward: -501.000, Loss: 296.811, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 7725, Reward: -501.000, End reward: -501.000, Loss: 299.409, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 7730, Reward: -501.000, End reward: -501.000, Loss: 298.463, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7735, Reward: -501.000, End reward: -501.000, Loss: 296.971, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7740, Reward: -501.000, End reward: -501.000, Loss: 296.718, Grad mean: 0.000004, Grad max: 0.141, Actions: 501, Entropy: 0.000\n",
      "Episode 7745, Reward: -501.000, End reward: -501.000, Loss: 299.673, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7750, Reward: -501.000, End reward: -501.000, Loss: 295.417, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 7755, Reward: -501.000, End reward: -501.000, Loss: 300.825, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 7760, Reward: -501.000, End reward: -501.000, Loss: 300.380, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 7765, Reward: -501.000, End reward: -501.000, Loss: 306.897, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 7770, Reward: -501.000, End reward: -501.000, Loss: 300.583, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 7775, Reward: -501.000, End reward: -501.000, Loss: 299.053, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 7780, Reward: -501.000, End reward: -501.000, Loss: 302.923, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 7785, Reward: -501.000, End reward: -501.000, Loss: 300.185, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 7790, Reward: -501.000, End reward: -501.000, Loss: 302.223, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 7795, Reward: -501.000, End reward: -501.000, Loss: 298.497, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 7800, Reward: -501.000, End reward: -501.000, Loss: 302.772, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 7805, Reward: -501.000, End reward: -501.000, Loss: 301.029, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 7810, Reward: -501.000, End reward: -501.000, Loss: 300.591, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 7815, Reward: -501.000, End reward: -501.000, Loss: 302.451, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 7820, Reward: -501.000, End reward: -501.000, Loss: 299.970, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 7825, Reward: -501.000, End reward: -501.000, Loss: 296.614, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 7830, Reward: -501.000, End reward: -501.000, Loss: 296.814, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 7835, Reward: -501.000, End reward: -501.000, Loss: 295.280, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 7840, Reward: -501.000, End reward: -501.000, Loss: 296.265, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 7845, Reward: -501.000, End reward: -501.000, Loss: 295.342, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 7850, Reward: -501.000, End reward: -501.000, Loss: 298.104, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 7855, Reward: -501.000, End reward: -501.000, Loss: 300.186, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 7860, Reward: -501.000, End reward: -501.000, Loss: 298.441, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 7865, Reward: -501.000, End reward: -501.000, Loss: 295.980, Grad mean: 0.000005, Grad max: 0.133, Actions: 501, Entropy: 0.000\n",
      "Episode 7870, Reward: -501.000, End reward: -501.000, Loss: 298.606, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 7875, Reward: -501.000, End reward: -501.000, Loss: 301.311, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 7880, Reward: -501.000, End reward: -501.000, Loss: 296.168, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 7885, Reward: -501.000, End reward: -501.000, Loss: 296.189, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 7890, Reward: -501.000, End reward: -501.000, Loss: 299.791, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 7895, Reward: -501.000, End reward: -501.000, Loss: 296.555, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 7900, Reward: -501.000, End reward: -501.000, Loss: 298.037, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 7905, Reward: -501.000, End reward: -501.000, Loss: 301.333, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 7910, Reward: -501.000, End reward: -501.000, Loss: 296.959, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 7915, Reward: -501.000, End reward: -501.000, Loss: 300.794, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 7920, Reward: -501.000, End reward: -501.000, Loss: 297.176, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 7925, Reward: -501.000, End reward: -501.000, Loss: 297.164, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 7930, Reward: -501.000, End reward: -501.000, Loss: 294.607, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 7935, Reward: -501.000, End reward: -501.000, Loss: 298.400, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 7940, Reward: -501.000, End reward: -501.000, Loss: 297.815, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 7945, Reward: -501.000, End reward: -501.000, Loss: 295.583, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 7950, Reward: -501.000, End reward: -501.000, Loss: 297.436, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 7955, Reward: -501.000, End reward: -501.000, Loss: 296.746, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 7960, Reward: -501.000, End reward: -501.000, Loss: 300.441, Grad mean: 0.000005, Grad max: 0.135, Actions: 501, Entropy: 0.000\n",
      "Episode 7965, Reward: -501.000, End reward: -501.000, Loss: 296.597, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 7970, Reward: -501.000, End reward: -501.000, Loss: 295.577, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 7975, Reward: -501.000, End reward: -501.000, Loss: 297.013, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 7980, Reward: -501.000, End reward: -501.000, Loss: 300.481, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 7985, Reward: -501.000, End reward: -501.000, Loss: 296.281, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 7990, Reward: -501.000, End reward: -501.000, Loss: 296.056, Grad mean: 0.000003, Grad max: 0.331, Actions: 501, Entropy: 0.000\n",
      "Episode 7995, Reward: -501.000, End reward: -501.000, Loss: 296.440, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 8000, Reward: -501.000, End reward: -501.000, Loss: 301.452, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8005, Reward: -501.000, End reward: -501.000, Loss: 296.281, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 8010, Reward: -501.000, End reward: -501.000, Loss: 295.794, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 8015, Reward: -501.000, End reward: -501.000, Loss: 296.947, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 8020, Reward: -501.000, End reward: -501.000, Loss: 297.174, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 8025, Reward: -501.000, End reward: -501.000, Loss: 296.494, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 8030, Reward: -501.000, End reward: -501.000, Loss: 296.330, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 8035, Reward: -501.000, End reward: -501.000, Loss: 298.564, Grad mean: 0.000005, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 8040, Reward: -501.000, End reward: -501.000, Loss: 304.351, Grad mean: 0.000005, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 8045, Reward: -501.000, End reward: -501.000, Loss: 295.800, Grad mean: 0.000005, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 8050, Reward: -501.000, End reward: -501.000, Loss: 297.470, Grad mean: 0.000005, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 8055, Reward: -501.000, End reward: -501.000, Loss: 296.007, Grad mean: 0.000005, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 8060, Reward: -501.000, End reward: -501.000, Loss: 297.095, Grad mean: 0.000005, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 8065, Reward: -501.000, End reward: -501.000, Loss: 296.408, Grad mean: 0.000005, Grad max: 0.141, Actions: 501, Entropy: 0.000\n",
      "Episode 8070, Reward: -501.000, End reward: -501.000, Loss: 296.577, Grad mean: 0.000005, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 8075, Reward: -501.000, End reward: -501.000, Loss: 297.661, Grad mean: 0.000005, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 8080, Reward: -501.000, End reward: -501.000, Loss: 296.014, Grad mean: 0.000005, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 8085, Reward: -501.000, End reward: -501.000, Loss: 297.680, Grad mean: 0.000005, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 8090, Reward: -501.000, End reward: -501.000, Loss: 296.600, Grad mean: 0.000005, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 8095, Reward: -501.000, End reward: -501.000, Loss: 295.746, Grad mean: 0.000005, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 8100, Reward: -501.000, End reward: -501.000, Loss: 304.487, Grad mean: 0.000005, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 8105, Reward: -501.000, End reward: -501.000, Loss: 299.525, Grad mean: 0.000005, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 8110, Reward: -501.000, End reward: -501.000, Loss: 296.306, Grad mean: 0.000005, Grad max: 0.142, Actions: 501, Entropy: 0.000\n",
      "Episode 8115, Reward: -501.000, End reward: -501.000, Loss: 297.452, Grad mean: 0.000005, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 8120, Reward: -501.000, End reward: -501.000, Loss: 298.072, Grad mean: 0.000005, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 8125, Reward: -501.000, End reward: -501.000, Loss: 299.223, Grad mean: 0.000005, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 8130, Reward: -501.000, End reward: -501.000, Loss: 297.959, Grad mean: 0.000005, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 8135, Reward: -501.000, End reward: -501.000, Loss: 296.299, Grad mean: 0.000005, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 8140, Reward: -501.000, End reward: -501.000, Loss: 297.931, Grad mean: 0.000005, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 8145, Reward: -501.000, End reward: -501.000, Loss: 304.095, Grad mean: 0.000005, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 8150, Reward: -501.000, End reward: -501.000, Loss: 297.108, Grad mean: 0.000005, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 8155, Reward: -501.000, End reward: -501.000, Loss: 299.112, Grad mean: 0.000005, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 8160, Reward: -501.000, End reward: -501.000, Loss: 296.821, Grad mean: 0.000005, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 8165, Reward: -501.000, End reward: -501.000, Loss: 296.136, Grad mean: 0.000005, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 8170, Reward: -501.000, End reward: -501.000, Loss: 295.739, Grad mean: 0.000005, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 8175, Reward: -501.000, End reward: -501.000, Loss: 296.436, Grad mean: 0.000005, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 8180, Reward: -501.000, End reward: -501.000, Loss: 295.936, Grad mean: 0.000005, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 8185, Reward: -501.000, End reward: -501.000, Loss: 297.422, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 8190, Reward: -501.000, End reward: -501.000, Loss: 296.456, Grad mean: 0.000005, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 8195, Reward: -501.000, End reward: -501.000, Loss: 296.918, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 8200, Reward: -501.000, End reward: -501.000, Loss: 295.952, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 8205, Reward: -501.000, End reward: -501.000, Loss: 297.122, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 8210, Reward: -501.000, End reward: -501.000, Loss: 296.571, Grad mean: 0.000005, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 8215, Reward: -501.000, End reward: -501.000, Loss: 295.975, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8220, Reward: -501.000, End reward: -501.000, Loss: 296.986, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8225, Reward: -501.000, End reward: -501.000, Loss: 298.029, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8230, Reward: -501.000, End reward: -501.000, Loss: 297.171, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 8235, Reward: -501.000, End reward: -501.000, Loss: 297.250, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 8240, Reward: -501.000, End reward: -501.000, Loss: 296.917, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8245, Reward: -501.000, End reward: -501.000, Loss: 295.070, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 8250, Reward: -501.000, End reward: -501.000, Loss: 296.880, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8255, Reward: -501.000, End reward: -501.000, Loss: 297.599, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8260, Reward: -501.000, End reward: -501.000, Loss: 297.895, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8265, Reward: -501.000, End reward: -501.000, Loss: 299.459, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8270, Reward: -501.000, End reward: -501.000, Loss: 297.450, Grad mean: 0.000005, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8275, Reward: -501.000, End reward: -501.000, Loss: 296.695, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8280, Reward: -501.000, End reward: -501.000, Loss: 297.124, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8285, Reward: -501.000, End reward: -501.000, Loss: 296.448, Grad mean: 0.000004, Grad max: 0.215, Actions: 501, Entropy: 0.000\n",
      "Episode 8290, Reward: -501.000, End reward: -501.000, Loss: 296.725, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8295, Reward: -501.000, End reward: -501.000, Loss: 296.187, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8300, Reward: -501.000, End reward: -501.000, Loss: 296.550, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8305, Reward: -501.000, End reward: -501.000, Loss: 296.807, Grad mean: 0.000004, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 8310, Reward: -501.000, End reward: -501.000, Loss: 295.915, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8315, Reward: -501.000, End reward: -501.000, Loss: 296.425, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8320, Reward: -501.000, End reward: -501.000, Loss: 301.848, Grad mean: 0.000005, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8325, Reward: -501.000, End reward: -501.000, Loss: 295.512, Grad mean: 0.000005, Grad max: 0.142, Actions: 501, Entropy: 0.000\n",
      "Episode 8330, Reward: -501.000, End reward: -501.000, Loss: 296.583, Grad mean: 0.000005, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8335, Reward: -501.000, End reward: -501.000, Loss: 296.072, Grad mean: 0.000004, Grad max: 0.141, Actions: 501, Entropy: 0.000\n",
      "Episode 8340, Reward: -501.000, End reward: -501.000, Loss: 295.476, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8345, Reward: -501.000, End reward: -501.000, Loss: 296.395, Grad mean: 0.000005, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8350, Reward: -501.000, End reward: -501.000, Loss: 301.031, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8355, Reward: -501.000, End reward: -501.000, Loss: 306.198, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8360, Reward: -501.000, End reward: -501.000, Loss: 296.769, Grad mean: 0.000005, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8365, Reward: -501.000, End reward: -501.000, Loss: 296.176, Grad mean: 0.000005, Grad max: 0.514, Actions: 501, Entropy: 0.000\n",
      "Episode 8370, Reward: -501.000, End reward: -501.000, Loss: 297.169, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8375, Reward: -501.000, End reward: -501.000, Loss: 301.092, Grad mean: 0.000005, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 8380, Reward: -501.000, End reward: -501.000, Loss: 298.460, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8385, Reward: -501.000, End reward: -501.000, Loss: 295.424, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8390, Reward: -501.000, End reward: -501.000, Loss: 297.069, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8395, Reward: -501.000, End reward: -501.000, Loss: 295.709, Grad mean: 0.000005, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8400, Reward: -501.000, End reward: -501.000, Loss: 295.312, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8405, Reward: -501.000, End reward: -501.000, Loss: 295.611, Grad mean: 0.000005, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 8410, Reward: -501.000, End reward: -501.000, Loss: 298.974, Grad mean: 0.000005, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8415, Reward: -501.000, End reward: -501.000, Loss: 300.849, Grad mean: 0.000005, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8420, Reward: -501.000, End reward: -501.000, Loss: 298.310, Grad mean: 0.000004, Grad max: 0.150, Actions: 501, Entropy: 0.000\n",
      "Episode 8425, Reward: -501.000, End reward: -501.000, Loss: 296.502, Grad mean: 0.000005, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8430, Reward: -501.000, End reward: -501.000, Loss: 300.403, Grad mean: 0.000005, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8435, Reward: -501.000, End reward: -501.000, Loss: 301.903, Grad mean: 0.000005, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8440, Reward: -501.000, End reward: -501.000, Loss: 295.025, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 8445, Reward: -501.000, End reward: -501.000, Loss: 296.766, Grad mean: 0.000005, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8450, Reward: -501.000, End reward: -501.000, Loss: 297.076, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8455, Reward: -501.000, End reward: -501.000, Loss: 296.497, Grad mean: 0.000005, Grad max: 0.137, Actions: 501, Entropy: 0.000\n",
      "Episode 8460, Reward: -501.000, End reward: -501.000, Loss: 299.944, Grad mean: 0.000005, Grad max: 0.322, Actions: 501, Entropy: 0.000\n",
      "Episode 8465, Reward: -501.000, End reward: -501.000, Loss: 294.661, Grad mean: 0.000005, Grad max: 0.142, Actions: 501, Entropy: 0.000\n",
      "Episode 8470, Reward: -501.000, End reward: -501.000, Loss: 299.657, Grad mean: 0.000005, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 8475, Reward: -501.000, End reward: -501.000, Loss: 301.426, Grad mean: 0.000005, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 8480, Reward: -501.000, End reward: -501.000, Loss: 296.141, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8485, Reward: -501.000, End reward: -501.000, Loss: 296.276, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8490, Reward: -501.000, End reward: -501.000, Loss: 296.109, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8495, Reward: -501.000, End reward: -501.000, Loss: 299.535, Grad mean: 0.000005, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8500, Reward: -501.000, End reward: -501.000, Loss: 296.779, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8505, Reward: -501.000, End reward: -501.000, Loss: 298.338, Grad mean: 0.000005, Grad max: 0.142, Actions: 501, Entropy: 0.000\n",
      "Episode 8510, Reward: -501.000, End reward: -501.000, Loss: 296.175, Grad mean: 0.000005, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8515, Reward: -501.000, End reward: -501.000, Loss: 298.410, Grad mean: 0.000005, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8520, Reward: -501.000, End reward: -501.000, Loss: 298.202, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8525, Reward: -501.000, End reward: -501.000, Loss: 300.426, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8530, Reward: -501.000, End reward: -501.000, Loss: 296.466, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8535, Reward: -501.000, End reward: -501.000, Loss: 298.171, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8540, Reward: -501.000, End reward: -501.000, Loss: 297.757, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8545, Reward: -501.000, End reward: -501.000, Loss: 299.712, Grad mean: 0.000003, Grad max: 0.239, Actions: 501, Entropy: 0.000\n",
      "Episode 8550, Reward: -501.000, End reward: -501.000, Loss: 297.413, Grad mean: 0.000005, Grad max: 0.142, Actions: 501, Entropy: 0.000\n",
      "Episode 8555, Reward: -501.000, End reward: -501.000, Loss: 295.442, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8560, Reward: -501.000, End reward: -501.000, Loss: 295.382, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 8565, Reward: -501.000, End reward: -501.000, Loss: 295.838, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8570, Reward: -501.000, End reward: -501.000, Loss: 300.122, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8575, Reward: -501.000, End reward: -501.000, Loss: 299.861, Grad mean: 0.000005, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8580, Reward: -501.000, End reward: -501.000, Loss: 296.956, Grad mean: 0.000005, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8585, Reward: -501.000, End reward: -501.000, Loss: 296.682, Grad mean: 0.000005, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8590, Reward: -501.000, End reward: -501.000, Loss: 297.294, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8595, Reward: -501.000, End reward: -501.000, Loss: 296.774, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8600, Reward: -501.000, End reward: -501.000, Loss: 296.045, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8605, Reward: -501.000, End reward: -501.000, Loss: 298.907, Grad mean: 0.000005, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8610, Reward: -501.000, End reward: -501.000, Loss: 299.569, Grad mean: 0.000005, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 8615, Reward: -501.000, End reward: -501.000, Loss: 296.208, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8620, Reward: -501.000, End reward: -501.000, Loss: 297.432, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8625, Reward: -501.000, End reward: -501.000, Loss: 296.403, Grad mean: 0.000005, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8630, Reward: -501.000, End reward: -501.000, Loss: 297.759, Grad mean: 0.000005, Grad max: 0.243, Actions: 501, Entropy: 0.000\n",
      "Episode 8635, Reward: -501.000, End reward: -501.000, Loss: 296.273, Grad mean: 0.000005, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8640, Reward: -501.000, End reward: -501.000, Loss: 296.469, Grad mean: 0.000005, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8645, Reward: -501.000, End reward: -501.000, Loss: 299.448, Grad mean: 0.000005, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8650, Reward: -501.000, End reward: -501.000, Loss: 298.574, Grad mean: 0.000005, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8655, Reward: -501.000, End reward: -501.000, Loss: 294.581, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8660, Reward: -501.000, End reward: -501.000, Loss: 296.529, Grad mean: 0.000005, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8665, Reward: -501.000, End reward: -501.000, Loss: 302.918, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8670, Reward: -501.000, End reward: -501.000, Loss: 300.084, Grad mean: 0.000004, Grad max: 0.136, Actions: 501, Entropy: 0.000\n",
      "Episode 8675, Reward: -501.000, End reward: -501.000, Loss: 296.552, Grad mean: 0.000005, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8680, Reward: -501.000, End reward: -501.000, Loss: 298.823, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8685, Reward: -501.000, End reward: -501.000, Loss: 298.004, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8690, Reward: -501.000, End reward: -501.000, Loss: 296.577, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8695, Reward: -501.000, End reward: -501.000, Loss: 298.106, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8700, Reward: -501.000, End reward: -501.000, Loss: 297.658, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8705, Reward: -501.000, End reward: -501.000, Loss: 298.644, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 8710, Reward: -501.000, End reward: -501.000, Loss: 296.286, Grad mean: 0.000005, Grad max: 0.165, Actions: 501, Entropy: 0.000\n",
      "Episode 8715, Reward: -501.000, End reward: -501.000, Loss: 296.451, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8720, Reward: -501.000, End reward: -501.000, Loss: 296.410, Grad mean: 0.000005, Grad max: 0.143, Actions: 501, Entropy: 0.000\n",
      "Episode 8725, Reward: -501.000, End reward: -501.000, Loss: 296.908, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8730, Reward: -501.000, End reward: -501.000, Loss: 296.353, Grad mean: 0.000005, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8735, Reward: -501.000, End reward: -501.000, Loss: 296.128, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8740, Reward: -501.000, End reward: -501.000, Loss: 295.336, Grad mean: 0.000005, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8745, Reward: -501.000, End reward: -501.000, Loss: 303.333, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8750, Reward: -501.000, End reward: -501.000, Loss: 302.213, Grad mean: 0.000005, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8755, Reward: -501.000, End reward: -501.000, Loss: 301.528, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8760, Reward: -501.000, End reward: -501.000, Loss: 299.278, Grad mean: 0.000006, Grad max: 0.169, Actions: 501, Entropy: 0.000\n",
      "Episode 8765, Reward: -501.000, End reward: -501.000, Loss: 294.837, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 8770, Reward: -501.000, End reward: -501.000, Loss: 304.571, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8775, Reward: -501.000, End reward: -501.000, Loss: 301.274, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 8780, Reward: -501.000, End reward: -501.000, Loss: 303.573, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8785, Reward: -501.000, End reward: -501.000, Loss: 296.218, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 8790, Reward: -501.000, End reward: -501.000, Loss: 296.346, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 8795, Reward: -501.000, End reward: -501.000, Loss: 297.401, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8800, Reward: -501.000, End reward: -501.000, Loss: 300.209, Grad mean: 0.000005, Grad max: 0.246, Actions: 501, Entropy: 0.000\n",
      "Episode 8805, Reward: -501.000, End reward: -501.000, Loss: 299.997, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 8810, Reward: -501.000, End reward: -501.000, Loss: 300.815, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 8815, Reward: -501.000, End reward: -501.000, Loss: 298.852, Grad mean: 0.000005, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8820, Reward: -501.000, End reward: -501.000, Loss: 296.612, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8825, Reward: -501.000, End reward: -501.000, Loss: 295.312, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 8830, Reward: -501.000, End reward: -501.000, Loss: 295.582, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8835, Reward: -501.000, End reward: -501.000, Loss: 297.573, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8840, Reward: -501.000, End reward: -501.000, Loss: 296.305, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 8845, Reward: -501.000, End reward: -501.000, Loss: 297.350, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 8850, Reward: -501.000, End reward: -501.000, Loss: 294.909, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 8855, Reward: -501.000, End reward: -501.000, Loss: 297.124, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 8860, Reward: -501.000, End reward: -501.000, Loss: 296.882, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 8865, Reward: -501.000, End reward: -501.000, Loss: 296.311, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8870, Reward: -501.000, End reward: -501.000, Loss: 297.559, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 8875, Reward: -501.000, End reward: -501.000, Loss: 297.711, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 8880, Reward: -501.000, End reward: -501.000, Loss: 297.914, Grad mean: 0.000005, Grad max: 0.210, Actions: 501, Entropy: 0.000\n",
      "Episode 8885, Reward: -501.000, End reward: -501.000, Loss: 295.800, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8890, Reward: -501.000, End reward: -501.000, Loss: 298.329, Grad mean: 0.000005, Grad max: 0.183, Actions: 501, Entropy: 0.000\n",
      "Episode 8895, Reward: -501.000, End reward: -501.000, Loss: 298.497, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 8900, Reward: -501.000, End reward: -501.000, Loss: 296.701, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 8905, Reward: -501.000, End reward: -501.000, Loss: 296.667, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 8910, Reward: -501.000, End reward: -501.000, Loss: 298.472, Grad mean: 0.000004, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 8915, Reward: -501.000, End reward: -501.000, Loss: 298.702, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 8920, Reward: -501.000, End reward: -501.000, Loss: 294.972, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8925, Reward: -501.000, End reward: -501.000, Loss: 297.157, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 8930, Reward: -501.000, End reward: -501.000, Loss: 298.328, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 8935, Reward: -501.000, End reward: -501.000, Loss: 303.754, Grad mean: 0.000004, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 8940, Reward: -501.000, End reward: -501.000, Loss: 302.667, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 8945, Reward: -501.000, End reward: -501.000, Loss: 298.791, Grad mean: 0.000004, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 8950, Reward: -501.000, End reward: -501.000, Loss: 295.115, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 8955, Reward: -501.000, End reward: -501.000, Loss: 298.078, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 8960, Reward: -501.000, End reward: -501.000, Loss: 296.355, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 8965, Reward: -501.000, End reward: -501.000, Loss: 296.757, Grad mean: 0.000005, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8970, Reward: -501.000, End reward: -501.000, Loss: 296.045, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 8975, Reward: -501.000, End reward: -501.000, Loss: 297.368, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 8980, Reward: -501.000, End reward: -501.000, Loss: 294.862, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 8985, Reward: -501.000, End reward: -501.000, Loss: 298.244, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 8990, Reward: -501.000, End reward: -501.000, Loss: 296.414, Grad mean: 0.000005, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 8995, Reward: -501.000, End reward: -501.000, Loss: 299.556, Grad mean: 0.000004, Grad max: 0.191, Actions: 501, Entropy: 0.000\n",
      "Episode 9000, Reward: -501.000, End reward: -501.000, Loss: 296.489, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9005, Reward: -501.000, End reward: -501.000, Loss: 299.588, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 9010, Reward: -501.000, End reward: -501.000, Loss: 298.182, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9015, Reward: -501.000, End reward: -501.000, Loss: 294.867, Grad mean: 0.000005, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 9020, Reward: -501.000, End reward: -501.000, Loss: 296.453, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9025, Reward: -501.000, End reward: -501.000, Loss: 295.507, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9030, Reward: -501.000, End reward: -501.000, Loss: 296.064, Grad mean: 0.000005, Grad max: 0.138, Actions: 501, Entropy: 0.000\n",
      "Episode 9035, Reward: -501.000, End reward: -501.000, Loss: 297.653, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9040, Reward: -501.000, End reward: -501.000, Loss: 299.277, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9045, Reward: -501.000, End reward: -501.000, Loss: 296.850, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9050, Reward: -501.000, End reward: -501.000, Loss: 296.155, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9055, Reward: -501.000, End reward: -501.000, Loss: 296.971, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 9060, Reward: -501.000, End reward: -501.000, Loss: 297.520, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9065, Reward: -501.000, End reward: -501.000, Loss: 296.573, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9070, Reward: -501.000, End reward: -501.000, Loss: 296.703, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 9075, Reward: -501.000, End reward: -501.000, Loss: 295.392, Grad mean: 0.000004, Grad max: 0.219, Actions: 501, Entropy: 0.000\n",
      "Episode 9080, Reward: -501.000, End reward: -501.000, Loss: 296.590, Grad mean: 0.000004, Grad max: 0.141, Actions: 501, Entropy: 0.000\n",
      "Episode 9085, Reward: -501.000, End reward: -501.000, Loss: 296.298, Grad mean: 0.000005, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 9090, Reward: -501.000, End reward: -501.000, Loss: 298.364, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 9095, Reward: -501.000, End reward: -501.000, Loss: 295.850, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9100, Reward: -501.000, End reward: -501.000, Loss: 296.291, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9105, Reward: -501.000, End reward: -501.000, Loss: 295.297, Grad mean: 0.000005, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 9110, Reward: -501.000, End reward: -501.000, Loss: 295.512, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9115, Reward: -501.000, End reward: -501.000, Loss: 297.276, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9120, Reward: -501.000, End reward: -501.000, Loss: 296.354, Grad mean: 0.000005, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 9125, Reward: -501.000, End reward: -501.000, Loss: 298.516, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9130, Reward: -501.000, End reward: -501.000, Loss: 301.264, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9135, Reward: -501.000, End reward: -501.000, Loss: 296.175, Grad mean: 0.000005, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 9140, Reward: -501.000, End reward: -501.000, Loss: 300.069, Grad mean: 0.000005, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 9145, Reward: -501.000, End reward: -501.000, Loss: 298.246, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9150, Reward: -501.000, End reward: -501.000, Loss: 299.396, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9155, Reward: -501.000, End reward: -501.000, Loss: 298.884, Grad mean: 0.000005, Grad max: 0.219, Actions: 501, Entropy: 0.000\n",
      "Episode 9160, Reward: -501.000, End reward: -501.000, Loss: 296.197, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9165, Reward: -501.000, End reward: -501.000, Loss: 296.497, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9170, Reward: -501.000, End reward: -501.000, Loss: 296.357, Grad mean: 0.000005, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 9175, Reward: -501.000, End reward: -501.000, Loss: 295.716, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9180, Reward: -501.000, End reward: -501.000, Loss: 296.243, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9185, Reward: -501.000, End reward: -501.000, Loss: 294.613, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9190, Reward: -501.000, End reward: -501.000, Loss: 296.271, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 9195, Reward: -501.000, End reward: -501.000, Loss: 295.806, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9200, Reward: -501.000, End reward: -501.000, Loss: 297.287, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9205, Reward: -501.000, End reward: -501.000, Loss: 298.547, Grad mean: 0.000005, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 9210, Reward: -501.000, End reward: -501.000, Loss: 302.427, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9215, Reward: -501.000, End reward: -501.000, Loss: 295.078, Grad mean: 0.000005, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 9220, Reward: -501.000, End reward: -501.000, Loss: 294.740, Grad mean: 0.000005, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 9225, Reward: -501.000, End reward: -501.000, Loss: 295.835, Grad mean: 0.000005, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 9230, Reward: -501.000, End reward: -501.000, Loss: 294.914, Grad mean: 0.000005, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 9235, Reward: -501.000, End reward: -501.000, Loss: 297.255, Grad mean: 0.000005, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 9240, Reward: -501.000, End reward: -501.000, Loss: 297.351, Grad mean: 0.000005, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 9245, Reward: -501.000, End reward: -501.000, Loss: 294.952, Grad mean: 0.000005, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 9250, Reward: -501.000, End reward: -501.000, Loss: 301.487, Grad mean: 0.000005, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 9255, Reward: -501.000, End reward: -501.000, Loss: 294.779, Grad mean: 0.000005, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 9260, Reward: -501.000, End reward: -501.000, Loss: 296.484, Grad mean: 0.000004, Grad max: 0.144, Actions: 501, Entropy: 0.000\n",
      "Episode 9265, Reward: -501.000, End reward: -501.000, Loss: 298.297, Grad mean: 0.000005, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 9270, Reward: -501.000, End reward: -501.000, Loss: 298.870, Grad mean: 0.000004, Grad max: 0.142, Actions: 501, Entropy: 0.000\n",
      "Episode 9275, Reward: -501.000, End reward: -501.000, Loss: 296.941, Grad mean: 0.000003, Grad max: 0.359, Actions: 501, Entropy: 0.000\n",
      "Episode 9280, Reward: -501.000, End reward: -501.000, Loss: 296.787, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 9285, Reward: -501.000, End reward: -501.000, Loss: 299.210, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9290, Reward: -501.000, End reward: -501.000, Loss: 298.840, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9295, Reward: -501.000, End reward: -501.000, Loss: 297.143, Grad mean: 0.000004, Grad max: 0.142, Actions: 501, Entropy: 0.000\n",
      "Episode 9300, Reward: -501.000, End reward: -501.000, Loss: 295.099, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9305, Reward: -501.000, End reward: -501.000, Loss: 298.259, Grad mean: 0.000004, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9310, Reward: -501.000, End reward: -501.000, Loss: 297.365, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9315, Reward: -501.000, End reward: -501.000, Loss: 296.766, Grad mean: 0.000004, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9320, Reward: -501.000, End reward: -501.000, Loss: 298.445, Grad mean: 0.000005, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9325, Reward: -501.000, End reward: -501.000, Loss: 297.498, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9330, Reward: -501.000, End reward: -501.000, Loss: 298.962, Grad mean: 0.000004, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9335, Reward: -501.000, End reward: -501.000, Loss: 296.690, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9340, Reward: -501.000, End reward: -501.000, Loss: 295.748, Grad mean: 0.000005, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9345, Reward: -501.000, End reward: -501.000, Loss: 297.263, Grad mean: 0.000005, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9350, Reward: -501.000, End reward: -501.000, Loss: 299.122, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9355, Reward: -501.000, End reward: -501.000, Loss: 296.858, Grad mean: 0.000004, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9360, Reward: -501.000, End reward: -501.000, Loss: 297.667, Grad mean: 0.000004, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9365, Reward: -501.000, End reward: -501.000, Loss: 297.445, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9370, Reward: -501.000, End reward: -501.000, Loss: 296.122, Grad mean: 0.000005, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9375, Reward: -501.000, End reward: -501.000, Loss: 296.790, Grad mean: 0.000005, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 9380, Reward: -501.000, End reward: -501.000, Loss: 296.372, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9385, Reward: -501.000, End reward: -501.000, Loss: 298.818, Grad mean: 0.000004, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9390, Reward: -501.000, End reward: -501.000, Loss: 297.500, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9395, Reward: -501.000, End reward: -501.000, Loss: 296.693, Grad mean: 0.000004, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9400, Reward: -501.000, End reward: -501.000, Loss: 298.220, Grad mean: 0.000005, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9405, Reward: -501.000, End reward: -501.000, Loss: 298.334, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9410, Reward: -501.000, End reward: -501.000, Loss: 296.400, Grad mean: 0.000005, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9415, Reward: -501.000, End reward: -501.000, Loss: 296.033, Grad mean: 0.000005, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9420, Reward: -501.000, End reward: -501.000, Loss: 296.043, Grad mean: 0.000005, Grad max: 0.224, Actions: 501, Entropy: 0.000\n",
      "Episode 9425, Reward: -501.000, End reward: -501.000, Loss: 295.870, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9430, Reward: -501.000, End reward: -501.000, Loss: 297.191, Grad mean: 0.000004, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9435, Reward: -501.000, End reward: -501.000, Loss: 297.054, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9440, Reward: -501.000, End reward: -501.000, Loss: 296.605, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9445, Reward: -501.000, End reward: -501.000, Loss: 296.030, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9450, Reward: -501.000, End reward: -501.000, Loss: 297.517, Grad mean: 0.000005, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9455, Reward: -501.000, End reward: -501.000, Loss: 299.299, Grad mean: 0.000004, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9460, Reward: -501.000, End reward: -501.000, Loss: 297.182, Grad mean: 0.000005, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9465, Reward: -501.000, End reward: -501.000, Loss: 297.481, Grad mean: 0.000004, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9470, Reward: -501.000, End reward: -501.000, Loss: 298.882, Grad mean: 0.000004, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9475, Reward: -501.000, End reward: -501.000, Loss: 297.832, Grad mean: 0.000004, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9480, Reward: -501.000, End reward: -501.000, Loss: 295.181, Grad mean: 0.000005, Grad max: 0.134, Actions: 501, Entropy: 0.000\n",
      "Episode 9485, Reward: -501.000, End reward: -501.000, Loss: 296.851, Grad mean: 0.000005, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 9490, Reward: -501.000, End reward: -501.000, Loss: 298.726, Grad mean: 0.000004, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9495, Reward: -501.000, End reward: -501.000, Loss: 297.867, Grad mean: 0.000004, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9500, Reward: -501.000, End reward: -501.000, Loss: 296.127, Grad mean: 0.000005, Grad max: 0.142, Actions: 501, Entropy: 0.000\n",
      "Episode 9505, Reward: -501.000, End reward: -501.000, Loss: 296.466, Grad mean: 0.000004, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9510, Reward: -501.000, End reward: -501.000, Loss: 298.888, Grad mean: 0.000004, Grad max: 0.149, Actions: 501, Entropy: 0.000\n",
      "Episode 9515, Reward: -501.000, End reward: -501.000, Loss: 297.713, Grad mean: 0.000004, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9520, Reward: -501.000, End reward: -501.000, Loss: 296.841, Grad mean: 0.000004, Grad max: 0.149, Actions: 501, Entropy: 0.000\n",
      "Episode 9525, Reward: -501.000, End reward: -501.000, Loss: 296.497, Grad mean: 0.000004, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9530, Reward: -501.000, End reward: -501.000, Loss: 296.285, Grad mean: 0.000004, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9535, Reward: -501.000, End reward: -501.000, Loss: 294.842, Grad mean: 0.000004, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9540, Reward: -501.000, End reward: -501.000, Loss: 297.814, Grad mean: 0.000004, Grad max: 0.149, Actions: 501, Entropy: 0.000\n",
      "Episode 9545, Reward: -501.000, End reward: -501.000, Loss: 299.095, Grad mean: 0.000004, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9550, Reward: -501.000, End reward: -501.000, Loss: 297.427, Grad mean: 0.000004, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9555, Reward: -501.000, End reward: -501.000, Loss: 296.109, Grad mean: 0.000004, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9560, Reward: -501.000, End reward: -501.000, Loss: 296.643, Grad mean: 0.000005, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 9565, Reward: -501.000, End reward: -501.000, Loss: 296.461, Grad mean: 0.000004, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9570, Reward: -501.000, End reward: -501.000, Loss: 297.285, Grad mean: 0.000004, Grad max: 0.270, Actions: 501, Entropy: 0.000\n",
      "Episode 9575, Reward: -501.000, End reward: -501.000, Loss: 295.632, Grad mean: 0.000005, Grad max: 0.193, Actions: 501, Entropy: 0.000\n",
      "Episode 9580, Reward: -501.000, End reward: -501.000, Loss: 297.339, Grad mean: 0.000005, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 9585, Reward: -501.000, End reward: -501.000, Loss: 296.827, Grad mean: 0.000004, Grad max: 0.149, Actions: 501, Entropy: 0.000\n",
      "Episode 9590, Reward: -501.000, End reward: -501.000, Loss: 296.372, Grad mean: 0.000004, Grad max: 0.149, Actions: 501, Entropy: 0.000\n",
      "Episode 9595, Reward: -501.000, End reward: -501.000, Loss: 295.458, Grad mean: 0.000004, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9600, Reward: -501.000, End reward: -501.000, Loss: 298.894, Grad mean: 0.000004, Grad max: 0.149, Actions: 501, Entropy: 0.000\n",
      "Episode 9605, Reward: -501.000, End reward: -501.000, Loss: 295.294, Grad mean: 0.000004, Grad max: 0.149, Actions: 501, Entropy: 0.000\n",
      "Episode 9610, Reward: -501.000, End reward: -501.000, Loss: 295.807, Grad mean: 0.000004, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9615, Reward: -501.000, End reward: -501.000, Loss: 295.910, Grad mean: 0.000004, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9620, Reward: -501.000, End reward: -501.000, Loss: 300.105, Grad mean: 0.000004, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9625, Reward: -501.000, End reward: -501.000, Loss: 302.457, Grad mean: 0.000004, Grad max: 0.150, Actions: 501, Entropy: 0.000\n",
      "Episode 9630, Reward: -501.000, End reward: -501.000, Loss: 295.384, Grad mean: 0.000004, Grad max: 0.149, Actions: 501, Entropy: 0.000\n",
      "Episode 9635, Reward: -501.000, End reward: -501.000, Loss: 299.879, Grad mean: 0.000004, Grad max: 0.146, Actions: 501, Entropy: 0.000\n",
      "Episode 9640, Reward: -501.000, End reward: -501.000, Loss: 297.494, Grad mean: 0.000004, Grad max: 0.148, Actions: 501, Entropy: 0.000\n",
      "Episode 9645, Reward: -501.000, End reward: -501.000, Loss: 296.801, Grad mean: 0.000005, Grad max: 0.147, Actions: 501, Entropy: 0.000\n",
      "Episode 9650, Reward: -501.000, End reward: -501.000, Loss: 295.359, Grad mean: 0.000004, Grad max: 0.149, Actions: 501, Entropy: 0.000\n",
      "Episode 9655, Reward: -501.000, End reward: -501.000, Loss: 303.353, Grad mean: 0.000004, Grad max: 0.149, Actions: 501, Entropy: 0.000\n",
      "Episode 9660, Reward: -501.000, End reward: -501.000, Loss: 297.058, Grad mean: 0.000004, Grad max: 0.149, Actions: 501, Entropy: 0.000\n",
      "Episode 9665, Reward: -501.000, End reward: -501.000, Loss: 301.815, Grad mean: 0.000004, Grad max: 0.149, Actions: 501, Entropy: 0.000\n",
      "Episode 9670, Reward: -501.000, End reward: -501.000, Loss: 301.742, Grad mean: 0.000004, Grad max: 0.150, Actions: 501, Entropy: 0.000\n",
      "Episode 9675, Reward: -501.000, End reward: -501.000, Loss: 299.319, Grad mean: 0.000004, Grad max: 0.149, Actions: 501, Entropy: 0.000\n",
      "Episode 9680, Reward: -501.000, End reward: -501.000, Loss: 300.499, Grad mean: 0.000004, Grad max: 0.150, Actions: 501, Entropy: 0.000\n",
      "Episode 9685, Reward: -501.000, End reward: -501.000, Loss: 299.690, Grad mean: 0.000004, Grad max: 0.149, Actions: 501, Entropy: 0.000\n",
      "Episode 9690, Reward: -501.000, End reward: -501.000, Loss: 299.816, Grad mean: 0.000004, Grad max: 0.150, Actions: 501, Entropy: 0.000\n",
      "Episode 9695, Reward: -501.000, End reward: -501.000, Loss: 301.501, Grad mean: 0.000004, Grad max: 0.150, Actions: 501, Entropy: 0.000\n",
      "Episode 9700, Reward: -501.000, End reward: -501.000, Loss: 301.473, Grad mean: 0.000004, Grad max: 0.150, Actions: 501, Entropy: 0.000\n",
      "Episode 9705, Reward: -501.000, End reward: -501.000, Loss: 303.609, Grad mean: 0.000004, Grad max: 0.149, Actions: 501, Entropy: 0.000\n",
      "Episode 9710, Reward: -501.000, End reward: -501.000, Loss: 301.635, Grad mean: 0.000004, Grad max: 0.150, Actions: 501, Entropy: 0.000\n",
      "Episode 9715, Reward: -501.000, End reward: -501.000, Loss: 299.607, Grad mean: 0.000004, Grad max: 0.150, Actions: 501, Entropy: 0.000\n",
      "Episode 9720, Reward: -501.000, End reward: -501.000, Loss: 301.250, Grad mean: 0.000004, Grad max: 0.150, Actions: 501, Entropy: 0.000\n",
      "Episode 9725, Reward: -501.000, End reward: -501.000, Loss: 300.689, Grad mean: 0.000004, Grad max: 0.150, Actions: 501, Entropy: 0.000\n",
      "Episode 9730, Reward: -501.000, End reward: -501.000, Loss: 299.649, Grad mean: 0.000004, Grad max: 0.150, Actions: 501, Entropy: 0.000\n",
      "Episode 9735, Reward: -501.000, End reward: -501.000, Loss: 300.857, Grad mean: 0.000004, Grad max: 0.150, Actions: 501, Entropy: 0.000\n",
      "Episode 9740, Reward: -501.000, End reward: -501.000, Loss: 299.668, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9745, Reward: -501.000, End reward: -501.000, Loss: 302.042, Grad mean: 0.000004, Grad max: 0.150, Actions: 501, Entropy: 0.000\n",
      "Episode 9750, Reward: -501.000, End reward: -501.000, Loss: 301.790, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9755, Reward: -501.000, End reward: -501.000, Loss: 299.264, Grad mean: 0.000004, Grad max: 0.150, Actions: 501, Entropy: 0.000\n",
      "Episode 9760, Reward: -501.000, End reward: -501.000, Loss: 301.614, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9765, Reward: -501.000, End reward: -501.000, Loss: 298.809, Grad mean: 0.000004, Grad max: 0.150, Actions: 501, Entropy: 0.000\n",
      "Episode 9770, Reward: -501.000, End reward: -501.000, Loss: 301.104, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9775, Reward: -501.000, End reward: -501.000, Loss: 298.499, Grad mean: 0.000004, Grad max: 0.150, Actions: 501, Entropy: 0.000\n",
      "Episode 9780, Reward: -501.000, End reward: -501.000, Loss: 300.691, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9785, Reward: -501.000, End reward: -501.000, Loss: 299.435, Grad mean: 0.000004, Grad max: 0.150, Actions: 501, Entropy: 0.000\n",
      "Episode 9790, Reward: -501.000, End reward: -501.000, Loss: 301.163, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9795, Reward: -501.000, End reward: -501.000, Loss: 303.304, Grad mean: 0.000004, Grad max: 0.150, Actions: 501, Entropy: 0.000\n",
      "Episode 9800, Reward: -501.000, End reward: -501.000, Loss: 302.025, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9805, Reward: -501.000, End reward: -501.000, Loss: 299.566, Grad mean: 0.000004, Grad max: 0.150, Actions: 501, Entropy: 0.000\n",
      "Episode 9810, Reward: -501.000, End reward: -501.000, Loss: 302.920, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9815, Reward: -501.000, End reward: -501.000, Loss: 300.665, Grad mean: 0.000004, Grad max: 0.150, Actions: 501, Entropy: 0.000\n",
      "Episode 9820, Reward: -501.000, End reward: -501.000, Loss: 298.509, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9825, Reward: -501.000, End reward: -501.000, Loss: 301.052, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9830, Reward: -501.000, End reward: -501.000, Loss: 300.017, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9835, Reward: -501.000, End reward: -501.000, Loss: 299.606, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9840, Reward: -501.000, End reward: -501.000, Loss: 301.951, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9845, Reward: -501.000, End reward: -501.000, Loss: 299.680, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9850, Reward: -501.000, End reward: -501.000, Loss: 301.733, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9855, Reward: -501.000, End reward: -501.000, Loss: 299.965, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9860, Reward: -501.000, End reward: -501.000, Loss: 301.547, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9865, Reward: -501.000, End reward: -501.000, Loss: 298.585, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9870, Reward: -501.000, End reward: -501.000, Loss: 296.856, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9875, Reward: -501.000, End reward: -501.000, Loss: 298.104, Grad mean: 0.000004, Grad max: 0.152, Actions: 501, Entropy: 0.000\n",
      "Episode 9880, Reward: -501.000, End reward: -501.000, Loss: 299.523, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9885, Reward: -501.000, End reward: -501.000, Loss: 305.857, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9890, Reward: -501.000, End reward: -501.000, Loss: 305.293, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9895, Reward: -501.000, End reward: -501.000, Loss: 295.981, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9900, Reward: -501.000, End reward: -501.000, Loss: 296.470, Grad mean: 0.000003, Grad max: 0.329, Actions: 501, Entropy: 0.000\n",
      "Episode 9905, Reward: -501.000, End reward: -501.000, Loss: 298.576, Grad mean: 0.000004, Grad max: 0.199, Actions: 501, Entropy: 0.000\n",
      "Episode 9910, Reward: -501.000, End reward: -501.000, Loss: 296.027, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9915, Reward: -501.000, End reward: -501.000, Loss: 294.790, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9920, Reward: -501.000, End reward: -501.000, Loss: 296.870, Grad mean: 0.000004, Grad max: 0.152, Actions: 501, Entropy: 0.000\n",
      "Episode 9925, Reward: -501.000, End reward: -501.000, Loss: 296.815, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9930, Reward: -501.000, End reward: -501.000, Loss: 296.663, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9935, Reward: -501.000, End reward: -501.000, Loss: 296.278, Grad mean: 0.000004, Grad max: 0.145, Actions: 501, Entropy: 0.000\n",
      "Episode 9940, Reward: -501.000, End reward: -501.000, Loss: 296.826, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9945, Reward: -501.000, End reward: -501.000, Loss: 298.127, Grad mean: 0.000005, Grad max: 0.131, Actions: 501, Entropy: 0.000\n",
      "Episode 9950, Reward: -501.000, End reward: -501.000, Loss: 296.385, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9955, Reward: -501.000, End reward: -501.000, Loss: 296.016, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9960, Reward: -501.000, End reward: -501.000, Loss: 297.323, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9965, Reward: -501.000, End reward: -501.000, Loss: 297.662, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9970, Reward: -501.000, End reward: -501.000, Loss: 298.737, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9975, Reward: -501.000, End reward: -501.000, Loss: 297.332, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9980, Reward: -501.000, End reward: -501.000, Loss: 296.436, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9985, Reward: -501.000, End reward: -501.000, Loss: 296.132, Grad mean: 0.000004, Grad max: 0.150, Actions: 501, Entropy: 0.000\n",
      "Episode 9990, Reward: -501.000, End reward: -501.000, Loss: 296.215, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Episode 9995, Reward: -501.000, End reward: -501.000, Loss: 296.090, Grad mean: 0.000004, Grad max: 0.151, Actions: 501, Entropy: 0.000\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_episodes = 10000\n",
    "gamma = 0.99\n",
    "end_rewards = []\n",
    "total_rewards = []\n",
    "num_actions_per_episode = []\n",
    "losses = []\n",
    "grad_means = []\n",
    "grad_maxs = []\n",
    "entropies = []\n",
    "max_steps_per_episode = 500\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    #state, _ = env.reset(seed=42)\n",
    "    state, _ = env.reset()\n",
    "    #env.render() #Debug\n",
    "    state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "    done = False\n",
    "    episode_reward = 0.0\n",
    "    episode_counter = 0\n",
    "    \n",
    "    states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, done, _, _ = env.step(action.squeeze().cpu().numpy())\n",
    "\n",
    "        # Check if we have reached the maximum number of steps per episode, else we could go on forever...\n",
    "        if episode_counter >= max_steps_per_episode:\n",
    "            done = True\n",
    "        \n",
    "        next_state = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "        reward = torch.FloatTensor([reward]).unsqueeze(0).to(device)\n",
    "        done = torch.FloatTensor([int(done)]).unsqueeze(0).to(device)\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        next_states.append(next_state)\n",
    "        dones.append(done)\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward.item()\n",
    "        episode_counter += 1\n",
    "        #print(f't:{episode_counter} Actions: {action} reward: {reward.item()} total_reward: {episode_reward}')\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    states = torch.vstack(states)\n",
    "    actions = torch.vstack(actions)\n",
    "    rewards = torch.vstack(rewards)\n",
    "    next_states = torch.vstack(next_states)\n",
    "    dones = torch.vstack(dones)\n",
    "    \n",
    "    # Compute returns and advantages\n",
    "    returns = []\n",
    "    R = 0\n",
    "    for reward, done in zip(reversed(rewards.cpu().numpy()), reversed(dones.cpu().numpy())):\n",
    "        R = reward + gamma * R * (1 - done)\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.FloatTensor(returns).to(device)\n",
    "\n",
    "    state_values = agent.get_value(states).squeeze()  # Ensure it's 1D\n",
    "    next_state_values = agent.get_value(next_states).squeeze()  # Ensure it's 1D\n",
    "\n",
    "    # Ensure returns has the same shape as state_values\n",
    "    returns = returns.view_as(state_values)\n",
    "\n",
    "    td_errors = rewards.squeeze() + gamma * next_state_values * (1 - dones.squeeze()) - state_values\n",
    "    advantages = td_errors.detach()\n",
    "\n",
    "    # Compute losses\n",
    "    action_log_probs, entropy = agent.get_action_log_probs(states, actions)\n",
    "    actor_loss = -(action_log_probs * advantages).mean()\n",
    "    critic_loss = F.mse_loss(state_values, returns)\n",
    "\n",
    "    #Add entropy to the loss\n",
    "    entropy_mean = entropy.mean()\n",
    "    loss = actor_loss + 0.5 * critic_loss - 0.001 * entropy_mean\n",
    "        \n",
    "    # Perform the update\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    #Clip the gradients\n",
    "    torch.nn.utils.clip_grad_norm_(agent.parameters(), 1.0)\n",
    "\n",
    "\n",
    "    total_grad = 0\n",
    "    total_params = 0\n",
    "    max_grad = 0\n",
    "\n",
    "    for param in agent.parameters():\n",
    "        if param.grad is not None:\n",
    "            total_grad += param.grad.abs().sum().item()\n",
    "            total_params += param.grad.numel()\n",
    "            max_grad = max(max_grad, param.grad.abs().max().item())\n",
    "\n",
    "    grad_mean = total_grad / total_params if total_params > 0 else 0\n",
    "\n",
    "    # Step optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    #Add metrics\n",
    "    grad_means.append(grad_mean)\n",
    "    grad_maxs.append(max_grad)\n",
    "    losses.append(loss.item())\n",
    "    #end_reward = env.fitness()\n",
    "    end_reward = episode_reward #Just use the episode reward for now\n",
    "    end_rewards.append(end_reward)\n",
    "    total_rewards.append(episode_reward)\n",
    "    num_actions_per_episode.append(episode_counter)\n",
    "    entropies.append(entropy_mean.item())\n",
    "        \n",
    "    if episode % 5 == 0:\n",
    "        print(f\"Episode {episode}, Reward: {episode_reward:.3f}, End reward: {end_reward:.3f}, Loss: {loss:.3f}, Grad mean: {grad_mean:.6f}, Grad max: {max_grad:.3f}, Actions: {episode_counter}, Entropy: {entropy_mean:.3f}\")\n",
    "\n",
    "print('Training complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state, reward, done, _, _ = env.step(action.cpu().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0004, device='cuda:0')\n",
      "tensor(0.0010, device='cuda:0')\n",
      "tensor(4.1190e-06, device='cuda:0')\n",
      "tensor(3.0017e-05, device='cuda:0')\n",
      "tensor(1.0258e-17, device='cuda:0')\n",
      "tensor(3.1808e-17, device='cuda:0')\n",
      "tensor(1.9361e-06, device='cuda:0')\n",
      "tensor(6.3705e-06, device='cuda:0')\n",
      "tensor(0.0046, device='cuda:0')\n",
      "tensor(0.0042, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for param in agent.parameters():\n",
    "    print(param.grad.data.abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Entropy')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the losses\n",
    "losses_rolling = [np.mean(losses[max(0, i-100):i+1]) for i in range(len(losses))]\n",
    "plt.plot(losses_rolling)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.figure()\n",
    "# Plot the rewards\n",
    "total_rewards_rolling = [np.mean(total_rewards[max(0, i-100):i+1]) for i in range(len(total_rewards))]\n",
    "plt.plot(total_rewards_rolling)\n",
    "#Plot the 100 episode rolling average\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "\n",
    "# plot the end rewards\n",
    "end_rewards_rolling = [np.mean(end_rewards[max(0, i-100):i+1]) for i in range(len(end_rewards))]\n",
    "plt.figure()\n",
    "plt.plot(end_rewards_rolling)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('End Reward')\n",
    "\n",
    "# plot the end rewards\n",
    "num_actions_per_episode_rolling = [np.mean(num_actions_per_episode[max(0, i-100):i+1]) for i in range(len(num_actions_per_episode))]\n",
    "plt.figure()\n",
    "plt.plot(num_actions_per_episode_rolling)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Actions per episode')\n",
    "\n",
    "# Entropies\n",
    "plt.figure()\n",
    "#Rolling average\n",
    "entropies_rolling = [np.mean(entropies[max(0, i-100):i+1]) for i in range(len(entropies))]\n",
    "plt.plot(entropies_rolling)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Entropy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_actor_critic_agent(agent, env, n_episodes=5, seed=None):\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        state, info = env.reset(seed=seed)\n",
    "        done = False\n",
    "        score = 0\n",
    "        max_t = 5000\n",
    "        t = 0\n",
    "        while not done:\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            action = agent.choose_action(state)\n",
    "            action = action.cpu().numpy()\n",
    "            print('-----------------')\n",
    "            print(f'action: {action}')\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            t += 1\n",
    "            if done or t >= max_t:\n",
    "                break\n",
    "        print(f\"Episode {i_episode}\\tScore: {score}\")\n",
    "\n",
    "        #Final render\n",
    "        env.render()\n",
    "    #env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "action: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\tf38\\lib\\site-packages\\gymnasium\\envs\\classic_control\\mountain_car.py:173: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "array([0], dtype=int64) (<class 'numpy.ndarray'>) invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mvisualize_actor_critic_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[36], line 15\u001b[0m, in \u001b[0;36mvisualize_actor_critic_agent\u001b[1;34m(agent, env, n_episodes, seed)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-----------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     17\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf38\\lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf38\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf38\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf38\\lib\\site-packages\\gymnasium\\envs\\classic_control\\mountain_car.py:129\u001b[0m, in \u001b[0;36mMountainCarEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mcontains(\n\u001b[0;32m    130\u001b[0m         action\n\u001b[0;32m    131\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(action)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) invalid\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    133\u001b[0m     position, velocity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\n\u001b[0;32m    134\u001b[0m     velocity \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (action \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforce \u001b[38;5;241m+\u001b[39m math\u001b[38;5;241m.\u001b[39mcos(\u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m position) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgravity)\n",
      "\u001b[1;31mAssertionError\u001b[0m: array([0], dtype=int64) (<class 'numpy.ndarray'>) invalid"
     ]
    }
   ],
   "source": [
    "visualize_actor_critic_agent(agent, env,n_episodes=1, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize gym agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_agent(agent, env, n_episodes=5):\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        max_t = 500\n",
    "        t = 0\n",
    "        while not done:\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            action = agent.choose_action(state)\n",
    "            action = action.squeeze().cpu().numpy()\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            t += 1\n",
    "            if done or t >= max_t:\n",
    "                break\n",
    "        print(f\"Episode {i_episode}\\tScore: {score}\")\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tScore: 500.0\n",
      "Episode 2\tScore: 500.0\n",
      "Episode 3\tScore: 500.0\n",
      "Episode 4\tScore: 500.0\n",
      "Episode 5\tScore: 500.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name, render_mode=\"human\")\n",
    "visualize_agent(agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
