{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from distutils.util import strtobool\n",
    "\n",
    "#import gym\n",
    "from ScheduleGym import ScheduleGym\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import tempfile\n",
    "import json\n",
    "import shutil\n",
    "import imageio\n",
    "from types import SimpleNamespace\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, state_size, action_sizes, hidden_dim=256):\n",
    "        super(Agent, self).__init__()\n",
    "\n",
    "        self.shared = nn.Sequential(\n",
    "            self.layer_init(nn.Linear(state_size, hidden_dim)),\n",
    "            nn.ReLU(),\n",
    "            self.layer_init(nn.Linear(hidden_dim, hidden_dim)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Create separate heads for each action dimension\n",
    "        self.action_heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                self.layer_init(nn.Linear(hidden_dim, action_size))\n",
    "            )\n",
    "            for action_size in action_sizes\n",
    "        ])\n",
    "\n",
    "        self.value_net = nn.Sequential(\n",
    "            self.layer_init(nn.Linear(hidden_dim, 1))\n",
    "        )\n",
    "\n",
    "        self.action_sizes = action_sizes\n",
    "\n",
    "    def layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared(x)\n",
    "        policy_logits = [head(x) for head in self.action_heads]\n",
    "        value = self.value_net(x)\n",
    "        return policy_logits, value\n",
    "\n",
    "    def get_action_and_value(self, state):\n",
    "        policy_logits, value = self.forward(state)\n",
    "        action_probs = [torch.softmax(logits, dim=-1) for logits in policy_logits]\n",
    "        dists = [Categorical(probs) for probs in action_probs]\n",
    "        actions = [dist.sample() for dist in dists]\n",
    "        logprobs = [dist.log_prob(action) for dist, action in zip(dists, actions)]\n",
    "        return actions, logprobs, [dist.entropy() for dist in dists], value\n",
    "\n",
    "    def get_value(self, state):\n",
    "        _, value = self.forward(state)\n",
    "        return value\n",
    "\n",
    "    def get_policy(self, state):\n",
    "        policy_logits, _ = self.forward(state)\n",
    "        return [torch.softmax(logits, dim=-1) for logits in policy_logits]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "\n",
    "class MCTSNode:\n",
    "    def __init__(self, env, agent, parent=None, prior=1.0):\n",
    "        self.env = env  # env is the environment instance, which can take actions\n",
    "        self.agent = agent\n",
    "        self.parent = parent\n",
    "        self.children = {}\n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0\n",
    "        self.prior = prior\n",
    "        self.explored_actions = set()\n",
    "\n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.explored_actions) == np.prod(self.agent.action_sizes)\n",
    "\n",
    "    def best_child(self, c_param=1.4):\n",
    "        choices_weights = [\n",
    "            (child.value_sum / child.visit_count) + c_param * child.prior * np.sqrt(self.visit_count) / (1 + child.visit_count)\n",
    "            for child in self.children.values()\n",
    "        ]\n",
    "        return list(self.children.values())[np.argmax(choices_weights)]\n",
    "\n",
    "    def select_child(self, c_param=1.4):\n",
    "        return self.best_child(c_param)\n",
    "    \n",
    "    def expand(self):\n",
    "        if self.is_fully_expanded():\n",
    "            raise Exception(\"Cannot expand a fully expanded node.\")\n",
    "\n",
    "        for _ in range(np.prod(self.agent.action_sizes)):\n",
    "            # Convert the action list to a tuple for correct set operations\n",
    "            action = tuple(random.randint(0, size - 1) for size in self.agent.action_sizes)\n",
    "            if action not in self.explored_actions:\n",
    "                self.explored_actions.add(action)\n",
    "                \n",
    "                # Deep copy the environment to create a new child node with its own environment\n",
    "                new_env = copy.deepcopy(self.env)\n",
    "                \n",
    "                # Perform the action in the copied environment\n",
    "                new_env.step(action)\n",
    "                \n",
    "                # Create a child node with this new environment state\n",
    "                child_node = MCTSNode(new_env, self.agent, parent=self, prior=self.prior)\n",
    "                self.children[action] = child_node\n",
    "                return child_node\n",
    "\n",
    "        raise Exception(\"No unexplored actions found, despite not being fully expanded.\")\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    def rollout(self):\n",
    "        # Start with a deep copy of the current environment (not just the state)\n",
    "        rollout_env = copy.deepcopy(self.env)\n",
    "        while not rollout_env.is_done():\n",
    "            \n",
    "            state_vector = torch.tensor(rollout_env.state2vector()).to(device).float()\n",
    "        \n",
    "            action_probs = self.agent.get_policy(state_vector)\n",
    "            action = [torch.multinomial(probs, 1).item() for probs in action_probs]\n",
    "            rollout_env.step(action)  # Perform the action in the copied environment\n",
    "        return rollout_env.fitness()  # Use the environment's method to compute fitness\n",
    "\n",
    "    def backpropagate(self, reward):\n",
    "        self.visit_count += 1\n",
    "        self.value_sum += reward\n",
    "        if self.parent:\n",
    "            self.parent.backpropagate(reward)\n",
    "\n",
    "def mcts(env, agent, num_simulations):\n",
    "    root = MCTSNode(env, agent)\n",
    "    for _ in range(num_simulations):\n",
    "        node = root\n",
    "        while node.is_fully_expanded() and node.children:\n",
    "            node = node.select_child()\n",
    "        if not node.is_fully_expanded():\n",
    "            node = node.expand()\n",
    "        reward = node.rollout()\n",
    "        node.backpropagate(reward)\n",
    "    return list(root.children.keys())[np.argmax([child.visit_count for child in root.children.values()])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ScheduleGym(num_days=2, num_hours=4, num_classes=1, num_subjects=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State dimension: 10\n",
      "Action dimensions: [2, 1, 2, 4, 2]\n"
     ]
    }
   ],
   "source": [
    "state_dim = env.observation_space.shape[0]\n",
    "action_dims = env.get_action_sizes()\n",
    "print(f'State dimension: {state_dim}')\n",
    "print(f'Action dimensions: {action_dims}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, agent, num_simulations, seed=None):\n",
    "    obs,_ = env.reset(seed=seed)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = mcts(env, agent, num_simulations=num_simulations)\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_ppo(env, agent, optimizer, ppo_epochs, batch_size, seed, clip_coef, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm):\n",
    "    obs, _ = env.reset(seed=seed)\n",
    "    done = False\n",
    "\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    values = []\n",
    "    logprobs = []\n",
    "    dones = []\n",
    "    next_obs = obs\n",
    "\n",
    "    # Rollout the episode\n",
    "    while not done:\n",
    "        action, logprob, _, value = agent.get_action_and_value(torch.tensor(next_obs.state2vector()).to(device))\n",
    "        next_obs, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        states.append(next_obs)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        values.append(value.item())\n",
    "        logprobs.append(logprob)\n",
    "        dones.append(done)\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    states = torch.tensor([s.state2vector() for s in states]).to(device)\n",
    "    rewards = torch.tensor(rewards).to(device)\n",
    "    dones = torch.tensor(dones).to(device)\n",
    "    values = torch.tensor(values).to(device)\n",
    "    \n",
    "    # Compute advantages using GAE\n",
    "    advantages = torch.zeros_like(rewards).to(device)\n",
    "    lastgaelam = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        if t == len(rewards) - 1:\n",
    "            nextnonterminal = 1.0 - dones[t]\n",
    "            nextvalues = agent.get_value(torch.tensor(next_obs.state2vector()).to(device))\n",
    "        else:\n",
    "            nextnonterminal = 1.0 - dones[t + 1]\n",
    "            nextvalues = values[t + 1]\n",
    "        delta = rewards[t] + gamma * nextvalues * nextnonterminal - values[t]\n",
    "        advantages[t] = lastgaelam = delta + gamma * gae_lambda * nextnonterminal * lastgaelam\n",
    "    \n",
    "    returns = advantages + values\n",
    "    \n",
    "    # PPO Update\n",
    "    for _ in range(ppo_epochs):\n",
    "        indices = np.arange(len(states))\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        for start in range(0, len(states), batch_size):\n",
    "            end = start + batch_size\n",
    "            mb_inds = indices[start:end]\n",
    "\n",
    "            b_obs = states[mb_inds]\n",
    "            b_advantages = advantages[mb_inds]\n",
    "            b_returns = returns[mb_inds]\n",
    "            b_values = values[mb_inds]\n",
    "            b_logprobs = [logprobs[i] for i in mb_inds]\n",
    "\n",
    "            _, newlogprobs, entropies, newvalue = agent.get_action_and_value(b_obs)\n",
    "            logratios = [newlogprob - oldlogprob for newlogprob, oldlogprob in zip(newlogprobs, b_logprobs)]\n",
    "            ratios = [logratio.exp() for logratio in logratios]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                approx_kl = ((ratios[0] - 1) - logratios[0]).mean()\n",
    "                clipfrac = ((ratios[0] - 1.0).abs() > clip_coef).float().mean().item()\n",
    "\n",
    "            # Policy loss\n",
    "            pg_loss1 = [-mb_adv * ratio for ratio, mb_adv in zip(ratios, b_advantages)]\n",
    "            pg_loss2 = [-mb_adv * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef) for ratio, mb_adv in zip(ratios, b_advantages)]\n",
    "            policy_loss = torch.max(torch.stack(pg_loss1), torch.stack(pg_loss2)).mean()\n",
    "\n",
    "            # Value loss\n",
    "            v_loss_unclipped = (newvalue - b_returns) ** 2\n",
    "            v_clipped = b_values + torch.clamp(\n",
    "                newvalue - b_values, -clip_coef, clip_coef)\n",
    "            v_loss_clipped = (v_clipped - b_returns) ** 2\n",
    "            value_loss = 0.5 * torch.max(v_loss_unclipped, v_loss_clipped).mean()\n",
    "\n",
    "            # Entropy loss\n",
    "            entropy_loss = torch.stack(entropies).mean()\n",
    "\n",
    "            # Total loss\n",
    "            loss = policy_loss + value_loss * vf_coef - entropy_loss * ent_coef\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Logging\n",
    "        print(f\"Update {ppo_epochs} - Policy Loss: {policy_loss.item()}, Value Loss: {value_loss.item()}, KL: {approx_kl.item()}, Clip Frac: {clipfrac}\")\n",
    "\n",
    "    print(\"PPO Training Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent_with_self_play(env, current_agent, best_agent, optimizer, num_updates, num_simulations=50, ppo_epochs=4, batch_size=64, clip_coef=0.2, gamma=0.99, gae_lambda=0.95, ent_coef=0.01, vf_coef=0.5, max_grad_norm=0.5):\n",
    "    best_score = -float('inf')\n",
    "    \n",
    "    for update in range(1, num_updates + 1):\n",
    "        # Set the environment seed for consistent self-play comparisons\n",
    "        seed = np.random.randint(0, 1e6)\n",
    "\n",
    "        # Play the game with both the current and best agent\n",
    "        current_score = play_game(env, current_agent, num_simulations, seed=seed)\n",
    "        best_score_in_game = play_game(env, best_agent, num_simulations, seed=seed)\n",
    "\n",
    "        # Compare performances\n",
    "        if current_score > best_score_in_game:\n",
    "            best_agent.load_state_dict(current_agent.state_dict())\n",
    "            best_score = current_score\n",
    "            print(f\"New best agent at update {update} with score {best_score}\")\n",
    "        \n",
    "        # Train the current agent with PPO\n",
    "        train_with_ppo(env, current_agent, optimizer, ppo_epochs, batch_size, seed, clip_coef, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_updates = 100\n",
    "num_simulations = 100\n",
    "num_rollouts = 10\n",
    "# Create two competing agents\n",
    "current_agent = Agent(state_dim, action_dims).to(device)\n",
    "best_agent = Agent(state_dim, action_dims).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(current_agent.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_agent_with_self_play\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_updates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_updates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_simulations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_simulations\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[71], line 9\u001b[0m, in \u001b[0;36mtrain_agent_with_self_play\u001b[1;34m(env, current_agent, best_agent, optimizer, num_updates, num_simulations, ppo_epochs, batch_size, clip_coef, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm)\u001b[0m\n\u001b[0;32m      6\u001b[0m seed \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1e6\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Play the game with both the current and best agent\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m current_score \u001b[38;5;241m=\u001b[39m \u001b[43mplay_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_simulations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m best_score_in_game \u001b[38;5;241m=\u001b[39m play_game(env, best_agent, num_simulations, seed\u001b[38;5;241m=\u001b[39mseed)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Compare performances\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[69], line 7\u001b[0m, in \u001b[0;36mplay_game\u001b[1;34m(env, agent, num_simulations, seed)\u001b[0m\n\u001b[0;32m      4\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m----> 7\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mmcts\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_simulations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_simulations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     obs, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m      9\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[65], line 79\u001b[0m, in \u001b[0;36mmcts\u001b[1;34m(env, agent, num_simulations)\u001b[0m\n\u001b[0;32m     77\u001b[0m     node \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mselect_child()\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m node\u001b[38;5;241m.\u001b[39mis_fully_expanded():\n\u001b[1;32m---> 79\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m reward \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mrollout()\n\u001b[0;32m     81\u001b[0m node\u001b[38;5;241m.\u001b[39mbackpropagate(reward)\n",
      "Cell \u001b[1;32mIn[65], line 49\u001b[0m, in \u001b[0;36mMCTSNode.expand\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren[action] \u001b[38;5;241m=\u001b[39m child_node\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m child_node\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;43;01mException\u001b[39;49;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo unexplored actions found, despite not being fully expanded.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[65], line 49\u001b[0m, in \u001b[0;36mMCTSNode.expand\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren[action] \u001b[38;5;241m=\u001b[39m child_node\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m child_node\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;43;01mException\u001b[39;49;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo unexplored actions found, despite not being fully expanded.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf38\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf38\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_agent_with_self_play(env, current_agent, best_agent, optimizer, num_updates=num_updates, num_simulations=num_simulations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
