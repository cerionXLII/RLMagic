{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "import pygame\n",
    "import random\n",
    "from collections import deque\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get device\n",
    "CPU or GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create environment\n",
    "The Schedule needs an environment, lets create it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a class for the schedule problem using the gym interface\n",
    "class ScheduleGym():\n",
    "    def __init__(self, num_days, num_hours, num_classes, num_subjects, verbose=False):\n",
    "        self.num_days = num_days\n",
    "        self.num_hours = num_hours\n",
    "        self.num_classes = num_classes\n",
    "        self.num_subjects = num_subjects\n",
    "        self.num_slots = num_days * num_hours\n",
    "        self.target_hours = np.zeros((num_classes, num_subjects), dtype=int) # Target hours for each class and subject\n",
    "        self.schedule = -1*np.ones((num_classes, num_days, num_hours), dtype=int) # Schedule for each class, -1 means no subject assigned\n",
    "        self.num_actions_left = 1000 #Number of actions left to take\n",
    "        self.verbose = verbose #Debug on or off\n",
    "        #This is to go from a one dimensional action space to a 4 dimensional action space\n",
    "        #class_id, day, hour, subject_id\n",
    "        self.max_values = np.array([num_classes, num_days, num_hours, num_subjects])\n",
    "        self.cumprod_max_values = np.cumprod(self.max_values[::-1])[::-1]\n",
    "        # self.decoder_base = np.cumprod(self.max_values)\n",
    "        # self.encoder_base = np.flip(np.cumprod(np.flip(self.max_values)))\n",
    "        self.initial_hours_to_assign = 0 #How many hours we have initially to assign, used to calculate score later\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        for class_id in range(self.num_classes):\n",
    "            for subject_id in range(self.num_subjects):\n",
    "                self.target_hours[class_id, subject_id] = np.random.randint(1, 5)\n",
    "        \n",
    "        self.initial_hours_to_assign = self.target_hours.sum().sum()\n",
    "\n",
    "        self.schedule = -1*np.ones((self.num_classes, self.num_days, self.num_hours), dtype=int) # Schedule for each class, -1 means no subject assigned\n",
    "        self.num_actions_left = self.initial_hours_to_assign * 10 #Optimally we would need to take number of hours to assign steps to complete the schedule, lets give it some wiggle room\n",
    "        \n",
    "        #Return the current state,info\n",
    "        info = {}\n",
    "        #return (self.target_hours, self.schedule), info\n",
    "        return self.state2vector(), info\n",
    "    \n",
    "    def state2vector(self):\n",
    "        #Convert the state to a vector\n",
    "        return np.concatenate([self.target_hours.flatten(), self.schedule.flatten()])/(self.initial_hours_to_assign * 10.0 + 1.0)\n",
    "    \n",
    "    def render(self):\n",
    "        #Print the schedule\n",
    "        for class_id in range(self.num_classes):\n",
    "            print(f\"Class {class_id + 1}:\")\n",
    "            for day in range(self.num_days):\n",
    "                print(f\"Day {day + 1}: {self.schedule[class_id, day]}\")\n",
    "            print()\n",
    "        print(f'Fitness: {self.fitness()}, Actions left: {self.num_actions_left}')\n",
    "\n",
    "        #print the target hours\n",
    "        print(\"Target Hours:\")\n",
    "        for class_id in range(self.num_classes):\n",
    "            print(f\"Class {class_id + 1}: {self.target_hours[class_id]}\")\n",
    "\n",
    "\n",
    "\n",
    "    def decode_action(self, actions):\n",
    "        actions = np.array(actions).reshape(-1) # Ensure numbers is a 1D column array\n",
    "        aAll = np.zeros((actions.shape[0], len(self.max_values)), dtype=int)\n",
    "        for i in range(len(self.max_values) - 1):\n",
    "            aAll[:, i] = actions // self.cumprod_max_values[i+1]\n",
    "            actions -= aAll[:, i]*self.cumprod_max_values[i+1]\n",
    "        aAll[:,-1] = actions\n",
    "        \n",
    "        return aAll\n",
    "    \n",
    "    # Go from a 4D action to a 1D action\n",
    "    def encode_action(self, actions):\n",
    "        number = np.zeros(actions.shape[0], dtype=int)\n",
    "        for i in range(len(self.max_values) - 1):\n",
    "            number += actions[:,i]*self.cumprod_max_values[i+1]\n",
    "        number += actions[:,-1]\n",
    "        \n",
    "        return number\n",
    "\n",
    "            \n",
    "    #next_state, reward, done, truncated, info = env.step(action)\n",
    "    def step(self, action):\n",
    "        # Update the schedule based on the action\n",
    "\n",
    "        #Check if the action is a tuple or a single value\n",
    "        if isinstance(action, tuple):\n",
    "            #We are already in the decoded format\n",
    "            class_id, day, hour, subject_id = action\n",
    "        else:\n",
    "            #Need to go from 1D to 4D\n",
    "            decoded = self.decode_action(action).squeeze()\n",
    "            class_id = decoded[0]\n",
    "            day = decoded[1]\n",
    "            hour= decoded[2]\n",
    "            subject_id = decoded[3]\n",
    "\n",
    "         \n",
    "        #If subject_id is >= num_subjects then this is a remove action for the class_id, day, hour slot\n",
    "        #If the slot is already occupied, then the old subject will be placed back into the target hours\n",
    "        \n",
    "        current_subject_id = self.schedule[class_id, day, hour]\n",
    "\n",
    "        if self.verbose:\n",
    "            print('Before action:')\n",
    "            for class_id in range(self.num_classes):\n",
    "                print(f\"Class {class_id + 1}: {self.target_hours[class_id]}\")\n",
    " \n",
    "        #The slot was already booked, lets reomove it (and all of its dependencies)\n",
    "        result = \"N/A\"\n",
    "        reward = 0\n",
    "        if current_subject_id != -1:\n",
    "            self.schedule[class_id, day, hour] = -1\n",
    "            self.target_hours[class_id, current_subject_id] += 1\n",
    "            result = \"Removed\"\n",
    "            reward = -0.16 #Penalty for removing a subject\n",
    "\n",
    "        \n",
    "        #See if it is an add action\n",
    "        elif subject_id < self.num_subjects:\n",
    "            #Yes its an add action, lets see if we have enough hours to actually add it\n",
    "            if self.target_hours[class_id, subject_id] > 0:\n",
    "                self.schedule[class_id, day, hour] = subject_id\n",
    "                self.target_hours[class_id, subject_id] -= 1\n",
    "                result = \"Added\"\n",
    "                reward = 0.15 #Reward for adding a subject\n",
    "            else:\n",
    "                #We are trying to add a subject that is already empty\n",
    "                reward = -0.25 #Penalty for invalid action    \n",
    "        else:\n",
    "            #We are trying to remove a subject that is not in the schedule, this is an invalid action\n",
    "            reward = -0.5 #Penalty for invalid action\n",
    "        \n",
    "        self.num_actions_left -= 1\n",
    "        done = self.is_done()  \n",
    "        if done:\n",
    "            reward += self.fitness()  #If we are done we get the full score of the schedule\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Action: {action}, Class: {class_id}, Subject: {subject_id}, Day: {day}, Hour: {hour}, , current_subject_id: {current_subject_id}, Result: {result}, reward: {reward}, done: {done}\")     \n",
    "            print('After action:')\n",
    "            for class_id in range(self.num_classes):\n",
    "                print(f\"Class {class_id + 1}: {self.target_hours[class_id]}\")\n",
    "\n",
    "            if done:\n",
    "                print('Final schedule:')\n",
    "                self.render()\n",
    "                \n",
    "        truncated, info = False, {} #To be implemented later if needed\n",
    "        #Return the next state, reward, done, truncated and info\n",
    "        #return (self.target_hours, self.schedule), reward, done, truncated, info\n",
    "        return self.state2vector(), reward, done, truncated, info\n",
    "           \n",
    "\n",
    "    def is_done(self):\n",
    "        #Check if the schedule is complete\n",
    "        return np.all(self.target_hours == 0) or self.num_actions_left <= 0\n",
    "    \n",
    "    def get_action_sizes(self):\n",
    "        #Return the sizes of the action space\n",
    "        return [self.num_classes, self.num_days, self.num_hours, self.num_subjects]\n",
    "        #return np.prod(self.max_values)\n",
    "    \n",
    "    def get_state_sizes(self):\n",
    "        #Return the shapes of the state space\n",
    "        #return self.target_hours.shape, self.schedule.shape\n",
    "        return self.state2vector().shape\n",
    "\n",
    "    \n",
    "    def fitness(self):\n",
    "        #Calculate the fitness of the schedule\n",
    "        #fitness = self.num_actions_left * 0.001 #We want to maximize the number of actions left\n",
    "        fitness = 0.0\n",
    "        # target hours remaining\n",
    "        target_hours_remaining = self.target_hours.sum().sum()\n",
    "\n",
    "        fitness -= target_hours_remaining * 1\n",
    "\n",
    "        # Count the number of holes in the schedule, that is where no subject is assigned, but is surrounded by subjects\n",
    "        # If there are no subjects assigned to the edges, that is not considered a hole\n",
    "        num_holes = 0\n",
    "\n",
    "        # Create shifted versions of the schedule to compare adjacent hours\n",
    "        left_shifted = np.roll(self.schedule, shift=-1, axis=2)\n",
    "        right_shifted = np.roll(self.schedule, shift=1, axis=2)\n",
    "\n",
    "        # Identify holes: -1 in the current schedule, and not -1 in both the left and right shifted schedules\n",
    "        # Avoid considering the edges by setting the comparison for the first and last hour to False\n",
    "        holes = (self.schedule == -1) & (left_shifted != -1) & (right_shifted != -1)\n",
    "        holes[:, :, 0] = False  # Ignore first hour edge cases\n",
    "        holes[:, :, -1] = False  # Ignore last hour edge cases\n",
    "\n",
    "        # Count the number of holes\n",
    "        num_holes = np.sum(holes)\n",
    "\n",
    "        fitness -= num_holes * 0.5\n",
    "\n",
    "        #return -fitness #Swapped fitness now....\n",
    "        return fitness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an agent that can do scheduling\n",
    "The agent is of type Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ScheduleAgent(nn.Module):\n",
    "    def __init__(self, state_size, action_sizes, hidden_dim=256, gamma=0.99):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.action_sizes = action_sizes\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.actor_heads = [nn.Linear(hidden_dim, dim) for dim in action_sizes]\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        #Go through the shared layers\n",
    "        shared_output = self.shared(state)\n",
    "        \n",
    "        #Each head predicts its own action, like Class, Day, Hour, Subject etc\n",
    "        action_probs = [torch.softmax(head(shared_output), dim=-1) for head in self.actor_heads]\n",
    "        \n",
    "        #The critic predicts the value of the state\n",
    "        state_value = self.critic(shared_output)\n",
    "        \n",
    "        return action_probs, state_value\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        device = next(self.parameters()).device\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "       \n",
    "        #Get the probabilities of the actions for each head\n",
    "        with torch.no_grad():\n",
    "            action_probs, _ = self.forward(state)\n",
    "        \n",
    "        #Choose an action for each head\n",
    "        actions =[torch.multinomial(probs, 1).item() for probs in action_probs]\n",
    "\n",
    "        return actions\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ScheduleGym(num_days=2, num_hours=4, num_classes=1, num_subjects=2)\n",
    "state_dim = env.get_state_sizes()[0]\n",
    "action_dims = env.get_action_sizes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ScheduleAgent(state_dim, action_dims, hidden_dim=256)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,_ = env.reset()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ost = agent(torch.FloatTensor(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "ost2 = agent.choose_action(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 0]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ost2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(agent.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ScheduleAgent(\n",
       "  (shared): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (critic): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Send the agent to the correct device\n",
    "agent.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected TensorOptions(dtype=float, device=cpu, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)) (got TensorOptions(dtype=float, device=cuda:0, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m episode_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m---> 10\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     next_state, reward, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n\u001b[0;32m     13\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[1;32mIn[67], line 37\u001b[0m, in \u001b[0;36mScheduleAgent.choose_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchoose_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m     36\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m---> 37\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m#Get the probabilities of the actions for each head\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;31mTypeError\u001b[0m: expected TensorOptions(dtype=float, device=cpu, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)) (got TensorOptions(dtype=float, device=cuda:0, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)))"
     ]
    }
   ],
   "source": [
    "\n",
    "num_episodes = 10\n",
    "gamma = 0.99\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        actions = agent.choose_action(state)\n",
    "        next_state, reward, done = env.step(actions)\n",
    "        \n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        next_state = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "        reward = torch.FloatTensor([reward]).unsqueeze(0).to(device)\n",
    "        done = torch.FloatTensor([int(done)]).unsqueeze(0).to(device)\n",
    "        \n",
    "        action_probs, state_value = agent(state)\n",
    "        _, next_state_value = agent(next_state)\n",
    "        \n",
    "        td_error = reward + gamma * next_state_value * (1 - done) - state_value\n",
    "        \n",
    "        actor_loss = -(1 / len(action_probs)) * torch.sum(torch.log(action_probs[torch.arange(len(actions)), actions])) * td_error.detach()\n",
    "        critic_loss = td_error.pow(2)\n",
    "        \n",
    "        loss = actor_loss + critic_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "    \n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode}, Reward: {episode_reward}\")\n",
    "\n",
    "print(\"Final schedule:\")\n",
    "print(env.schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self, state, actions, reward, next_state, done):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
