{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "import gymnasium as gym\n",
    "import pygame\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import time\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import tempfile\n",
    "import json\n",
    "import shutil\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#Override device to cpu\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'MountainCar-v0'\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "#action_dims = [env.action_space.n]\n",
    "action_dims = env.action_space.n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, state_size, action_sizes):\n",
    "        super().__init__()\n",
    "        self.action_sizes = action_sizes\n",
    "        self.state_size = state_size\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(state_size, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(state_size, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, action_sizes), std=0.01),\n",
    "        )\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        logits = self.actor(x)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_dims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_dim, action_dims).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 2.5e-4\n",
    "num_steps = 128\n",
    "num_envs = 1\n",
    "total_timesteps = 1000\n",
    "num_minibatches = 4\n",
    "update_epochs = 4\n",
    "batch_size = num_steps * num_envs\n",
    "minibatch_size = int(batch_size // num_minibatches)\n",
    "anneal_lr = True\n",
    "target_kl = None\n",
    "\n",
    "\n",
    "\n",
    "gae = True #To fix later\n",
    "norm_adv = True\n",
    "clip_vloss = True #To fix later\n",
    "gamma = 0.99\n",
    "gae_lambda = 0.95\n",
    "update_epochs = 3\n",
    "clip_coef = 0.2\n",
    "ent_coef = 0.01\n",
    "vf_coef = 0.5\n",
    "max_grad_norm = 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALGO Logic: Storage setup\n",
    "obs = torch.zeros((num_steps, num_envs) + (state_dim,)).to(device) #Need to fix dims later\n",
    "actions = torch.zeros((num_steps, num_envs) + (action_dims,)).to(device) #Need to fix dims later\n",
    "logprobs = torch.zeros((num_steps, num_envs)).to(device)\n",
    "rewards = torch.zeros((num_steps, num_envs)).to(device)\n",
    "dones = torch.zeros((num_steps, num_envs)).to(device)\n",
    "values = torch.zeros((num_steps, num_envs)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 3])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "start_time = time.time()\n",
    "x,_ = env.reset()\n",
    "next_obs = torch.Tensor(x).to(device)\n",
    "next_done = torch.zeros(num_envs).to(device)\n",
    "num_updates = total_timesteps // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(agent.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Value is not broadcastable with batch_shape+event_shape: torch.Size([32, 3]) vs torch.Size([32]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 74\u001b[0m\n\u001b[0;32m     71\u001b[0m end \u001b[38;5;241m=\u001b[39m start \u001b[38;5;241m+\u001b[39m minibatch_size\n\u001b[0;32m     72\u001b[0m mb_inds \u001b[38;5;241m=\u001b[39m b_inds[start:end]\n\u001b[1;32m---> 74\u001b[0m _, newlogprob, entropy, newvalue \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action_and_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_obs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmb_inds\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_actions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmb_inds\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m logratio \u001b[38;5;241m=\u001b[39m newlogprob \u001b[38;5;241m-\u001b[39m b_logprobs[mb_inds]\n\u001b[0;32m     76\u001b[0m ratio \u001b[38;5;241m=\u001b[39m logratio\u001b[38;5;241m.\u001b[39mexp()\n",
      "Cell \u001b[1;32mIn[4], line 36\u001b[0m, in \u001b[0;36mAgent.get_action_and_value\u001b[1;34m(self, x, action)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     action \u001b[38;5;241m=\u001b[39m probs\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action, \u001b[43mprobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m, probs\u001b[38;5;241m.\u001b[39mentropy(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic(x)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\distributions\\categorical.py:137\u001b[0m, in \u001b[0;36mCategorical.log_prob\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_args:\n\u001b[1;32m--> 137\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m     value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    139\u001b[0m     value, log_pmf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\distributions\\distribution.py:297\u001b[0m, in \u001b[0;36mDistribution._validate_sample\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mreversed\u001b[39m(actual_shape), \u001b[38;5;28mreversed\u001b[39m(expected_shape)):\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m j \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m j:\n\u001b[1;32m--> 297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    298\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue is not broadcastable with batch_shape+event_shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactual_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m vs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    299\u001b[0m         )\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    301\u001b[0m     support \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport\n",
      "\u001b[1;31mValueError\u001b[0m: Value is not broadcastable with batch_shape+event_shape: torch.Size([32, 3]) vs torch.Size([32])."
     ]
    }
   ],
   "source": [
    "for update in range(1, num_updates + 1):\n",
    "        # Annealing the rate if instructed to do so.\n",
    "        if anneal_lr:\n",
    "            frac = 1.0 - (update - 1.0) / num_updates\n",
    "            lrnow = frac * learning_rate\n",
    "            optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "        for step in range(0, num_steps):\n",
    "            global_step += 1 * num_envs\n",
    "            obs[step] = next_obs\n",
    "            dones[step] = next_done\n",
    "\n",
    "            # ALGO LOGIC: action logic\n",
    "            with torch.no_grad():\n",
    "                action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "                values[step] = value.flatten()\n",
    "            actions[step] = action\n",
    "            logprobs[step] = logprob\n",
    "\n",
    "\n",
    "            next_obs, reward, done, info, _ = env.step(action.cpu().numpy())\n",
    "            rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "            \n",
    "            next_obs = torch.Tensor(next_obs).to(device)\n",
    "            next_done = torch.Tensor([int(done)]).to(device)\n",
    "\n",
    "\n",
    "\n",
    "        # bootstrap value if not done\n",
    "        with torch.no_grad():\n",
    "            next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "            if gae:\n",
    "                advantages = torch.zeros_like(rewards).to(device)\n",
    "                lastgaelam = 0\n",
    "                for t in reversed(range(num_steps)):\n",
    "                    if t == num_steps - 1:\n",
    "                        nextnonterminal = 1.0 - next_done\n",
    "                        nextvalues = next_value\n",
    "                    else:\n",
    "                        nextnonterminal = 1.0 - dones[t + 1]\n",
    "                        nextvalues = values[t + 1]\n",
    "                    delta = rewards[t] + gamma * nextvalues * nextnonterminal - values[t]\n",
    "                    advantages[t] = lastgaelam = delta + gamma * gae_lambda * nextnonterminal * lastgaelam\n",
    "                returns = advantages + values\n",
    "            else:\n",
    "                returns = torch.zeros_like(rewards).to(device)\n",
    "                for t in reversed(range(num_steps)):\n",
    "                    if t == num_steps - 1:\n",
    "                        nextnonterminal = 1.0 - next_done\n",
    "                        next_return = next_value\n",
    "                    else:\n",
    "                        nextnonterminal = 1.0 - dones[t + 1]\n",
    "                        next_return = returns[t + 1]\n",
    "                    returns[t] = rewards[t] + gamma * nextnonterminal * next_return\n",
    "                advantages = returns - values\n",
    "\n",
    "        # flatten the batch\n",
    "        b_obs = obs.reshape((-1,) + (state_dim,))\n",
    "        b_logprobs = logprobs.reshape(-1)\n",
    "        b_actions = actions.reshape((-1,) + (action_dims,)) #Need to fix dims later\n",
    "        b_advantages = advantages.reshape(-1)\n",
    "        b_returns = returns.reshape(-1)\n",
    "        b_values = values.reshape(-1)\n",
    "\n",
    "        # Optimizing the policy and value network\n",
    "        b_inds = np.arange(batch_size)\n",
    "        clipfracs = []\n",
    "        for epoch in range(update_epochs):\n",
    "            np.random.shuffle(b_inds)\n",
    "            for start in range(0, batch_size, minibatch_size):\n",
    "                end = start + minibatch_size\n",
    "                mb_inds = b_inds[start:end]\n",
    "\n",
    "                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n",
    "                logratio = newlogprob - b_logprobs[mb_inds]\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clipfracs += [((ratio - 1.0).abs() > clip_coef).float().mean().item()]\n",
    "\n",
    "                mb_advantages = b_advantages[mb_inds]\n",
    "                if norm_adv:\n",
    "                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "                # Policy loss\n",
    "                pg_loss1 = -mb_advantages * ratio\n",
    "                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # Value loss\n",
    "                newvalue = newvalue.view(-1)\n",
    "                if clip_vloss:\n",
    "                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                    v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                        newvalue - b_values[mb_inds],\n",
    "                        -clip_coef,\n",
    "                        clip_coef,\n",
    "                    )\n",
    "                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                    v_loss = 0.5 * v_loss_max.mean()\n",
    "                else:\n",
    "                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "                entropy_loss = entropy.mean()\n",
    "                loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "            if target_kl is not None:\n",
    "                if approx_kl > target_kl:\n",
    "                    break\n",
    "\n",
    "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "        # # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "        # writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "        # writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
    "        # writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
    "        # writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "        # writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
    "        # writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "        # writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
    "        # writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "        print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "        # writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]], device='cuda:0')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dones"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
