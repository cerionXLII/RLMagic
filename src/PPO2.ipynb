{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cf5-oDPjwf8"
      },
      "source": [
        "# Unit 8: Proximal Policy Gradient (PPO) with PyTorch ü§ñ\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/thumbnail.png\" alt=\"Unit 8\"/>\n",
        "\n",
        "\n",
        "In this notebook, you'll learn to **code your PPO agent from scratch with PyTorch using CleanRL implementation as model**.\n",
        "\n",
        "To test its robustness, we're going to train it in:\n",
        "\n",
        "- [LunarLander-v2 üöÄ](https://www.gymlibrary.dev/environments/box2d/lunar_lander/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Fl6Rxt0lc0O"
      },
      "source": [
        "‚¨áÔ∏è Here is an example of what you will achieve. ‚¨áÔ∏è"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6lIPYFghhYL"
      },
      "source": [
        "## Objectives of this notebook üèÜ\n",
        "\n",
        "At the end of the notebook, you will:\n",
        "\n",
        "- Be able to **code your PPO agent from scratch using PyTorch**.\n",
        "- Be able to **push your trained agent and the code to the Hub** with a nice video replay and an evaluation score üî•.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rasqqGQlhujA"
      },
      "source": [
        "## Prerequisites üèóÔ∏è\n",
        "Before diving into the notebook, you need to:\n",
        "\n",
        "üî≤ üìö Study [PPO by reading Unit 8](https://huggingface.co/deep-rl-course/unit8/introduction) ü§ó  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fd731S8-NuJA"
      },
      "outputs": [],
      "source": [
        "# !pip install setuptools==65.5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jV6wjQ7Be7p5"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !apt install python-opengl\n",
        "# !apt install ffmpeg\n",
        "# !apt install xvfb\n",
        "# !apt install swig cmake\n",
        "# !pip install pyglet==1.5\n",
        "# !pip3 install pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ww5PQH1gNLI4"
      },
      "outputs": [],
      "source": [
        "# # Virtual display\n",
        "# from pyvirtualdisplay import Display\n",
        "\n",
        "# virtual_display = Display(visible=0, size=(1400, 900))\n",
        "# virtual_display.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncIgfNf3mOtc"
      },
      "source": [
        "## Install dependencies üîΩ\n",
        "For this exercise, we use `gym==0.22`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xZQFTPcsKUK"
      },
      "outputs": [],
      "source": [
        "# !pip install gym==0.22\n",
        "# !pip install imageio-ffmpeg\n",
        "# !pip install huggingface_hub\n",
        "# !pip install gym[box2d]==0.22"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDkUufewmq6v"
      },
      "source": [
        "## Let's code PPO from scratch with Costa Huang tutorial\n",
        "- For the core implementation of PPO we're going to use the excellent [Costa Huang](https://costa.sh/) tutorial.\n",
        "- In addition to the tutorial, to go deeper you can read the 37 core implementation details: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/\n",
        "\n",
        "üëâ The video tutorial: https://youtu.be/MEt6rrxH8W4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f34ILn7AvTbt"
      },
      "source": [
        "- The best is to code first on the cell below, this way, if you kill the machine **you don't loose the implementation**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPi1Nme-oGWd"
      },
      "source": [
        "- Add dependencies we need to push our model to the Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sj8bz-AmoNVj"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import HfApi, upload_folder\n",
        "# from huggingface_hub.repocard import metadata_eval_result, metadata_save\n",
        "\n",
        "# from pathlib import Path\n",
        "# import datetime\n",
        "# import tempfile\n",
        "# import json\n",
        "# import shutil\n",
        "# import imageio\n",
        "\n",
        "# from wasabi import Printer\n",
        "# msg = Printer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blLZMiBAoUVT"
      },
      "source": [
        "- Next, we add the methods needed to push the model to the Hub\n",
        "\n",
        "- These methods will:\n",
        "  - `_evalutate_agent()`: evaluate the agent.\n",
        "  - `_generate_model_card()`: generate the model card of your agent.\n",
        "  - `_record_video()`: record a video of your agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlLcz4L9odXs"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def _evaluate_agent(env, n_eval_episodes, policy):\n",
        "  \"\"\"\n",
        "  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
        "  :param env: The evaluation environment\n",
        "  :param n_eval_episodes: Number of episode to evaluate the agent\n",
        "  :param policy: The agent\n",
        "  \"\"\"\n",
        "  episode_rewards = []\n",
        "  for episode in range(n_eval_episodes):\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards_ep = 0\n",
        "\n",
        "    while done is False:\n",
        "      state = torch.Tensor(state).to(device)\n",
        "      action, _, _, _ = policy.get_action_and_value(state)\n",
        "      new_state, reward, done, info = env.step(action.cpu().numpy())\n",
        "      total_rewards_ep += reward\n",
        "      if done:\n",
        "        break\n",
        "      state = new_state\n",
        "    episode_rewards.append(total_rewards_ep)\n",
        "  mean_reward = np.mean(episode_rewards)\n",
        "  std_reward = np.std(episode_rewards)\n",
        "\n",
        "  return mean_reward, std_reward\n",
        "\n",
        "\n",
        "def record_video(env, policy, out_directory, fps=30):\n",
        "  images = []\n",
        "  done = False\n",
        "  state = env.reset()\n",
        "  img = env.render(mode='rgb_array')\n",
        "  images.append(img)\n",
        "  while not done:\n",
        "    state = torch.Tensor(state).to(device)\n",
        "    # Take the action (index) that have the maximum expected future reward given that state\n",
        "    action, _, _, _  = policy.get_action_and_value(state)\n",
        "    state, reward, done, info = env.step(action.cpu().numpy()) # We directly put next_state = state for recording logic\n",
        "    img = env.render(mode='rgb_array')\n",
        "    images.append(img)\n",
        "  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_env(env_id, seed, idx, capture_video, run_name):\n",
        "    def thunk():\n",
        "        env = gym.make(env_id)\n",
        "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "        if capture_video:\n",
        "            if idx == 0:\n",
        "                env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
        "        # env.seed(seed)\n",
        "        # env.action_space.seed(seed)\n",
        "        # env.observation_space.seed(seed)\n",
        "        return env\n",
        "\n",
        "    return thunk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRqkGvk7pFQ6"
      },
      "source": [
        "## Let's start the training üî•\n",
        "- ‚ö†Ô∏è ‚ö†Ô∏è ‚ö†Ô∏è  Don't use **the same repo id with the one you used for the Unit 1**\n",
        "- Now that you've coded from scratch PPO and added the Hugging Face Integration, we're ready to start the training üî•"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tmEArP8ug2l"
      },
      "source": [
        "- First, you need to copy all your code to a file you create called `ppo.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KHcrITxgQ59h"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppopy\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from distutils.util import strtobool\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions.categorical import Categorical\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from huggingface_hub import HfApi, upload_folder\n",
        "from huggingface_hub.repocard import metadata_eval_result, metadata_save\n",
        "\n",
        "from pathlib import Path\n",
        "import datetime\n",
        "import tempfile\n",
        "import json\n",
        "import shutil\n",
        "import imageio\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "KtOk8q9ZQm1Y"
      },
      "outputs": [],
      "source": [
        "from types import SimpleNamespace\n",
        "def parse_args():\n",
        "    # fmt: off\n",
        "    args = SimpleNamespace()\n",
        "    args.exp_name = \"fiskmacka\"\n",
        "    args.seed = 1\n",
        "    args.torch_deterministic = True\n",
        "    args.cuda = True\n",
        "    args.track = False\n",
        "\n",
        "    args.capture_video = False\n",
        "    #args.env_id = \"CartPole-v1\"\n",
        "    args.env_id = \"LunarLander-v2\"\n",
        "    args.capture_video = False\n",
        "\n",
        "    args.total_timesteps = 50000*2*100*4\n",
        "    args.learning_rate = 2.5e-4\n",
        "    args.num_envs = 16\n",
        "    #args.num_steps = 128\n",
        "    args.num_steps = 256\n",
        "    args.anneal_lr = True\n",
        "    args.gae = True\n",
        "    args.gamma = 0.99\n",
        "    args.gae_lambda = 0.95\n",
        "    args.num_minibatches = 4\n",
        "    args.update_epochs = 4\n",
        "    args.norm_adv = True\n",
        "\n",
        "    args.clip_range = 0.2\n",
        "    args.clip_vloss = True\n",
        "    args.clip_coef = 0.2\n",
        "    args.ent_coef = 0.01\n",
        "    args.vf_coef = 0.5\n",
        "    args.max_grad_norm = 0.5\n",
        "    args.target_kl = None\n",
        "    args.clip_range_vf = None\n",
        "\n",
        "\n",
        "    args.batch_size = int(args.num_envs * args.num_steps)\n",
        "    args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
        "    # fmt: on\n",
        "    return args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "XLAqfdkNV1tK"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "namespace(exp_name='fiskmacka',\n",
              "          seed=1,\n",
              "          torch_deterministic=True,\n",
              "          cuda=True,\n",
              "          track=False,\n",
              "          capture_video=False,\n",
              "          env_id='LunarLander-v2',\n",
              "          total_timesteps=40000000,\n",
              "          learning_rate=0.00025,\n",
              "          num_envs=16,\n",
              "          num_steps=256,\n",
              "          anneal_lr=True,\n",
              "          gae=True,\n",
              "          gamma=0.99,\n",
              "          gae_lambda=0.95,\n",
              "          num_minibatches=4,\n",
              "          update_epochs=4,\n",
              "          norm_adv=True,\n",
              "          clip_range=0.2,\n",
              "          clip_vloss=True,\n",
              "          clip_coef=0.2,\n",
              "          ent_coef=0.01,\n",
              "          vf_coef=0.5,\n",
              "          max_grad_norm=0.5,\n",
              "          target_kl=None,\n",
              "          clip_range_vf=None,\n",
              "          batch_size=4096,\n",
              "          minibatch_size=1024)"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "args = parse_args()\n",
        "args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "gd0WXXsOXrVA"
      },
      "outputs": [],
      "source": [
        "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "    torch.nn.init.orthogonal_(layer.weight, std)\n",
        "    torch.nn.init.constant_(layer.bias, bias_const)\n",
        "    return layer\n",
        "\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, envs):\n",
        "        super().__init__()\n",
        "        self.critic = nn.Sequential(\n",
        "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, 1), std=1.0),\n",
        "        )\n",
        "        self.actor = nn.Sequential(\n",
        "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, envs.single_action_space.n), std=0.01),\n",
        "        )\n",
        "\n",
        "    def get_value(self, x):\n",
        "        return self.critic(x)\n",
        "\n",
        "    def get_action_and_value(self, x, action=None):\n",
        "        logits = self.actor(x)\n",
        "        probs = Categorical(logits=logits)\n",
        "        if action is None:\n",
        "            action = probs.sample()\n",
        "        return action, probs.log_prob(action), probs.entropy(), self.critic(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "Rmj-JVk7RaiT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "args = parse_args()\n",
        "run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
        "random.seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "torch.backends.cudnn.deterministic = args.torch_deterministic\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
        "print(f'Device: {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "fdEmJ-ah0CBg"
      },
      "outputs": [],
      "source": [
        "# env setup\n",
        "envs = gym.vector.SyncVectorEnv(\n",
        "    [make_env(args.env_id, args.seed + i, i, args.capture_video, run_name) for i in range(args.num_envs)]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "_f-hqTNG0Hln"
      },
      "outputs": [],
      "source": [
        "agent = Agent(envs).to(device)\n",
        "optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "CRmH5cLr0PdK"
      },
      "outputs": [],
      "source": [
        "# ALGO Logic: Storage setup\n",
        "obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n",
        "actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n",
        "logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
        "rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
        "dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
        "values = torch.zeros((args.num_steps, args.num_envs)).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "zEYGy73j0Tte"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num updates: 9765\n"
          ]
        }
      ],
      "source": [
        "# TRY NOT TO MODIFY: start the game\n",
        "global_step = 0\n",
        "start_time = time.time()\n",
        "next_obs, _ = envs.reset()\n",
        "next_obs = torch.Tensor(next_obs).to(device)\n",
        "next_done = torch.zeros(args.num_envs).to(device)\n",
        "num_updates = args.total_timesteps // args.batch_size\n",
        "print(f'Num updates: {num_updates}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "ljZLemj12RcC"
      },
      "outputs": [],
      "source": [
        "writer = SummaryWriter(f\"runs/{run_name}\")\n",
        "writer.add_text(\n",
        "    \"hyperparameters\",\n",
        "    \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "aJOxDDVe0bM3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SPS: 441\n",
            "SPS: 739\n",
            "SPS: 943\n",
            "SPS: 1088\n"
          ]
        }
      ],
      "source": [
        "for update in range(1, num_updates + 1):\n",
        "    # Annealing the rate if instructed to do so.\n",
        "    if args.anneal_lr:\n",
        "        frac = 1.0 - (update - 1.0) / num_updates\n",
        "        lrnow = frac * args.learning_rate\n",
        "        optimizer.param_groups[0][\"lr\"] = lrnow\n",
        "\n",
        "    for step in range(0, args.num_steps):\n",
        "        global_step += 1 * args.num_envs\n",
        "        obs[step] = next_obs\n",
        "        dones[step] = next_done\n",
        "\n",
        "        # ALGO LOGIC: action logic\n",
        "        with torch.no_grad():\n",
        "            action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
        "            values[step] = value.flatten()\n",
        "        actions[step] = action\n",
        "        logprobs[step] = logprob\n",
        "\n",
        "        # TRY NOT TO MODIFY: execute the game and log data.\n",
        "        next_obs, reward, done, info, _ = envs.step(action.cpu().numpy())\n",
        "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
        "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
        "\n",
        "        # for item in info:\n",
        "        #     if \"episode\" in item.keys():\n",
        "        #         print(f\"global_step={global_step}, episodic_return={item['episode']['r']}\")\n",
        "        #         writer.add_scalar(\"charts/episodic_return\", item[\"episode\"][\"r\"], global_step)\n",
        "        #         writer.add_scalar(\"charts/episodic_length\", item[\"episode\"][\"l\"], global_step)\n",
        "        #         break\n",
        "\n",
        "    # bootstrap value if not done\n",
        "    with torch.no_grad():\n",
        "        next_value = agent.get_value(next_obs).reshape(1, -1)\n",
        "        if args.gae:\n",
        "            advantages = torch.zeros_like(rewards).to(device)\n",
        "            lastgaelam = 0\n",
        "            for t in reversed(range(args.num_steps)):\n",
        "                if t == args.num_steps - 1:\n",
        "                    nextnonterminal = 1.0 - next_done\n",
        "                    nextvalues = next_value\n",
        "                else:\n",
        "                    nextnonterminal = 1.0 - dones[t + 1]\n",
        "                    nextvalues = values[t + 1]\n",
        "                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n",
        "                advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
        "            returns = advantages + values\n",
        "        else:\n",
        "            returns = torch.zeros_like(rewards).to(device)\n",
        "            for t in reversed(range(args.num_steps)):\n",
        "                if t == args.num_steps - 1:\n",
        "                    nextnonterminal = 1.0 - next_done\n",
        "                    next_return = next_value\n",
        "                else:\n",
        "                    nextnonterminal = 1.0 - dones[t + 1]\n",
        "                    next_return = returns[t + 1]\n",
        "                returns[t] = rewards[t] + args.gamma * nextnonterminal * next_return\n",
        "            advantages = returns - values\n",
        "\n",
        "    # flatten the batch\n",
        "    b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
        "    b_logprobs = logprobs.reshape(-1)\n",
        "    b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
        "    b_advantages = advantages.reshape(-1)\n",
        "    b_returns = returns.reshape(-1)\n",
        "    b_values = values.reshape(-1)\n",
        "\n",
        "    # Optimizing the policy and value network\n",
        "    b_inds = np.arange(args.batch_size)\n",
        "    clipfracs = []\n",
        "    for epoch in range(args.update_epochs):\n",
        "        np.random.shuffle(b_inds)\n",
        "        for start in range(0, args.batch_size, args.minibatch_size):\n",
        "            end = start + args.minibatch_size\n",
        "            mb_inds = b_inds[start:end]\n",
        "\n",
        "            _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n",
        "            logratio = newlogprob - b_logprobs[mb_inds]\n",
        "            ratio = logratio.exp()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
        "                old_approx_kl = (-logratio).mean()\n",
        "                approx_kl = ((ratio - 1) - logratio).mean()\n",
        "                clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
        "\n",
        "            mb_advantages = b_advantages[mb_inds]\n",
        "            if args.norm_adv:\n",
        "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
        "\n",
        "            # Policy loss\n",
        "            pg_loss1 = -mb_advantages * ratio\n",
        "            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
        "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
        "\n",
        "            # Value loss\n",
        "            newvalue = newvalue.view(-1)\n",
        "            if args.clip_vloss:\n",
        "                v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
        "                v_clipped = b_values[mb_inds] + torch.clamp(\n",
        "                    newvalue - b_values[mb_inds],\n",
        "                    -args.clip_coef,\n",
        "                    args.clip_coef,\n",
        "                )\n",
        "                v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
        "                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
        "                v_loss = 0.5 * v_loss_max.mean()\n",
        "            else:\n",
        "                v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
        "\n",
        "            entropy_loss = entropy.mean()\n",
        "            loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "        if args.target_kl is not None:\n",
        "            if approx_kl > args.target_kl:\n",
        "                break\n",
        "\n",
        "    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
        "    var_y = np.var(y_true)\n",
        "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
        "\n",
        "    # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
        "    writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
        "    writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
        "    writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
        "    writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
        "    writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
        "    writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
        "    writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
        "    writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
        "    print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
        "    writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_agent(agent, env, n_episodes=5):\n",
        "    for i_episode in range(1, n_episodes + 1):\n",
        "        state, info = env.reset()\n",
        "        done = False\n",
        "        score = 0\n",
        "        max_t = 256\n",
        "        t = 0\n",
        "        while not done:\n",
        "            env.render()\n",
        "            state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "            #action = agent.choose_action(state)\n",
        "            action, logprob, _, value = agent.get_action_and_value(state)\n",
        "            action = action.squeeze().cpu().numpy()\n",
        "            #action = action.cpu().numpy()\n",
        "            next_state, reward, done, truncated, info = env.step(action)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            t += 1\n",
        "            if done or t >= max_t:\n",
        "                break\n",
        "        print(f\"Episode {i_episode}\\tScore: {score}\")\n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1\tScore: 243.58248479845102\n",
            "Episode 2\tScore: 183.61589167988012\n",
            "Episode 3\tScore: 177.363281192169\n",
            "Episode 4\tScore: 288.65466814093804\n",
            "Episode 5\tScore: 25.35379975232631\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(args.env_id, render_mode=\"human\")\n",
        "visualize_agent(agent, env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX8rdrjjQfUo"
      },
      "outputs": [],
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()\n",
        "    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
        "    if args.track:\n",
        "        import wandb\n",
        "\n",
        "        wandb.init(\n",
        "            project=args.wandb_project_name,\n",
        "            entity=args.wandb_entity,\n",
        "            sync_tensorboard=True,\n",
        "            config=vars(args),\n",
        "            name=run_name,\n",
        "            monitor_gym=True,\n",
        "            save_code=True,\n",
        "        )\n",
        "    writer = SummaryWriter(f\"runs/{run_name}\")\n",
        "    writer.add_text(\n",
        "        \"hyperparameters\",\n",
        "        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
        "    )\n",
        "\n",
        "    # TRY NOT TO MODIFY: seeding\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
        "\n",
        "    # env setup\n",
        "    envs = gym.vector.SyncVectorEnv(\n",
        "        [make_env(args.env_id, args.seed + i, i, args.capture_video, run_name) for i in range(args.num_envs)]\n",
        "    )\n",
        "    assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n",
        "\n",
        "    agent = Agent(envs).to(device)\n",
        "    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n",
        "\n",
        "    # ALGO Logic: Storage setup\n",
        "    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n",
        "    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n",
        "    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
        "    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
        "    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
        "    values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
        "\n",
        "    # TRY NOT TO MODIFY: start the game\n",
        "    global_step = 0\n",
        "    start_time = time.time()\n",
        "    next_obs = torch.Tensor(envs.reset()).to(device)\n",
        "    next_done = torch.zeros(args.num_envs).to(device)\n",
        "    num_updates = args.total_timesteps // args.batch_size\n",
        "\n",
        "    for update in range(1, num_updates + 1):\n",
        "        # Annealing the rate if instructed to do so.\n",
        "        if args.anneal_lr:\n",
        "            frac = 1.0 - (update - 1.0) / num_updates\n",
        "            lrnow = frac * args.learning_rate\n",
        "            optimizer.param_groups[0][\"lr\"] = lrnow\n",
        "\n",
        "        for step in range(0, args.num_steps):\n",
        "            global_step += 1 * args.num_envs\n",
        "            obs[step] = next_obs\n",
        "            dones[step] = next_done\n",
        "\n",
        "            # ALGO LOGIC: action logic\n",
        "            with torch.no_grad():\n",
        "                action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
        "                values[step] = value.flatten()\n",
        "            actions[step] = action\n",
        "            logprobs[step] = logprob\n",
        "\n",
        "            # TRY NOT TO MODIFY: execute the game and log data.\n",
        "            next_obs, reward, done, info = envs.step(action.cpu().numpy())\n",
        "            rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
        "            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
        "\n",
        "            for item in info:\n",
        "                if \"episode\" in item.keys():\n",
        "                    print(f\"global_step={global_step}, episodic_return={item['episode']['r']}\")\n",
        "                    writer.add_scalar(\"charts/episodic_return\", item[\"episode\"][\"r\"], global_step)\n",
        "                    writer.add_scalar(\"charts/episodic_length\", item[\"episode\"][\"l\"], global_step)\n",
        "                    break\n",
        "\n",
        "        # bootstrap value if not done\n",
        "        with torch.no_grad():\n",
        "            next_value = agent.get_value(next_obs).reshape(1, -1)\n",
        "            if args.gae:\n",
        "                advantages = torch.zeros_like(rewards).to(device)\n",
        "                lastgaelam = 0\n",
        "                for t in reversed(range(args.num_steps)):\n",
        "                    if t == args.num_steps - 1:\n",
        "                        nextnonterminal = 1.0 - next_done\n",
        "                        nextvalues = next_value\n",
        "                    else:\n",
        "                        nextnonterminal = 1.0 - dones[t + 1]\n",
        "                        nextvalues = values[t + 1]\n",
        "                    delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n",
        "                    advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
        "                returns = advantages + values\n",
        "            else:\n",
        "                returns = torch.zeros_like(rewards).to(device)\n",
        "                for t in reversed(range(args.num_steps)):\n",
        "                    if t == args.num_steps - 1:\n",
        "                        nextnonterminal = 1.0 - next_done\n",
        "                        next_return = next_value\n",
        "                    else:\n",
        "                        nextnonterminal = 1.0 - dones[t + 1]\n",
        "                        next_return = returns[t + 1]\n",
        "                    returns[t] = rewards[t] + args.gamma * nextnonterminal * next_return\n",
        "                advantages = returns - values\n",
        "\n",
        "        # flatten the batch\n",
        "        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
        "        b_logprobs = logprobs.reshape(-1)\n",
        "        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
        "        b_advantages = advantages.reshape(-1)\n",
        "        b_returns = returns.reshape(-1)\n",
        "        b_values = values.reshape(-1)\n",
        "\n",
        "        # Optimizing the policy and value network\n",
        "        b_inds = np.arange(args.batch_size)\n",
        "        clipfracs = []\n",
        "        for epoch in range(args.update_epochs):\n",
        "            np.random.shuffle(b_inds)\n",
        "            for start in range(0, args.batch_size, args.minibatch_size):\n",
        "                end = start + args.minibatch_size\n",
        "                mb_inds = b_inds[start:end]\n",
        "\n",
        "                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n",
        "                logratio = newlogprob - b_logprobs[mb_inds]\n",
        "                ratio = logratio.exp()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
        "                    old_approx_kl = (-logratio).mean()\n",
        "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
        "                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
        "\n",
        "                mb_advantages = b_advantages[mb_inds]\n",
        "                if args.norm_adv:\n",
        "                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
        "\n",
        "                # Policy loss\n",
        "                pg_loss1 = -mb_advantages * ratio\n",
        "                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
        "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
        "\n",
        "                # Value loss\n",
        "                newvalue = newvalue.view(-1)\n",
        "                if args.clip_vloss:\n",
        "                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
        "                    v_clipped = b_values[mb_inds] + torch.clamp(\n",
        "                        newvalue - b_values[mb_inds],\n",
        "                        -args.clip_coef,\n",
        "                        args.clip_coef,\n",
        "                    )\n",
        "                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
        "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
        "                    v_loss = 0.5 * v_loss_max.mean()\n",
        "                else:\n",
        "                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
        "\n",
        "                entropy_loss = entropy.mean()\n",
        "                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
        "                optimizer.step()\n",
        "\n",
        "            if args.target_kl is not None:\n",
        "                if approx_kl > args.target_kl:\n",
        "                    break\n",
        "\n",
        "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
        "        var_y = np.var(y_true)\n",
        "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
        "\n",
        "        # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
        "        writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
        "        writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
        "        writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
        "        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
        "        writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
        "        writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
        "        writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
        "        writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
        "        print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
        "        writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
        "\n",
        "    envs.close()\n",
        "    writer.close()\n",
        "\n",
        "    # Create the evaluation environment\n",
        "    eval_env = gym.make(args.env_id)\n",
        "\n",
        "    package_to_hub(repo_id = args.repo_id,\n",
        "                model = agent, # The model we want to save\n",
        "                hyperparameters = args,\n",
        "                eval_env = gym.make(args.env_id),\n",
        "                logs= f\"runs/{run_name}\",\n",
        "                )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq0My0LOjPYR"
      },
      "source": [
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/step1.png\" alt=\"PPO\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8C-Q5ZyjUe3"
      },
      "source": [
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/step2.png\" alt=\"PPO\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrS80GmMu_j5"
      },
      "source": [
        "- Now we just need to run this python script using `python <name-of-python-script>.py` with the additional parameters we defined with `argparse`\n",
        "\n",
        "- You should modify more hyperparameters otherwise the training will not be super stable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXLih6mKseBs"
      },
      "outputs": [],
      "source": [
        "!python ppo.py --env-id=\"LunarLander-v2\" --repo-id=\"YOUR_REPO_ID\" --total-timesteps=50000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVsVJ5AdqLE7"
      },
      "source": [
        "## Some additional challenges üèÜ\n",
        "The best way to learn **is to try things by your own**! Why not trying  another environment?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYdl758GqLXT"
      },
      "source": [
        "See you on Unit 8, part 2 where we going to train agents to play Doom üî•\n",
        "## Keep learning, stay awesome ü§ó"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "history_visible": true,
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
